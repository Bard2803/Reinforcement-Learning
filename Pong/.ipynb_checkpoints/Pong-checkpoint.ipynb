{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce32b9b5",
   "metadata": {},
   "source": [
    "conda create -n pong numpy tensorflow keras jupyter notebook matplotlib\n",
    "\n",
    "pip install --upgrade pip --user\n",
    "\n",
    "pip install gym\n",
    "\n",
    "pip install gym[atari]\n",
    "\n",
    "pip install ale-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b06c9f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym in d:\\program files\\anaconda\\envs\\pong\\lib\\site-packages (0.23.1)\n",
      "Requirement already satisfied: pyvirtualdisplay in d:\\program files\\anaconda\\envs\\pong\\lib\\site-packages (3.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.10.0 in d:\\program files\\anaconda\\envs\\pong\\lib\\site-packages (from gym) (4.11.3)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in d:\\program files\\anaconda\\envs\\pong\\lib\\site-packages (from gym) (0.0.6)\n",
      "Requirement already satisfied: numpy>=1.18.0 in d:\\program files\\anaconda\\envs\\pong\\lib\\site-packages (from gym) (1.21.5)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in d:\\program files\\anaconda\\envs\\pong\\lib\\site-packages (from gym) (1.6.0)\n",
      "Requirement already satisfied: zipp>=0.5 in d:\\program files\\anaconda\\envs\\pong\\lib\\site-packages (from importlib-metadata>=4.10.0->gym) (3.7.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'apt-get' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'apt-get' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'apt-get' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: setuptools in d:\\program files\\anaconda\\envs\\pong\\lib\\site-packages (60.10.0)\n",
      "Requirement already satisfied: ez_setup in d:\\program files\\anaconda\\envs\\pong\\lib\\site-packages (0.9)\n",
      "Requirement already satisfied: gym[atari] in d:\\program files\\anaconda\\envs\\pong\\lib\\site-packages (0.23.1)\n",
      "Requirement already satisfied: numpy>=1.18.0 in d:\\program files\\anaconda\\envs\\pong\\lib\\site-packages (from gym[atari]) (1.21.5)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in d:\\program files\\anaconda\\envs\\pong\\lib\\site-packages (from gym[atari]) (0.0.6)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in d:\\program files\\anaconda\\envs\\pong\\lib\\site-packages (from gym[atari]) (1.6.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.10.0 in d:\\program files\\anaconda\\envs\\pong\\lib\\site-packages (from gym[atari]) (4.11.3)\n",
      "Requirement already satisfied: ale-py~=0.7.4 in d:\\program files\\anaconda\\envs\\pong\\lib\\site-packages (from gym[atari]) (0.7.4)\n",
      "Requirement already satisfied: importlib-resources in d:\\program files\\anaconda\\envs\\pong\\lib\\site-packages (from ale-py~=0.7.4->gym[atari]) (5.4.0)\n",
      "Requirement already satisfied: zipp>=0.5 in d:\\program files\\anaconda\\envs\\pong\\lib\\site-packages (from importlib-metadata>=4.10.0->gym[atari]) (3.7.0)\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies first for graphics visualization within Colaboratory\n",
    "\n",
    "#remove \" > /dev/null 2>&1\" to see what is going on under the hood\n",
    "!pip install gym pyvirtualdisplay \n",
    "!apt-get install -y xvfb python-opengl ffmpeg \n",
    "\n",
    "!apt-get update \n",
    "!apt-get install cmake \n",
    "!pip install --upgrade setuptools \n",
    "!pip install ez_setup \n",
    "!pip install gym[atari]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4ca0a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "\n",
    "# gym initialization\n",
    "\n",
    "env = gym.make(\"ALE/Pong-v5\")\n",
    "observation = env.reset()\n",
    "prev_input = None\n",
    "\n",
    "# Declaring the two actions that can happen in Pong for an agent, move up or move down\n",
    "# Decalring 0 means staying still. Note that this is pre-defined specific to package.\n",
    "UP_ACTION = 2\n",
    "DOWN_ACTION = 3\n",
    "\n",
    "# Hyperparameters. Gamma here allows you to measure the effect of future events\n",
    "gamma = 0.99\n",
    "\n",
    "# initialization of variables used in the main loop\n",
    "x_train, y_train, rewards = [],[],[]\n",
    "reward_sum = 0\n",
    "episode_nb = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3969397d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAM4AAAD8CAYAAAA/rZtiAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAPqUlEQVR4nO3df4wc9X3G8ffD+RfYCT7/rGtsbCMTgZvEJC6phKBJafilKgYkEtMKOS2qQYI2SKlUA2qKUllK0xBUqYLIFIJbUcAtASwVKJYbhUZKgm0wxsYYztjA2ZZNDohNjO9850//2LljMbf2+ju7N7PL85JOu/OdmZ3P3N1zOzs3+1lFBGZ2ck4pugCzVuTgmCVwcMwSODhmCRwcswQOjlmCpgVH0mWStkvqkrS8WdsxK4Ka8X8cSR3Aq8BXgW5gPXBtRLzc8I2ZFaBZzzjnA10R8XpE9AEPA4ubtC2zETeqSY87E3irarob+FKthSUd92lv2vhTGNuhBpVmVp+3Dgz8OiKmDjevWcEZ7rf8I+GQtAxYBtA5TnznD08//gNqZINz3jnnMHni8Wuq1tvXx/9tfL6JFbWuV68+nwNnTql7+dEHD/P5f/3fJlZUn1uefveNWvOaFZxuYFbV9BnAnuoFImIlsBJg9umjYqSDcSLSyIe1rZ3M97IFvu3Neo2zHpgvaa6kMcASYE2TtmU24pryjBMR/ZJuBv4H6ADuj4itzdiWWRGadahGRDwJPNmsxx9pu3bv5o09e4emJ51+Op89e36BFbWu6et38Dsbdw5NH5g9mZ1XnFdgRSevacFpNwMDR+k7cmRo+kh/f4HVtLaOIwOMPtQ7ND3q8JHjLF1OvuTGLIGDY5bAwTFL4OCYJXBwzBI4OGYJHByzBA6OWQIHxyyBg2OWwJfc1Om0U8cxZeLEoelPT5hQXDEt7nDneN6b++H7ww5Nn1hcMYkcnDrNmDqVGVOHfTOgnaR3zpnJO+fMLLqMXHyoZpbAwTFL4EO1GvqOHOFwb++JF8z09rXepfEjZdQHfYw++EHdy4/+bf3f96I4ODVsea2r6BLaxrynNhVdQsMlH6pJmiXpp5K2Sdoq6VvZ+B2SdkvalH1d0bhyzcohzzNOP/DtiHhe0qeAjZLWZvPuiogf1P1IEqeMGp2jFLORlRyciNgL7M3uH5S0jUojwpM2ac4C/vSBdamlmDXFX0+p3QuuIWfVJM0BzgN+lQ3dLGmzpPsldTZiG2Zlkjs4kiYAjwK3RMQB4B7gLGAhlWekO2ust0zSBkkbenp68pZhNqJyBUfSaCqheTAifgIQEfsiYiAijgL3UmnA/jERsTIiFkXEosmTJ+cpw2zE5TmrJuA+YFtE/LBqfEbVYlcBW9LLMyunPGfVLgCuA16StCkbuw24VtJCKk3WdwE35NiGWSnlOav2c4Zvj9023TvNavG1amYJHByzBA6OWYJSXOR5YM8Onvq7q4suw6xupQhOf+8H9Ox8qegyzOrmQzWzBA6OWQIHxyyBg2OWwMExS+DgmCVwcMwSODhmCRwcswQOjlkCB8csgYNjliDXRZ6SdgEHgQGgPyIWSZoEPALMofLW6a9HxLv5yjQrl0Y843wlIhZGxKJsejmwLiLmA+uyabO20oxDtcXAquz+KuDKJmzDrFB5gxPAM5I2SlqWjU3P2uMOtsmdlnMbZqWT941sF0TEHknTgLWSXql3xSxoywA6x/kchbWWXL+xEbEnu90PPEala+e+waaE2e3+GusOdfKcMGa4LlNm5ZWnk+f47OM9kDQeuIRK1841wNJssaXAE3mLNCubPIdq04HHKp1wGQX8R0Q8LWk9sFrS9cCbwDX5yzQrlzydPF8HPj/MeA9wcZ6izMrOr8rNEjg4ZgkcHLMEDo5ZAgfHLIGDY5bAwTFL4OCYJXBwzBI4OGYJHByzBA6OWQIHxyyBg2OWwMExS+DgmCVwcMwSJL8DVNJnqHTsHDQP+A4wEfhL4O1s/LaIeDJ1O2ZllOet09uBhQCSOoDdVDrd/DlwV0T8oBEFmpVRow7VLgZ2RMQbDXo8s1JrVHCWAA9VTd8sabOk+yV1NmgbZqWROziSxgBfA/4zG7oHOIvKYdxe4M4a6y2TtEHShvf7Im8ZZiOqEc84lwPPR8Q+gIjYFxEDEXEUuJdKd8+PcSdPa2WNCM61VB2mDba/zVxFpbunWVvJ+8FSpwFfBW6oGv6+pIVUPslg1zHzzNpCruBExCFg8jFj1+WqyKwF+MoBswQOjlkCB8csgYNjlsDBMUvg4JglcHDMEjg4ZgkcHLMEDo5ZAgfHLIGDY5bAwTFL4OCYJXBwzBLkej+OWVkc/N1Ojo7uGJo+7e2DjD7U27TtOTjWFnZd8jl6J00Ymp73388zafvepm3vhIdqWYun/ZK2VI1NkrRW0mvZbWfVvFsldUnaLunSZhVuVqR6XuM8AFx2zNhyYF1EzAfWZdNIOpdKj7UF2Tp3Z10+zdrKCYMTEc8C7xwzvBhYld1fBVxZNf5wRPRGxE6gixrtocxaWepZtekRsRcgu52Wjc8E3qparjsb+xg3JLRW1ujT0cN1Fhw2FW5IaK0sNTj7BhsPZrf7s/FuYFbVcmcAe9LLMyun1OCsAZZm95cCT1SNL5E0VtJcYD7wXL4SzcrnhP/HkfQQ8GVgiqRu4O+B7wGrJV0PvAlcAxARWyWtBl4G+oGbImKgSbWbFeaEwYmIa2vMurjG8iuAFXmKMis7X6tmlsDBMUvg4JglcHDMEjg4ZgkcHLMEfj+OtYX5j68nOj58Hhh98HBTt+fgWFsY996hEd2eD9XMEjg4ZgkcHLMEDo5ZAgfHLIGDY5bAwTFL4OCYJXBwzBKkdvL8J0mvSNos6TFJE7PxOZI+kLQp+/pRE2s3K0xqJ8+1wO9FxOeAV4Fbq+btiIiF2deNjSnTrFySOnlGxDMR0Z9N/pJKGyizT4xGvMb5C+Cpqum5kl6Q9DNJF9ZayZ08rZXlujpa0u1U2kA9mA3tBWZHRI+kLwKPS1oQEQeOXTciVgIrAWafPsrJsZaS/IwjaSnwJ8CfRUQAZM3We7L7G4EdwNmNKNSsTJKCI+ky4G+Br0XEoarxqYMf6yFpHpVOnq83olCzMknt5HkrMBZYKwngl9kZtIuA70rqBwaAGyPi2I8IMWt5qZ0876ux7KPAo3mLMis7XzlglsDBMUvg4JglcHDMEjg4ZgkcHLMEDo5ZAgfHLIGDY5bAwTFL4OCYJXBwzBI4OGYJHByzBA6OWQIHxyyBg2OWILWT5x2Sdld17Lyiat6tkrokbZd0abMKNytSaidPgLuqOnY+CSDpXGAJsCBb5+7B5h1m7SSpk+dxLAYeztpE7QS6gPNz1GdWSnle49ycNV2/X1JnNjYTeKtqme5s7GMa2cmzo6OD8aeeOvR16tixuR7P7ERSO3neA/wDENntnVRa4WqYZYdNRSM7eU7t7OSzZ88fmn7/0CF+senFPA85rFHjxjNm/KcZ6D1M7/vvNvzxrXUkPeNExL6IGIiIo8C9fHg41g3Mqlr0DGBPvhLLY/5XvsHV//wsv//NO4ouxQqW2slzRtXkVcDgGbc1wBJJYyXNpdLJ87l8JZqVT2onzy9LWkjlMGwXcANARGyVtBp4mUoz9psiYqAplZsVqKGdPLPlVwAr8hRVVhHB0YF+8N+CT7xcH/PxSfPK0z/mlad/XHQZVgK+5MYsgYNjlsDBMUvg4JglcHDMEjg4ZgkcHLMEDo5ZAgfHLIGDY5agLS65effAb3jh5W1D0/1HfS2ZNVdbBKe37wi9fe8VXYZ9gvhQzSyBg2OWwMExS5DakPCRqmaEuyRtysbnSPqgat6Pmli7WWHqOTnwAPAvwL8NDkTENwbvS7oT+E3V8jsiYmGD6jMrpXreOv2spDnDzZMk4OvAHzW4LrNSy/sa50JgX0S8VjU2V9ILkn4m6cKcj29WSnn/j3Mt8FDV9F5gdkT0SPoi8LikBRFx4NgVJS0DlgF0jvM5Cmstyb+xkkYBVwOPDI5lPaN7svsbgR3A2cOtHxErI2JRRCyaMGa4BqBm5ZXnT/0fA69ERPfggKSpg59OIGkelYaEr+cr0ax86jkd/RDwC+AzkrolXZ/NWsJHD9MALgI2S3oR+C/gxoio95MOzFpGakNCIuKbw4w9CjyavyyzcvOrcrMEDo5ZAgfHLIGDY5bAwTFL4OCYJXBwzBI4OGYJHByzBA6OWQIHxyyBg2OWwMExS+DgmCUoRQvcCdNmc+FffbfoMsw+6unras4qRXDGjP80Z37p8qLLMKubD9XMEtTz1ulZkn4qaZukrZK+lY1PkrRW0mvZbWfVOrdK6pK0XdKlzdwBsyLU84zTD3w7Is4B/gC4SdK5wHJgXUTMB9Zl02TzlgALgMuAuwcbeJi1ixMGJyL2RsTz2f2DwDZgJrAYWJUttgq4Mru/GHg4axW1E+gCzm9w3WaFOqnXOFkr3POAXwHTI2IvVMIFTMsWmwm8VbVadzZm1jbqDo6kCVQ62NwyXGfO6kWHGYthHm+ZpA2SNvT09NRbhlkp1BUcSaOphObBiPhJNrxP0oxs/gxgfzbeDcyqWv0MYM+xj1ndyXPy5Mmp9ZsVop6zagLuA7ZFxA+rZq0Blmb3lwJPVI0vkTRW0lwq3Tyfa1zJZsWr5x+gFwDXAS8NfoAUcBvwPWB11tnzTeAagIjYKmk18DKVM3I3RYQ/BtraSj2dPH/O8K9bAC6usc4KYEWOusxKzVcOmCVwcMwSODhmCRwcswQOjlkCRXzsn/ojX4T0NvBb4NdF19JAU2if/WmnfYH69+fMiJg63IxSBAdA0oaIWFR0HY3STvvTTvsCjdkfH6qZJXBwzBKUKTgriy6gwdppf9ppX6AB+1Oa1zhmraRMzzhmLaPw4Ei6LGvq0SVpedH1pJC0S9JLkjZJ2pCN1WxmUjaS7pe0X9KWqrGWbcZSY3/ukLQ7+xltknRF1byT35+IKOwL6AB2APOAMcCLwLlF1pS4H7uAKceMfR9Ynt1fDvxj0XUep/6LgC8AW05UP3Bu9nMaC8zNfn4dRe9DHftzB/A3wyybtD9FP+OcD3RFxOsR0Qc8TKXZRzuo1cykdCLiWeCdY4ZbthlLjf2pJWl/ig5OuzT2COAZSRslLcvGajUzaRXt2IzlZkmbs0O5wUPPpP0pOjh1NfZoARdExBeAy6n0nbuo6IKaqFV/ZvcAZwELgb3Andl40v4UHZy6GnuUXUTsyW73A49Reaqv1cykVeRqxlI2EbEvIgYi4ihwLx8ejiXtT9HBWQ/MlzRX0hgqHUDXFFzTSZE0XtKnBu8DlwBbqN3MpFW0VTOWwT8Cmauo/IwgdX9KcAbkCuBVKmczbi+6noT651E5K/MisHVwH4DJVFoDv5bdTiq61uPsw0NUDl+OUPkLfP3x6gduz35e24HLi66/zv35d+AlYHMWlhl59sdXDpglKPpQzawlOThmCRwcswQOjlkCB8csgYNjlsDBMUvg4Jgl+H9So5GY6M1SoQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Let's take a look at the game in action.\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "env = gym.make(\"ALE/Pong-v5\") # environment info\n",
    "observation = env.reset()\n",
    "# The ball is released after 20 frames\n",
    "for i in range(25):\n",
    "  \n",
    "  if i > 20:\n",
    "    plt.imshow(observation)\n",
    "    plt.show()\n",
    "\n",
    "  observation, _, _, _ = env.step(1)\n",
    "  \n",
    "  \n",
    "  \n",
    "  #Can plot more later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7021c376",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing function\n",
    "\n",
    "\n",
    "def prepro(I):\n",
    "  \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector \"\"\"\n",
    "  I = I[35:195] # crop\n",
    "  I = I[::2,::2,0] # downsample by factor of 2\n",
    "  I[I == 144] = 0 # erase background (background type 1)\n",
    "  I[I == 109] = 0 # erase background (background type 2)\n",
    "  I[I != 0] = 1 # everything else (paddles, ball) just set to 1\n",
    "  return I.astype(float).ravel()\n",
    "  #Can plot more later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "342b9da5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Show preprocessed\n",
    "\n",
    "obs_preprocessed = prepro(observation).reshape(80,80)\n",
    "plt.imshow(obs_preprocessed, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "520b9b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Karpathy Suport class\n",
    "\n",
    "\n",
    "# reward discount used by Karpathy (cf. https://gist.github.com/karpathy/a4166c7fe253700972fcbc77e4ea32c5)\n",
    "def discount_rewards(r, gamma):\n",
    "  \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
    "  r = np.array(r)\n",
    "  discounted_r = np.zeros_like(r)\n",
    "  running_add = 0\n",
    "  # we go from last reward to first one so we don't have to do exponentiations\n",
    "  for t in reversed(range(0, r.size)):\n",
    "    if r[t] != 0: running_add = 0 # if the game ended (in Pong), reset the reward sum\n",
    "    running_add = running_add * gamma + r[t] # the point here is to use Horner's method to compute those rewards efficiently\n",
    "    discounted_r[t] = running_add\n",
    "  discounted_r -= np.mean(discounted_r) #normalizing the result\n",
    "  discounted_r /= np.std(discounted_r) #idem using standar deviation\n",
    "  return discounted_r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7d76394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 200)               1280200   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 201       \n",
      "=================================================================\n",
      "Total params: 1,280,401\n",
      "Trainable params: 1,280,401\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# import necessary modules from keras\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import Flatten\n",
    "from keras.models import Sequential\n",
    "import keras\n",
    "from keras.models import InputLayer\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# creates a generic neural network architecture\n",
    "\n",
    "\"\"\"\n",
    "The 80 * 80 input dimension comes from the pre-processing of the raw pixels made by Karpathy (the only important pixels are the balls and the paddle)\n",
    "Input here represents the difference in pixels betewen one frame and another, giving you direction of agents and ball. Encoded in Karpathy's own preprocessing functions\n",
    "\n",
    "TODO, try adding a 400 layer infront of the 200 unit layer\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# hidden layer takes a pre-processed frame as input, and has 200 units. Simple layer architectur of 200 x1, 1x1\n",
    "model.add(Dense(units=200,input_dim=80*80, activation='relu', kernel_initializer='glorot_uniform'))\n",
    "\n",
    "# output layer - we use a Sigmoid here, in order to get a 0, or 1 value to represent ACTION UP\n",
    "model.add(Dense(units=1, activation='sigmoid', kernel_initializer='RandomNormal'))\n",
    "\n",
    "# compile the model using traditional Machine Learning losses and optimizers\n",
    "model.compile(loss='binary_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d6f4879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6400\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[[0.5]]\n"
     ]
    }
   ],
   "source": [
    "history = []\n",
    "observation = env.reset()\n",
    "prev_input = None\n",
    "\n",
    "#sanity checks\n",
    "cur_input = prepro(observation)\n",
    "print(len(cur_input))\n",
    "x = cur_input - prev_input if prev_input is not None else np.zeros(80 * 80)\n",
    "prev_input = cur_input\n",
    "print(x)\n",
    "\n",
    "proba = model.predict(np.expand_dims(x, axis=1).T)\n",
    "print(proba)\n",
    "\n",
    "proba = model.predict(np.expand_dims(x, axis=1).T)\n",
    "action = UP_ACTION if np.random.uniform() < proba else DOWN_ACTION\n",
    "y = 1 if action == 2 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3706a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At the end of episode 0 the total reward was : -27.0\n",
      "49/49 [==============================] - 0s 7ms/step - loss: -0.0023 - accuracy: 0.5161\n",
      "At the end of episode 1 the total reward was : -20.0\n",
      "39/39 [==============================] - 0s 7ms/step - loss: 0.0024 - accuracy: 0.5550\n",
      "At the end of episode 2 the total reward was : -21.0\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.0066 - accuracy: 0.6030\n",
      "At the end of episode 3 the total reward was : -21.0\n",
      "32/32 [==============================] - 0s 7ms/step - loss: -0.0150 - accuracy: 0.6079\n",
      "At the end of episode 4 the total reward was : -21.0\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.0083 - accuracy: 0.6002\n",
      "At the end of episode 5 the total reward was : -21.0\n",
      "30/30 [==============================] - 0s 7ms/step - loss: -9.4839e-04 - accuracy: 0.6270\n",
      "At the end of episode 6 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 7ms/step - loss: -0.0117 - accuracy: 0.6570\n",
      "At the end of episode 7 the total reward was : -21.0\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.0023 - accuracy: 0.6749\n",
      "At the end of episode 8 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 7ms/step - loss: -0.0045 - accuracy: 0.6759\n",
      "At the end of episode 9 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 8ms/step - loss: -0.0041 - accuracy: 0.7001\n",
      "At the end of episode 10 the total reward was : -21.0\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0190 - accuracy: 0.7089\n",
      "At the end of episode 11 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.0408 - accuracy: 0.7248\n",
      "At the end of episode 12 the total reward was : -21.0\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.0230 - accuracy: 0.7057\n",
      "At the end of episode 13 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.0196 - accuracy: 0.7261\n",
      "At the end of episode 14 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.0124 - accuracy: 0.7316\n",
      "At the end of episode 15 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.0222 - accuracy: 0.7219\n",
      "At the end of episode 16 the total reward was : -21.0\n",
      "34/34 [==============================] - 0s 7ms/step - loss: 0.0238 - accuracy: 0.7402\n",
      "At the end of episode 17 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.0424 - accuracy: 0.7554\n",
      "At the end of episode 18 the total reward was : -21.0\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.0305 - accuracy: 0.7508\n",
      "At the end of episode 19 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.0387 - accuracy: 0.7231\n",
      "At the end of episode 20 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.0205 - accuracy: 0.7588\n",
      "At the end of episode 21 the total reward was : -21.0\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.0402 - accuracy: 0.7354TA: 0s - loss: 0.0153 - accuracy: 0\n",
      "At the end of episode 22 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.0287 - accuracy: 0.7527\n",
      "At the end of episode 23 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.0649 - accuracy: 0.7673\n",
      "At the end of episode 24 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.0442 - accuracy: 0.7721\n",
      "At the end of episode 25 the total reward was : -20.0\n",
      "29/29 [==============================] - 0s 7ms/step - loss: 0.0171 - accuracy: 0.7511\n",
      "At the end of episode 26 the total reward was : -21.0\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 0.0485 - accuracy: 0.7609\n",
      "At the end of episode 27 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.0346 - accuracy: 0.7170\n",
      "At the end of episode 28 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.0547 - accuracy: 0.7352\n",
      "At the end of episode 29 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.0672 - accuracy: 0.7333\n",
      "At the end of episode 30 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.0696 - accuracy: 0.7527\n",
      "At the end of episode 31 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.0494 - accuracy: 0.7276\n",
      "At the end of episode 32 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.0634 - accuracy: 0.7364: 0s - loss: 0.0423 - accuracy: \n",
      "At the end of episode 33 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.0741 - accuracy: 0.7388\n",
      "At the end of episode 34 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.0577 - accuracy: 0.7279\n",
      "At the end of episode 35 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.0538 - accuracy: 0.7412\n",
      "At the end of episode 36 the total reward was : -21.0\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.0429 - accuracy: 0.7376: 0s - loss: 0.0315 - accuracy: \n",
      "At the end of episode 37 the total reward was : -21.0\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 0.0242 - accuracy: 0.7241\n",
      "At the end of episode 38 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.0551 - accuracy: 0.7352\n",
      "At the end of episode 39 the total reward was : -21.0\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.0242 - accuracy: 0.7518\n",
      "At the end of episode 40 the total reward was : -21.0\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 0.0463 - accuracy: 0.7548\n",
      "At the end of episode 41 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.0680 - accuracy: 0.7152\n",
      "At the end of episode 42 the total reward was : -21.0\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.0295 - accuracy: 0.7418\n",
      "At the end of episode 43 the total reward was : -21.0\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 0.0261 - accuracy: 0.7441\n",
      "At the end of episode 44 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.0503 - accuracy: 0.7782\n",
      "At the end of episode 45 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.0603 - accuracy: 0.7884\n",
      "At the end of episode 46 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.0539 - accuracy: 0.7630\n",
      "At the end of episode 47 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.0580 - accuracy: 0.7276\n",
      "At the end of episode 48 the total reward was : -21.0\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.0455 - accuracy: 0.7418\n",
      "At the end of episode 49 the total reward was : -21.0\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.0052 - accuracy: 0.7313\n",
      "At the end of episode 50 the total reward was : -21.0\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.0141 - accuracy: 0.7355\n",
      "At the end of episode 51 the total reward was : -21.0\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 0.0281 - accuracy: 0.7475\n",
      "At the end of episode 52 the total reward was : -21.0\n",
      "34/34 [==============================] - 0s 7ms/step - loss: 0.0176 - accuracy: 0.7637\n",
      "At the end of episode 53 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.0362 - accuracy: 0.7576\n",
      "At the end of episode 54 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.0434 - accuracy: 0.7418\n",
      "At the end of episode 55 the total reward was : -21.0\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.0513 - accuracy: 0.7666\n",
      "At the end of episode 56 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.0170 - accuracy: 0.7648\n",
      "At the end of episode 57 the total reward was : -21.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 0s 6ms/step - loss: 0.0510 - accuracy: 0.7678\n",
      "At the end of episode 58 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.0556 - accuracy: 0.7872\n",
      "At the end of episode 59 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.0275 - accuracy: 0.7867\n",
      "At the end of episode 60 the total reward was : -21.0\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.0765 - accuracy: 0.7838\n",
      "At the end of episode 61 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.0590 - accuracy: 0.7690\n",
      "At the end of episode 62 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.0390 - accuracy: 0.7479\n",
      "At the end of episode 63 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.0437 - accuracy: 0.7576\n",
      "At the end of episode 64 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.0548 - accuracy: 0.7382\n",
      "At the end of episode 65 the total reward was : -21.0\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.0260 - accuracy: 0.7482\n",
      "At the end of episode 66 the total reward was : -21.0\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.0303 - accuracy: 0.7351\n",
      "At the end of episode 67 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.0362 - accuracy: 0.7412\n",
      "At the end of episode 68 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.0404 - accuracy: 0.7449\n",
      "At the end of episode 69 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.0398 - accuracy: 0.7473\n",
      "At the end of episode 70 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.0507 - accuracy: 0.7782\n",
      "At the end of episode 71 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.0485 - accuracy: 0.7503\n",
      "At the end of episode 72 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.0114 - accuracy: 0.7430\n",
      "At the end of episode 73 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 7ms/step - loss: -0.0183 - accuracy: 0.6985\n",
      "At the end of episode 74 the total reward was : -21.0\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.0145 - accuracy: 0.7291\n",
      "At the end of episode 75 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.0414 - accuracy: 0.7418\n",
      "At the end of episode 76 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.0230 - accuracy: 0.7418\n",
      "At the end of episode 77 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.0099 - accuracy: 0.7527\n",
      "At the end of episode 78 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.0209 - accuracy: 0.7200\n",
      "At the end of episode 79 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.0576 - accuracy: 0.7515\n",
      "At the end of episode 80 the total reward was : -21.0\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.0341 - accuracy: 0.7351\n",
      "At the end of episode 81 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.0165 - accuracy: 0.7624\n",
      "At the end of episode 82 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.0514 - accuracy: 0.7630\n",
      "At the end of episode 83 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.0213 - accuracy: 0.7424\n",
      "At the end of episode 84 the total reward was : -21.0\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.0268 - accuracy: 0.7835\n",
      "At the end of episode 85 the total reward was : -20.0\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 0.0055 - accuracy: 0.7663\n",
      "At the end of episode 86 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.0466 - accuracy: 0.7576\n",
      "At the end of episode 87 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.0590 - accuracy: 0.7709\n",
      "At the end of episode 88 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.0454 - accuracy: 0.7787\n",
      "At the end of episode 89 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.0276 - accuracy: 0.8027\n",
      "At the end of episode 90 the total reward was : -21.0\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 0.0094 - accuracy: 0.7587\n",
      "At the end of episode 91 the total reward was : -21.0\n",
      "28/28 [==============================] - 0s 6ms/step - loss: -0.0143 - accuracy: 0.7745\n",
      "At the end of episode 92 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.0405 - accuracy: 0.7497\n",
      "At the end of episode 93 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.0282 - accuracy: 0.7799\n",
      "At the end of episode 94 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.0404 - accuracy: 0.7867\n",
      "At the end of episode 95 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.0469 - accuracy: 0.7724\n",
      "At the end of episode 96 the total reward was : -21.0\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 0.0278 - accuracy: 0.7587\n",
      "At the end of episode 97 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.0312 - accuracy: 0.7818\n",
      "At the end of episode 98 the total reward was : -21.0\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0101 - accuracy: 0.7974\n",
      "At the end of episode 99 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.0039 - accuracy: 0.7455\n",
      "At the end of episode 100 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 7ms/step - loss: -0.0075 - accuracy: 0.7678\n",
      "At the end of episode 101 the total reward was : -21.0\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 0.0057 - accuracy: 0.7889 \n",
      "At the end of episode 102 the total reward was : -21.0\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 1.7371e-04 - accuracy: 0.7892\n",
      "At the end of episode 103 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.0247 - accuracy: 0.8170\n",
      "At the end of episode 104 the total reward was : -21.0\n",
      "28/28 [==============================] - 0s 6ms/step - loss: -0.0136 - accuracy: 0.7773\n",
      "At the end of episode 105 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 7ms/step - loss: -0.0259 - accuracy: 0.7939\n",
      "At the end of episode 106 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.0390 - accuracy: 0.8247\n",
      "At the end of episode 107 the total reward was : -21.0\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.0352 - accuracy: 0.8331\n",
      "At the end of episode 108 the total reward was : -21.0\n",
      "32/32 [==============================] - 0s 7ms/step - loss: -0.0215 - accuracy: 0.8154\n",
      "At the end of episode 109 the total reward was : -21.0\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.0155 - accuracy: 0.8410\n",
      "At the end of episode 110 the total reward was : -21.0\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.0283 - accuracy: 0.8219\n",
      "At the end of episode 111 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.0195 - accuracy: 0.8145\n",
      "At the end of episode 112 the total reward was : -21.0\n",
      "34/34 [==============================] - 0s 7ms/step - loss: 0.0060 - accuracy: 0.8304\n",
      "At the end of episode 113 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.0024 - accuracy: 0.8339\n",
      "At the end of episode 114 the total reward was : -21.0\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 0.0105 - accuracy: 0.8162\n",
      "At the end of episode 115 the total reward was : -20.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/29 [==============================] - 0s 7ms/step - loss: -0.0129 - accuracy: 0.7918\n",
      "At the end of episode 116 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.0666 - accuracy: 0.8133\n",
      "At the end of episode 117 the total reward was : -21.0\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 0.0079 - accuracy: 0.8219\n",
      "At the end of episode 118 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.0530 - accuracy: 0.8376\n",
      "At the end of episode 119 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.0232 - accuracy: 0.8291\n",
      "At the end of episode 120 the total reward was : -20.0\n",
      "29/29 [==============================] - 0s 7ms/step - loss: 0.0281 - accuracy: 0.8337\n",
      "At the end of episode 121 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.0039 - accuracy: 0.8182\n",
      "At the end of episode 122 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.0357 - accuracy: 0.8085\n",
      "At the end of episode 123 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.0755 - accuracy: 0.8412\n",
      "At the end of episode 124 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.0416 - accuracy: 0.8012\n",
      "At the end of episode 125 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 1.2036e-04 - accuracy: 0.8036\n",
      "At the end of episode 126 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.0287 - accuracy: 0.7939\n",
      "At the end of episode 127 the total reward was : -21.0\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 0.0282 - accuracy: 0.8174\n",
      "At the end of episode 128 the total reward was : -21.0\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 0.0376 - accuracy: 0.8275\n",
      "At the end of episode 129 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.0379 - accuracy: 0.8339\n",
      "At the end of episode 130 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.0428 - accuracy: 0.8230\n",
      "At the end of episode 131 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.0537 - accuracy: 0.8242\n",
      "At the end of episode 132 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.0396 - accuracy: 0.8230\n",
      "At the end of episode 133 the total reward was : -21.0\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 0.0143 - accuracy: 0.8027\n",
      "At the end of episode 134 the total reward was : -21.0\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.0121 - accuracy: 0.7983\n",
      "At the end of episode 135 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.0148 - accuracy: 0.8255\n",
      "At the end of episode 136 the total reward was : -21.0\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 0.0119 - accuracy: 0.8286\n",
      "At the end of episode 137 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.0090 - accuracy: 0.8339\n",
      "At the end of episode 138 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.0613 - accuracy: 0.8509\n",
      "At the end of episode 139 the total reward was : -20.0\n",
      "29/29 [==============================] - 0s 6ms/step - loss: 0.0047 - accuracy: 0.8492\n",
      "At the end of episode 140 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.0174 - accuracy: 0.8073\n",
      "At the end of episode 141 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.0553 - accuracy: 0.8606\n",
      "At the end of episode 142 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.0074 - accuracy: 0.8267\n",
      "At the end of episode 143 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.0279 - accuracy: 0.8343\n",
      "At the end of episode 144 the total reward was : -21.0\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.0379 - accuracy: 0.8504\n",
      "At the end of episode 145 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.0376 - accuracy: 0.8436\n",
      "At the end of episode 146 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.0011 - accuracy: 0.8206\n",
      "At the end of episode 147 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.0258 - accuracy: 0.8376\n",
      "At the end of episode 148 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.0315 - accuracy: 0.8545\n",
      "At the end of episode 149 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.0632 - accuracy: 0.8461\n",
      "At the end of episode 150 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.0115 - accuracy: 0.8424\n",
      "At the end of episode 151 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.0354 - accuracy: 0.8315\n",
      "At the end of episode 152 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.0090 - accuracy: 0.8376\n",
      "At the end of episode 153 the total reward was : -20.0\n",
      "29/29 [==============================] - 0s 7ms/step - loss: -0.0180 - accuracy: 0.8048\n",
      "At the end of episode 154 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.0268 - accuracy: 0.8255\n",
      "At the end of episode 155 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.0446 - accuracy: 0.8376\n",
      "At the end of episode 156 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.0154 - accuracy: 0.8339\n",
      "At the end of episode 157 the total reward was : -21.0\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.0229 - accuracy: 0.8422\n",
      "At the end of episode 158 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 7ms/step - loss: -0.0092 - accuracy: 0.8315\n",
      "At the end of episode 159 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.0489 - accuracy: 0.8461\n",
      "At the end of episode 160 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.0428 - accuracy: 0.8570\n",
      "At the end of episode 161 the total reward was : -21.0\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.0266 - accuracy: 0.8579\n",
      "At the end of episode 162 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.0237 - accuracy: 0.8436\n",
      "At the end of episode 163 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.0322 - accuracy: 0.8679\n",
      "At the end of episode 164 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.0395 - accuracy: 0.8473\n",
      "At the end of episode 165 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.0162 - accuracy: 0.8642\n",
      "At the end of episode 166 the total reward was : -21.0\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.0347 - accuracy: 0.8659\n",
      "At the end of episode 167 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.0172 - accuracy: 0.8533\n",
      "At the end of episode 168 the total reward was : -21.0\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.0300 - accuracy: 0.8602\n",
      "At the end of episode 169 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.0409 - accuracy: 0.8594\n",
      "At the end of episode 170 the total reward was : -20.0\n",
      "35/35 [==============================] - 0s 7ms/step - loss: -0.0311 - accuracy: 0.8553\n",
      "At the end of episode 171 the total reward was : -21.0\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.0321 - accuracy: 0.8757\n",
      "At the end of episode 172 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.0313 - accuracy: 0.8715\n",
      "At the end of episode 173 the total reward was : -21.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 0s 7ms/step - loss: 0.0085 - accuracy: 0.8915\n",
      "At the end of episode 174 the total reward was : -21.0\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.0086 - accuracy: 0.8873\n",
      "At the end of episode 175 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.0576 - accuracy: 0.8861\n",
      "At the end of episode 176 the total reward was : -21.0\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 0.0120 - accuracy: 0.8870\n",
      "At the end of episode 177 the total reward was : -21.0\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.0084 - accuracy: 0.8659\n",
      "At the end of episode 178 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.0235 - accuracy: 0.8824\n",
      "At the end of episode 179 the total reward was : -21.0\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.0479 - accuracy: 0.8689\n",
      "At the end of episode 180 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.0391 - accuracy: 0.8655\n",
      "At the end of episode 181 the total reward was : -21.0\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.0067 - accuracy: 0.8815\n",
      "At the end of episode 182 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.0108 - accuracy: 0.8800\n",
      "At the end of episode 183 the total reward was : -21.0\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.0255 - accuracy: 0.8805\n",
      "At the end of episode 184 the total reward was : -21.0\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.0135 - accuracy: 0.8956\n",
      "At the end of episode 185 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.0451 - accuracy: 0.8958\n",
      "At the end of episode 186 the total reward was : -21.0\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.0624 - accuracy: 0.8782\n",
      "At the end of episode 187 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.0288 - accuracy: 0.8812\n",
      "At the end of episode 188 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.0341 - accuracy: 0.8776\n",
      "At the end of episode 189 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.0447 - accuracy: 0.8945\n",
      "At the end of episode 190 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.0250 - accuracy: 0.9018\n",
      "At the end of episode 191 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.0455 - accuracy: 0.8861\n",
      "At the end of episode 192 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.0321 - accuracy: 0.8848\n",
      "At the end of episode 193 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.0302 - accuracy: 0.8897\n",
      "At the end of episode 194 the total reward was : -21.0\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.0098 - accuracy: 0.8826\n",
      "At the end of episode 195 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.0029 - accuracy: 0.8727\n",
      "At the end of episode 196 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.0129 - accuracy: 0.8824\n",
      "At the end of episode 197 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.0273 - accuracy: 0.8848\n",
      "At the end of episode 198 the total reward was : -21.0\n",
      "30/30 [==============================] - 0s 8ms/step - loss: -5.1594e-04 - accuracy: 0.8921\n",
      "At the end of episode 199 the total reward was : -21.0\n",
      "32/32 [==============================] - 0s 7ms/step - loss: -0.0046 - accuracy: 0.8709\n",
      "At the end of episode 200 the total reward was : -21.0\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.0169 - accuracy: 0.9044\n",
      "At the end of episode 201 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.0099 - accuracy: 0.9018\n",
      "At the end of episode 202 the total reward was : -21.0\n",
      "57/57 [==============================] - 0s 8ms/step - loss: -0.0084 - accuracy: 0.8917\n",
      "At the end of episode 203 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 7ms/step - loss: -0.0215 - accuracy: 0.8873\n",
      "At the end of episode 204 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.0116 - accuracy: 0.8958\n",
      "At the end of episode 205 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 7ms/step - loss: -0.0069 - accuracy: 0.9103\n",
      "At the end of episode 206 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.0277 - accuracy: 0.9188\n",
      "At the end of episode 207 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.0269 - accuracy: 0.9103\n",
      "At the end of episode 208 the total reward was : -21.0\n",
      "28/28 [==============================] - 0s 7ms/step - loss: -2.5624e-04 - accuracy: 0.8952\n",
      "At the end of episode 209 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.0199 - accuracy: 0.8885\n",
      "At the end of episode 210 the total reward was : -21.0\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.0064 - accuracy: 0.8974\n",
      "At the end of episode 211 the total reward was : -21.0\n",
      "30/30 [==============================] - 0s 7ms/step - loss: -0.0071 - accuracy: 0.8819\n",
      "At the end of episode 212 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.0290 - accuracy: 0.8861\n",
      "At the end of episode 213 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 8ms/step - loss: -3.8324e-04 - accuracy: 0.8958\n",
      "At the end of episode 214 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.0098 - accuracy: 0.8921\n",
      "At the end of episode 215 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.0424 - accuracy: 0.8739\n",
      "At the end of episode 216 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.0242 - accuracy: 0.8703\n",
      "At the end of episode 217 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.0021 - accuracy: 0.8606\n",
      "At the end of episode 218 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.0058 - accuracy: 0.8642\n",
      "At the end of episode 219 the total reward was : -21.0\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.0049 - accuracy: 0.8760\n",
      "At the end of episode 220 the total reward was : -21.0\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.0048 - accuracy: 0.8794\n",
      "At the end of episode 221 the total reward was : -21.0\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.0170 - accuracy: 0.8936\n",
      "At the end of episode 222 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.0248 - accuracy: 0.8873\n",
      "At the end of episode 223 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.0311 - accuracy: 0.8994\n",
      "At the end of episode 224 the total reward was : -20.0\n",
      "27/27 [==============================] - 0s 7ms/step - loss: -0.0234 - accuracy: 0.8943\n",
      "At the end of episode 225 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.0362 - accuracy: 0.9115\n",
      "At the end of episode 226 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.0319 - accuracy: 0.9018\n",
      "At the end of episode 227 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.0212 - accuracy: 0.8945\n",
      "At the end of episode 228 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.0090 - accuracy: 0.8788\n",
      "At the end of episode 229 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 7ms/step - loss: -0.0228 - accuracy: 0.9030\n",
      "At the end of episode 230 the total reward was : -21.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 0s 7ms/step - loss: 0.0385 - accuracy: 0.8994\n",
      "At the end of episode 231 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.0181 - accuracy: 0.9030\n",
      "At the end of episode 232 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 7ms/step - loss: -0.0303 - accuracy: 0.9079\n",
      "At the end of episode 233 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 7ms/step - loss: -0.0039 - accuracy: 0.8970\n",
      "At the end of episode 234 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 7ms/step - loss: -1.3108e-04 - accuracy: 0.9055\n",
      "At the end of episode 235 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 7ms/step - loss: -0.0021 - accuracy: 0.9103\n",
      "At the end of episode 236 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.0265 - accuracy: 0.9164\n",
      "At the end of episode 237 the total reward was : -21.0\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.0172 - accuracy: 0.8963\n",
      "At the end of episode 238 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.0213 - accuracy: 0.9176\n",
      "At the end of episode 239 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.0362 - accuracy: 0.9105\n",
      "At the end of episode 240 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.0402 - accuracy: 0.9055\n",
      "At the end of episode 241 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.0089 - accuracy: 0.9018\n",
      "At the end of episode 242 the total reward was : -21.0\n",
      "28/28 [==============================] - 0s 7ms/step - loss: -0.0079 - accuracy: 0.9086\n",
      "At the end of episode 243 the total reward was : -20.0\n",
      "35/35 [==============================] - 0s 7ms/step - loss: 0.0108 - accuracy: 0.9048\n",
      "At the end of episode 244 the total reward was : -21.0\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 0.0217 - accuracy: 0.9062\n",
      "At the end of episode 245 the total reward was : -20.0\n",
      "29/29 [==============================] - 0s 6ms/step - loss: 0.0151 - accuracy: 0.9111\n",
      "At the end of episode 246 the total reward was : -21.0\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.0285 - accuracy: 0.9086\n",
      "At the end of episode 247 the total reward was : -21.0\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.0113 - accuracy: 0.9164\n",
      "At the end of episode 248 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.0345 - accuracy: 0.9042\n",
      "At the end of episode 249 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.0055 - accuracy: 0.9285\n",
      "At the end of episode 250 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 7ms/step - loss: -0.0053 - accuracy: 0.9139\n",
      "At the end of episode 251 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 7ms/step - loss: -0.0022 - accuracy: 0.9236\n",
      "At the end of episode 252 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.0136 - accuracy: 0.9042\n",
      "At the end of episode 253 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.0023 - accuracy: 0.8970\n",
      "At the end of episode 254 the total reward was : -21.0\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.0064 - accuracy: 0.9081\n",
      "At the end of episode 255 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.0340 - accuracy: 0.9212\n",
      "At the end of episode 256 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.0016 - accuracy: 0.9212\n",
      "At the end of episode 257 the total reward was : -20.0\n",
      "35/35 [==============================] - 0s 7ms/step - loss: 0.0115 - accuracy: 0.9102\n",
      "At the end of episode 258 the total reward was : -21.0\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.0172 - accuracy: 0.9186\n",
      "At the end of episode 259 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.0207 - accuracy: 0.9067\n",
      "At the end of episode 260 the total reward was : -20.0\n",
      "29/29 [==============================] - 0s 7ms/step - loss: -0.0134 - accuracy: 0.9089\n",
      "At the end of episode 261 the total reward was : -21.0\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.0190 - accuracy: 0.9153\n",
      "At the end of episode 262 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.0082 - accuracy: 0.9430\n",
      "At the end of episode 263 the total reward was : -21.0\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.0016 - accuracy: 0.9121\n",
      "At the end of episode 264 the total reward was : -21.0\n",
      "30/30 [==============================] - 0s 7ms/step - loss: -0.0034 - accuracy: 0.9112\n",
      "At the end of episode 265 the total reward was : -20.0\n",
      "27/27 [==============================] - 0s 7ms/step - loss: -0.0067 - accuracy: 0.9164\n",
      "At the end of episode 266 the total reward was : -20.0\n",
      "29/29 [==============================] - 0s 7ms/step - loss: 0.0044 - accuracy: 0.9076\n",
      "At the end of episode 267 the total reward was : -21.0\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.0180 - accuracy: 0.9301\n",
      "At the end of episode 268 the total reward was : -20.0\n",
      "35/35 [==============================] - 0s 7ms/step - loss: -0.0187 - accuracy: 0.9076\n",
      "At the end of episode 269 the total reward was : -21.0\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.0174 - accuracy: 0.9071\n",
      "At the end of episode 270 the total reward was : -21.0\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.0111 - accuracy: 0.9093\n",
      "At the end of episode 271 the total reward was : -21.0\n",
      "30/30 [==============================] - 0s 7ms/step - loss: -0.0142 - accuracy: 0.9176\n",
      "At the end of episode 272 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.0267 - accuracy: 0.9370\n",
      "At the end of episode 273 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.0196 - accuracy: 0.9285\n",
      "At the end of episode 274 the total reward was : -21.0\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.0157 - accuracy: 0.9291\n",
      "At the end of episode 275 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.0235 - accuracy: 0.9200\n",
      "At the end of episode 276 the total reward was : -20.0\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.0075 - accuracy: 0.9164\n",
      "At the end of episode 277 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.0310 - accuracy: 0.9176\n",
      "At the end of episode 278 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 6ms/step - loss: -0.0039 - accuracy: 0.9042\n",
      "At the end of episode 279 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 6ms/step - loss: -0.0045 - accuracy: 0.9152\n",
      "At the end of episode 280 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.0111 - accuracy: 0.9321\n",
      "At the end of episode 281 the total reward was : -20.0\n",
      "29/29 [==============================] - 0s 6ms/step - loss: -0.0379 - accuracy: 0.9121\n",
      "At the end of episode 282 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.0148 - accuracy: 0.9236\n",
      "At the end of episode 283 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 6ms/step - loss: -0.0018 - accuracy: 0.9188\n",
      "At the end of episode 284 the total reward was : -20.0\n",
      "31/31 [==============================] - 0s 6ms/step - loss: -0.0107 - accuracy: 0.9126\n",
      "At the end of episode 285 the total reward was : -21.0\n",
      "61/61 [==============================] - 0s 7ms/step - loss: -0.0199 - accuracy: 0.9180\n",
      "At the end of episode 286 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 6ms/step - loss: -3.8743e-04 - accuracy: 0.9176\n",
      "At the end of episode 287 the total reward was : -21.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 0s 6ms/step - loss: 0.0033 - accuracy: 0.9236\n",
      "At the end of episode 288 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 3.8287e-04 - accuracy: 0.9212\n",
      "At the end of episode 289 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.0019 - accuracy: 0.9224\n",
      "At the end of episode 290 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 6ms/step - loss: -0.0116 - accuracy: 0.9115\n",
      "At the end of episode 291 the total reward was : -21.0\n",
      "30/30 [==============================] - 0s 6ms/step - loss: -0.0123 - accuracy: 0.9176\n",
      "At the end of episode 292 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 6ms/step - loss: -0.0202 - accuracy: 0.9103\n",
      "At the end of episode 293 the total reward was : -21.0\n",
      "28/28 [==============================] - 0s 6ms/step - loss: -0.0230 - accuracy: 0.9132\n",
      "At the end of episode 294 the total reward was : -21.0\n",
      "30/30 [==============================] - 0s 6ms/step - loss: -0.0160 - accuracy: 0.9125\n",
      "At the end of episode 295 the total reward was : -21.0\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 0.0032 - accuracy: 0.9064\n",
      "At the end of episode 296 the total reward was : -21.0\n",
      "28/28 [==============================] - 0s 7ms/step - loss: -0.0114 - accuracy: 0.9210\n",
      "At the end of episode 297 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.0081 - accuracy: 0.9067\n",
      "At the end of episode 298 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 6ms/step - loss: -0.0175 - accuracy: 0.9091\n",
      "At the end of episode 299 the total reward was : -21.0\n",
      "32/32 [==============================] - 0s 6ms/step - loss: -0.0207 - accuracy: 0.9088\n",
      "At the end of episode 300 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 6ms/step - loss: -0.0099 - accuracy: 0.9188\n",
      "At the end of episode 301 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 6ms/step - loss: -0.0207 - accuracy: 0.8982\n",
      "At the end of episode 302 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 9.8732e-04 - accuracy: 0.9030\n",
      "At the end of episode 303 the total reward was : -21.0\n",
      "28/28 [==============================] - 0s 6ms/step - loss: -0.0147 - accuracy: 0.9177\n",
      "At the end of episode 304 the total reward was : -21.0\n",
      "30/30 [==============================] - 0s 6ms/step - loss: -0.0124 - accuracy: 0.9208\n",
      "At the end of episode 305 the total reward was : -21.0\n",
      "28/28 [==============================] - 0s 7ms/step - loss: -0.0024 - accuracy: 0.9267\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 14>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# forward the policy network and sample action according to the probability distribution\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;124;03mRunning model.predict to know what the current model thinks about the probability of doing the UP_ACTION, \u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;124;03mgiven the current frame setting.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;124;03mKeras requires a third?? dimension perhaps hre\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m proba \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand_dims\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# proba=model.predict(np.expand_dims(x.reshape(80,80), axis=0)) 2D model stuff DELETE\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \n\u001b[0;32m     33\u001b[0m \n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m#Intorucing another probability distirubtion here, not sure\u001b[39;00m\n\u001b[0;32m     35\u001b[0m action \u001b[38;5;241m=\u001b[39m UP_ACTION \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39muniform() \u001b[38;5;241m<\u001b[39m proba \u001b[38;5;28;01melse\u001b[39;00m DOWN_ACTION\n",
      "File \u001b[1;32mD:\\Program files\\Anaconda\\envs\\pong\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:130\u001b[0m, in \u001b[0;36mdisable_multi_worker.<locals>._method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_multi_worker_mode():  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    128\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m is not supported in multi-worker mode.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    129\u001b[0m       method\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m))\n\u001b[1;32m--> 130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Program files\\Anaconda\\envs\\pong\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1569\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1566\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1567\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_strategy\u001b[38;5;241m.\u001b[39mscope():\n\u001b[0;32m   1568\u001b[0m   \u001b[38;5;66;03m# Creates a `tf.data.Dataset` and handles batch and epoch iteration.\u001b[39;00m\n\u001b[1;32m-> 1569\u001b[0m   data_handler \u001b[38;5;241m=\u001b[39m \u001b[43mdata_adapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataHandler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1570\u001b[0m \u001b[43m      \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1571\u001b[0m \u001b[43m      \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1572\u001b[0m \u001b[43m      \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1573\u001b[0m \u001b[43m      \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1574\u001b[0m \u001b[43m      \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1575\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1576\u001b[0m \u001b[43m      \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1577\u001b[0m \u001b[43m      \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1578\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1579\u001b[0m \u001b[43m      \u001b[49m\u001b[43msteps_per_execution\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_steps_per_execution\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1581\u001b[0m   \u001b[38;5;66;03m# Container that configures and calls `tf.keras.Callback`s.\u001b[39;00m\n\u001b[0;32m   1582\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(callbacks, callbacks_module\u001b[38;5;241m.\u001b[39mCallbackList):\n",
      "File \u001b[1;32mD:\\Program files\\Anaconda\\envs\\pong\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py:1105\u001b[0m, in \u001b[0;36mDataHandler.__init__\u001b[1;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution)\u001b[0m\n\u001b[0;32m   1102\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_steps_per_execution_value \u001b[38;5;241m=\u001b[39m steps_per_execution\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m   1104\u001b[0m adapter_cls \u001b[38;5;241m=\u001b[39m select_data_adapter(x, y)\n\u001b[1;32m-> 1105\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_adapter \u001b[38;5;241m=\u001b[39m \u001b[43madapter_cls\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1106\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1107\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1108\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1109\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1110\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1111\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1112\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1113\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1114\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1115\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1116\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdistribution_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mds_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_strategy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1117\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1119\u001b[0m strategy \u001b[38;5;241m=\u001b[39m ds_context\u001b[38;5;241m.\u001b[39mget_strategy()\n\u001b[0;32m   1120\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_adapter\u001b[38;5;241m.\u001b[39mget_dataset()\n",
      "File \u001b[1;32mD:\\Program files\\Anaconda\\envs\\pong\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py:362\u001b[0m, in \u001b[0;36mTensorLikeDataAdapter.__init__\u001b[1;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[0;32m    359\u001b[0m     flat_dataset \u001b[38;5;241m=\u001b[39m flat_dataset\u001b[38;5;241m.\u001b[39mshuffle(\u001b[38;5;241m1024\u001b[39m)\u001b[38;5;241m.\u001b[39mrepeat(epochs)\n\u001b[0;32m    360\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m flat_dataset\n\u001b[1;32m--> 362\u001b[0m indices_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mindices_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mslice_batch_indices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    364\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mslice_inputs(indices_dataset, inputs)\n\u001b[0;32m    366\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shuffle \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mD:\\Program files\\Anaconda\\envs\\pong\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:1727\u001b[0m, in \u001b[0;36mDatasetV2.flat_map\u001b[1;34m(self, map_func)\u001b[0m\n\u001b[0;32m   1704\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mflat_map\u001b[39m(\u001b[38;5;28mself\u001b[39m, map_func):\n\u001b[0;32m   1705\u001b[0m   \u001b[38;5;124;03m\"\"\"Maps `map_func` across this dataset and flattens the result.\u001b[39;00m\n\u001b[0;32m   1706\u001b[0m \n\u001b[0;32m   1707\u001b[0m \u001b[38;5;124;03m  Use `flat_map` if you want to make sure that the order of your dataset\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1725\u001b[0m \u001b[38;5;124;03m    Dataset: A `Dataset`.\u001b[39;00m\n\u001b[0;32m   1726\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1727\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mFlatMapDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_func\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Program files\\Anaconda\\envs\\pong\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:4122\u001b[0m, in \u001b[0;36mFlatMapDataset.__init__\u001b[1;34m(self, input_dataset, map_func)\u001b[0m\n\u001b[0;32m   4120\u001b[0m \u001b[38;5;124;03m\"\"\"See `Dataset.flat_map()` for details.\"\"\"\u001b[39;00m\n\u001b[0;32m   4121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_dataset \u001b[38;5;241m=\u001b[39m input_dataset\n\u001b[1;32m-> 4122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_map_func \u001b[38;5;241m=\u001b[39m \u001b[43mStructuredFunctionWrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4123\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmap_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transformation_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4124\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_map_func\u001b[38;5;241m.\u001b[39moutput_structure, DatasetSpec):\n\u001b[0;32m   4125\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m   4126\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`map_func` must return a `Dataset` object. Got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   4127\u001b[0m           \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_map_func\u001b[38;5;241m.\u001b[39moutput_structure)))\n",
      "File \u001b[1;32mD:\\Program files\\Anaconda\\envs\\pong\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:3371\u001b[0m, in \u001b[0;36mStructuredFunctionWrapper.__init__\u001b[1;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[0;32m   3368\u001b[0m resource_tracker \u001b[38;5;241m=\u001b[39m tracking\u001b[38;5;241m.\u001b[39mResourceTracker()\n\u001b[0;32m   3369\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tracking\u001b[38;5;241m.\u001b[39mresource_tracker_scope(resource_tracker):\n\u001b[0;32m   3370\u001b[0m   \u001b[38;5;66;03m# TODO(b/141462134): Switch to using garbage collection.\u001b[39;00m\n\u001b[1;32m-> 3371\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function \u001b[38;5;241m=\u001b[39m \u001b[43mwrapper_fn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_concrete_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3372\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m add_to_graph:\n\u001b[0;32m   3373\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function\u001b[38;5;241m.\u001b[39madd_to_graph(ops\u001b[38;5;241m.\u001b[39mget_default_graph())\n",
      "File \u001b[1;32mD:\\Program files\\Anaconda\\envs\\pong\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2938\u001b[0m, in \u001b[0;36mFunction.get_concrete_function\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2931\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_concrete_function\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   2932\u001b[0m   \u001b[38;5;124;03m\"\"\"Returns a `ConcreteFunction` specialized to inputs and execution context.\u001b[39;00m\n\u001b[0;32m   2933\u001b[0m \n\u001b[0;32m   2934\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[0;32m   2935\u001b[0m \u001b[38;5;124;03m    *args: inputs to specialize on.\u001b[39;00m\n\u001b[0;32m   2936\u001b[0m \u001b[38;5;124;03m    **kwargs: inputs to specialize on.\u001b[39;00m\n\u001b[0;32m   2937\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2938\u001b[0m   graph_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_concrete_function_garbage_collected\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2939\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2940\u001b[0m   graph_function\u001b[38;5;241m.\u001b[39m_garbage_collector\u001b[38;5;241m.\u001b[39mrelease()  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   2941\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m graph_function\n",
      "File \u001b[1;32mD:\\Program files\\Anaconda\\envs\\pong\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2906\u001b[0m, in \u001b[0;36mFunction._get_concrete_function_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2904\u001b[0m   args, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   2905\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m-> 2906\u001b[0m   graph_function, args, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_define_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2907\u001b[0m   seen_names \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[0;32m   2908\u001b[0m   captured \u001b[38;5;241m=\u001b[39m object_identity\u001b[38;5;241m.\u001b[39mObjectIdentitySet(\n\u001b[0;32m   2909\u001b[0m       graph_function\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39minternal_captures)\n",
      "File \u001b[1;32mD:\\Program files\\Anaconda\\envs\\pong\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:3213\u001b[0m, in \u001b[0;36mFunction._maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3210\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_define_function_with_shape_relaxation(args, kwargs)\n\u001b[0;32m   3212\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function_cache\u001b[38;5;241m.\u001b[39mmissed\u001b[38;5;241m.\u001b[39madd(call_context_key)\n\u001b[1;32m-> 3213\u001b[0m graph_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_graph_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3214\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function_cache\u001b[38;5;241m.\u001b[39mprimary[cache_key] \u001b[38;5;241m=\u001b[39m graph_function\n\u001b[0;32m   3215\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m graph_function, args, kwargs\n",
      "File \u001b[1;32mD:\\Program files\\Anaconda\\envs\\pong\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:3065\u001b[0m, in \u001b[0;36mFunction._create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3060\u001b[0m missing_arg_names \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m   3061\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (arg, i) \u001b[38;5;28;01mfor\u001b[39;00m i, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(missing_arg_names)\n\u001b[0;32m   3062\u001b[0m ]\n\u001b[0;32m   3063\u001b[0m arg_names \u001b[38;5;241m=\u001b[39m base_arg_names \u001b[38;5;241m+\u001b[39m missing_arg_names\n\u001b[0;32m   3064\u001b[0m graph_function \u001b[38;5;241m=\u001b[39m ConcreteFunction(\n\u001b[1;32m-> 3065\u001b[0m     \u001b[43mfunc_graph_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc_graph_from_py_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3066\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3067\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_python_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3068\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3069\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3070\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_signature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3071\u001b[0m \u001b[43m        \u001b[49m\u001b[43mautograph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_autograph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3072\u001b[0m \u001b[43m        \u001b[49m\u001b[43mautograph_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_autograph_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3073\u001b[0m \u001b[43m        \u001b[49m\u001b[43marg_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43marg_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3074\u001b[0m \u001b[43m        \u001b[49m\u001b[43moverride_flat_arg_shapes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverride_flat_arg_shapes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3075\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapture_by_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_capture_by_value\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m   3076\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function_attributes,\n\u001b[0;32m   3077\u001b[0m     function_spec\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_spec,\n\u001b[0;32m   3078\u001b[0m     \u001b[38;5;66;03m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[39;00m\n\u001b[0;32m   3079\u001b[0m     \u001b[38;5;66;03m# scope. This is not the default behavior since it gets used in some\u001b[39;00m\n\u001b[0;32m   3080\u001b[0m     \u001b[38;5;66;03m# places (like Keras) where the FuncGraph lives longer than the\u001b[39;00m\n\u001b[0;32m   3081\u001b[0m     \u001b[38;5;66;03m# ConcreteFunction.\u001b[39;00m\n\u001b[0;32m   3082\u001b[0m     shared_func_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   3083\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m graph_function\n",
      "File \u001b[1;32mD:\\Program files\\Anaconda\\envs\\pong\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:986\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    983\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    984\u001b[0m   _, original_func \u001b[38;5;241m=\u001b[39m tf_decorator\u001b[38;5;241m.\u001b[39munwrap(python_func)\n\u001b[1;32m--> 986\u001b[0m func_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mpython_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfunc_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfunc_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    988\u001b[0m \u001b[38;5;66;03m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[39;00m\n\u001b[0;32m    989\u001b[0m \u001b[38;5;66;03m# TensorArrays and `None`s.\u001b[39;00m\n\u001b[0;32m    990\u001b[0m func_outputs \u001b[38;5;241m=\u001b[39m nest\u001b[38;5;241m.\u001b[39mmap_structure(convert, func_outputs,\n\u001b[0;32m    991\u001b[0m                                   expand_composites\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mD:\\Program files\\Anaconda\\envs\\pong\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:3364\u001b[0m, in \u001b[0;36mStructuredFunctionWrapper.__init__.<locals>.wrapper_fn\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m   3358\u001b[0m \u001b[38;5;129m@eager_function\u001b[39m\u001b[38;5;241m.\u001b[39mdefun_with_attributes(\n\u001b[0;32m   3359\u001b[0m     input_signature\u001b[38;5;241m=\u001b[39mstructure\u001b[38;5;241m.\u001b[39mget_flat_tensor_specs(\n\u001b[0;32m   3360\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_structure),\n\u001b[0;32m   3361\u001b[0m     autograph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   3362\u001b[0m     attributes\u001b[38;5;241m=\u001b[39mdefun_kwargs)\n\u001b[0;32m   3363\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper_fn\u001b[39m(\u001b[38;5;241m*\u001b[39margs):  \u001b[38;5;66;03m# pylint: disable=missing-docstring\u001b[39;00m\n\u001b[1;32m-> 3364\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43m_wrapper_helper\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3365\u001b[0m   ret \u001b[38;5;241m=\u001b[39m structure\u001b[38;5;241m.\u001b[39mto_tensor_list(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_structure, ret)\n\u001b[0;32m   3366\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m [ops\u001b[38;5;241m.\u001b[39mconvert_to_tensor(t) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m ret]\n",
      "File \u001b[1;32mD:\\Program files\\Anaconda\\envs\\pong\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:3299\u001b[0m, in \u001b[0;36mStructuredFunctionWrapper.__init__.<locals>._wrapper_helper\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m   3296\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _should_unpack_args(nested_args):\n\u001b[0;32m   3297\u001b[0m   nested_args \u001b[38;5;241m=\u001b[39m (nested_args,)\n\u001b[1;32m-> 3299\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[43mautograph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtf_convert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag_ctx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mnested_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3300\u001b[0m \u001b[38;5;66;03m# If `func` returns a list of tensors, `nest.flatten()` and\u001b[39;00m\n\u001b[0;32m   3301\u001b[0m \u001b[38;5;66;03m# `ops.convert_to_tensor()` would conspire to attempt to stack\u001b[39;00m\n\u001b[0;32m   3302\u001b[0m \u001b[38;5;66;03m# those tensors into a single tensor, because the customized\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3308\u001b[0m \u001b[38;5;66;03m# the return value into a single tensor can use an explicit\u001b[39;00m\n\u001b[0;32m   3309\u001b[0m \u001b[38;5;66;03m# `tf.stack()` before returning.\u001b[39;00m\n\u001b[0;32m   3310\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, \u001b[38;5;28mlist\u001b[39m):\n",
      "File \u001b[1;32mD:\\Program files\\Anaconda\\envs\\pong\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:255\u001b[0m, in \u001b[0;36mconvert.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    253\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    254\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m conversion_ctx:\n\u001b[1;32m--> 255\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[0;32m    257\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mag_error_metadata\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "File \u001b[1;32mD:\\Program files\\Anaconda\\envs\\pong\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:532\u001b[0m, in \u001b[0;36mconverted_call\u001b[1;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[0;32m    529\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _call_unconverted(f, args, kwargs, options)\n\u001b[0;32m    531\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m options\u001b[38;5;241m.\u001b[39muser_requested \u001b[38;5;129;01mand\u001b[39;00m conversion\u001b[38;5;241m.\u001b[39mis_whitelisted(f):\n\u001b[1;32m--> 532\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_call_unconverted\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    534\u001b[0m \u001b[38;5;66;03m# internal_convert_user_code is for example turned off when issuing a dynamic\u001b[39;00m\n\u001b[0;32m    535\u001b[0m \u001b[38;5;66;03m# call conversion from generated code while in nonrecursive mode. In that\u001b[39;00m\n\u001b[0;32m    536\u001b[0m \u001b[38;5;66;03m# case we evidently don't want to recurse, but we still have to convert\u001b[39;00m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;66;03m# things like builtins.\u001b[39;00m\n\u001b[0;32m    538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m options\u001b[38;5;241m.\u001b[39minternal_convert_user_code:\n",
      "File \u001b[1;32mD:\\Program files\\Anaconda\\envs\\pong\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:339\u001b[0m, in \u001b[0;36m_call_unconverted\u001b[1;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[0;32m    336\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m f\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__self__\u001b[39m\u001b[38;5;241m.\u001b[39mcall(args, kwargs)\n\u001b[0;32m    338\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 339\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    340\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs)\n",
      "File \u001b[1;32mD:\\Program files\\Anaconda\\envs\\pong\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py:353\u001b[0m, in \u001b[0;36mTensorLikeDataAdapter.__init__.<locals>.slice_batch_indices\u001b[1;34m(indices)\u001b[0m\n\u001b[0;32m    351\u001b[0m flat_dataset \u001b[38;5;241m=\u001b[39m dataset_ops\u001b[38;5;241m.\u001b[39mDatasetV2\u001b[38;5;241m.\u001b[39mfrom_tensor_slices(first_k_indices)\n\u001b[0;32m    352\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_partial_batch_size:\n\u001b[1;32m--> 353\u001b[0m   index_remainder \u001b[38;5;241m=\u001b[39m dataset_ops\u001b[38;5;241m.\u001b[39mDatasetV2\u001b[38;5;241m.\u001b[39mfrom_tensors(\u001b[43marray_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mslice\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m      \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mnum_in_full_batch\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_partial_batch_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    355\u001b[0m   flat_dataset \u001b[38;5;241m=\u001b[39m flat_dataset\u001b[38;5;241m.\u001b[39mconcatenate(index_remainder)\n\u001b[0;32m    357\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shuffle \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    358\u001b[0m   \u001b[38;5;66;03m# 1024 is a magic constant that has not been properly evaluated\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Program files\\Anaconda\\envs\\pong\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:201\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;124;03m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[39;00m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 201\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[0;32m    203\u001b[0m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[0;32m    204\u001b[0m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[0;32m    205\u001b[0m   result \u001b[38;5;241m=\u001b[39m dispatch(wrapper, args, kwargs)\n",
      "File \u001b[1;32mD:\\Program files\\Anaconda\\envs\\pong\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:1080\u001b[0m, in \u001b[0;36mslice\u001b[1;34m(input_, begin, size, name)\u001b[0m\n\u001b[0;32m   1028\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mslice\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1029\u001b[0m \u001b[38;5;129m@dispatch\u001b[39m\u001b[38;5;241m.\u001b[39madd_dispatch_support\n\u001b[0;32m   1030\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mslice\u001b[39m(input_, begin, size, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1031\u001b[0m   \u001b[38;5;66;03m# pylint: disable=redefined-builtin\u001b[39;00m\n\u001b[0;32m   1032\u001b[0m   \u001b[38;5;124;03m\"\"\"Extracts a slice from a tensor.\u001b[39;00m\n\u001b[0;32m   1033\u001b[0m \n\u001b[0;32m   1034\u001b[0m \u001b[38;5;124;03m  See also `tf.strided_slice`.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1078\u001b[0m \u001b[38;5;124;03m    A `Tensor` the same type as `input_`.\u001b[39;00m\n\u001b[0;32m   1079\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1080\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgen_array_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_slice\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbegin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Program files\\Anaconda\\envs\\pong\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py:9254\u001b[0m, in \u001b[0;36m_slice\u001b[1;34m(input, begin, size, name)\u001b[0m\n\u001b[0;32m   9252\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# Add nodes to the TensorFlow graph.\u001b[39;00m\n\u001b[0;32m   9253\u001b[0m \u001b[38;5;66;03m# Add nodes to the TensorFlow graph.\u001b[39;00m\n\u001b[1;32m-> 9254\u001b[0m _, _, _op, _outputs \u001b[38;5;241m=\u001b[39m \u001b[43m_op_def_library\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply_op_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   9255\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSlice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbegin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   9256\u001b[0m _result \u001b[38;5;241m=\u001b[39m _outputs[:]\n\u001b[0;32m   9257\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _execute\u001b[38;5;241m.\u001b[39mmust_record_gradient():\n",
      "File \u001b[1;32mD:\\Program files\\Anaconda\\envs\\pong\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:465\u001b[0m, in \u001b[0;36m_apply_op_helper\u001b[1;34m(op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    462\u001b[0m   default_dtype \u001b[38;5;241m=\u001b[39m default_type_attr_map[input_arg\u001b[38;5;241m.\u001b[39mtype_attr]\n\u001b[0;32m    464\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 465\u001b[0m   values \u001b[38;5;241m=\u001b[39m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    466\u001b[0m \u001b[43m      \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    467\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_arg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    468\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    469\u001b[0m \u001b[43m      \u001b[49m\u001b[43mas_ref\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_arg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_ref\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    470\u001b[0m \u001b[43m      \u001b[49m\u001b[43mpreferred_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_dtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    471\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    472\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mD:\\Program files\\Anaconda\\envs\\pong\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1499\u001b[0m, in \u001b[0;36mconvert_to_tensor\u001b[1;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[0;32m   1494\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconvert_to_tensor did not convert to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1495\u001b[0m                       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe preferred dtype: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m vs \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1496\u001b[0m                       (ret\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mbase_dtype, preferred_dtype\u001b[38;5;241m.\u001b[39mbase_dtype))\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1499\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mconversion_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_ref\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mas_ref\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1501\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[0;32m   1502\u001b[0m   \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Program files\\Anaconda\\envs\\pong\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:338\u001b[0m, in \u001b[0;36m_constant_tensor_conversion_function\u001b[1;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[0;32m    335\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_constant_tensor_conversion_function\u001b[39m(v, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    336\u001b[0m                                          as_ref\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    337\u001b[0m   _ \u001b[38;5;241m=\u001b[39m as_ref\n\u001b[1;32m--> 338\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconstant\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Program files\\Anaconda\\envs\\pong\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:263\u001b[0m, in \u001b[0;36mconstant\u001b[1;34m(value, dtype, shape, name)\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconstant\u001b[39m\u001b[38;5;124m\"\u001b[39m, v1\u001b[38;5;241m=\u001b[39m[])\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconstant\u001b[39m(value, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, shape\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConst\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    168\u001b[0m   \u001b[38;5;124;03m\"\"\"Creates a constant tensor from a tensor-like object.\u001b[39;00m\n\u001b[0;32m    169\u001b[0m \n\u001b[0;32m    170\u001b[0m \u001b[38;5;124;03m  Note: All eager `tf.Tensor` values are immutable (in contrast to\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    261\u001b[0m \u001b[38;5;124;03m    ValueError: if called on a symbolic tensor.\u001b[39;00m\n\u001b[0;32m    262\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 263\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_constant_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    264\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mallow_broadcast\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Program files\\Anaconda\\envs\\pong\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:285\u001b[0m, in \u001b[0;36m_constant_impl\u001b[1;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[0;32m    283\u001b[0m dtype_value \u001b[38;5;241m=\u001b[39m attr_value_pb2\u001b[38;5;241m.\u001b[39mAttrValue(\u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39mtensor_value\u001b[38;5;241m.\u001b[39mtensor\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m    284\u001b[0m attrs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m: tensor_value, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: dtype_value}\n\u001b[1;32m--> 285\u001b[0m const_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_op_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    286\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mConst\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mdtype_value\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtype\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39moutputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m op_callbacks\u001b[38;5;241m.\u001b[39mshould_invoke_op_callbacks():\n\u001b[0;32m    289\u001b[0m   \u001b[38;5;66;03m# TODO(b/147670703): Once the special-op creation code paths\u001b[39;00m\n\u001b[0;32m    290\u001b[0m   \u001b[38;5;66;03m# are unified. Remove this `if` block.\u001b[39;00m\n\u001b[0;32m    291\u001b[0m   callback_outputs \u001b[38;5;241m=\u001b[39m op_callbacks\u001b[38;5;241m.\u001b[39minvoke_op_callbacks(\n\u001b[0;32m    292\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConst\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mtuple\u001b[39m(), attrs, (const_tensor,), op_name\u001b[38;5;241m=\u001b[39mname, graph\u001b[38;5;241m=\u001b[39mg)\n",
      "File \u001b[1;32mD:\\Program files\\Anaconda\\envs\\pong\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:591\u001b[0m, in \u001b[0;36mFuncGraph._create_op_internal\u001b[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[0;32m    589\u001b[0m   inp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcapture(inp)\n\u001b[0;32m    590\u001b[0m   inputs[i] \u001b[38;5;241m=\u001b[39m inp\n\u001b[1;32m--> 591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mFuncGraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_op_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    592\u001b[0m \u001b[43m    \u001b[49m\u001b[43mop_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_types\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_def\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    593\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute_device\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Program files\\Anaconda\\envs\\pong\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:3477\u001b[0m, in \u001b[0;36mGraph._create_op_internal\u001b[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[0;32m   3474\u001b[0m \u001b[38;5;66;03m# _create_op_helper mutates the new Operation. `_mutation_lock` ensures a\u001b[39;00m\n\u001b[0;32m   3475\u001b[0m \u001b[38;5;66;03m# Session.run call cannot occur between creating and mutating the op.\u001b[39;00m\n\u001b[0;32m   3476\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mutation_lock():\n\u001b[1;32m-> 3477\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mOperation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3478\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnode_def\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3479\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3480\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3481\u001b[0m \u001b[43m      \u001b[49m\u001b[43moutput_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3482\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcontrol_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontrol_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3483\u001b[0m \u001b[43m      \u001b[49m\u001b[43minput_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3484\u001b[0m \u001b[43m      \u001b[49m\u001b[43moriginal_op\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_default_original_op\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3485\u001b[0m \u001b[43m      \u001b[49m\u001b[43mop_def\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mop_def\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3486\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_op_helper(ret, compute_device\u001b[38;5;241m=\u001b[39mcompute_device)\n\u001b[0;32m   3487\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "File \u001b[1;32mD:\\Program files\\Anaconda\\envs\\pong\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1974\u001b[0m, in \u001b[0;36mOperation.__init__\u001b[1;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[0;32m   1972\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m op_def \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1973\u001b[0m     op_def \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph\u001b[38;5;241m.\u001b[39m_get_op_def(node_def\u001b[38;5;241m.\u001b[39mop)\n\u001b[1;32m-> 1974\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_c_op \u001b[38;5;241m=\u001b[39m \u001b[43m_create_c_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode_def\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1975\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mcontrol_input_ops\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_def\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1976\u001b[0m   name \u001b[38;5;241m=\u001b[39m compat\u001b[38;5;241m.\u001b[39mas_str(node_def\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m   1977\u001b[0m \u001b[38;5;66;03m# pylint: enable=protected-access\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Program files\\Anaconda\\envs\\pong\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1785\u001b[0m, in \u001b[0;36m_create_c_op\u001b[1;34m(graph, node_def, inputs, control_inputs, op_def)\u001b[0m\n\u001b[0;32m   1783\u001b[0m inputs \u001b[38;5;241m=\u001b[39m _reconstruct_sequence_inputs(op_def, inputs, node_def\u001b[38;5;241m.\u001b[39mattr)\n\u001b[0;32m   1784\u001b[0m \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m-> 1785\u001b[0m op_desc \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tf_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTF_NewOperation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_c_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1786\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mcompat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_str\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode_def\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1787\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mcompat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_str\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode_def\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m node_def\u001b[38;5;241m.\u001b[39mdevice:\n\u001b[0;32m   1789\u001b[0m   pywrap_tf_session\u001b[38;5;241m.\u001b[39mTF_SetDevice(op_desc, compat\u001b[38;5;241m.\u001b[39mas_str(node_def\u001b[38;5;241m.\u001b[39mdevice))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = []\n",
    "observation = env.reset()\n",
    "prev_input = None\n",
    "# main training loop\n",
    "while (True):\n",
    "\n",
    "    \"\"\"\n",
    "    Start by preprocessing the observation frame, and then doing the difference with the previous frame. \n",
    "    Naturally if frame 1 then we subtract by zeros\n",
    "    \n",
    "    X here is the frame-frame difference, Y is the next action (kind of like an RNN). \n",
    "    We are trying to predict the next action given two observations.\n",
    "    \"\"\"\n",
    "    cur_input = prepro(observation)\n",
    "    #print(len(cur_input)) - Sanity Check reasons only\n",
    "    \n",
    "    x = cur_input - prev_input if prev_input is not None else np.zeros(80 * 80)\n",
    "    prev_input = cur_input\n",
    "    \n",
    "    # forward the policy network and sample action according to the probability distribution\n",
    "    \"\"\"\n",
    "    Running model.predict to know what the current model thinks about the probability of doing the UP_ACTION, \n",
    "    given the current frame setting.\n",
    "    \n",
    "    Double check size and shape of the array here im pretty sure its for bias term?\n",
    "    Keras requires a third?? dimension perhaps hre\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    proba = model.predict(np.expand_dims(x, axis=1).T)\n",
    "    # proba=model.predict(np.expand_dims(x.reshape(80,80), axis=0)) 2D model stuff DELETE\n",
    "    \n",
    "    \n",
    "    #Intorucing another probability distirubtion here, not sure\n",
    "    action = UP_ACTION if np.random.uniform() < proba else DOWN_ACTION\n",
    "    y = 1 if action == 2 else 0 # 0 and 1 are our labels\n",
    "\n",
    "    # log the input and label to train later\n",
    "    x_train.append(x)\n",
    "    y_train.append(y)\n",
    "\n",
    "    # do one step in our environment - This is returned by our environment in OpenAI gym.  \n",
    "    observation, reward, done, info = env.step(action)\n",
    "    #Note how AT EACH STEP A REWARD IS CALCULATED. THIS IS NOT EACH GAME BUT EACH FRAME-FRAME DIFFERENCE. \n",
    "    #MOST OF THE TIME THIS IS 0\n",
    "    #THESE REWARDS ARE USED TO ENCOURAGE OR DISCOURAGE MOVEMENTS\n",
    "    rewards.append(reward)\n",
    "    reward_sum += reward\n",
    "    \n",
    "    \"\"\"\n",
    "    rewards : to each frame (x_train[frame_number]) and action (y_train[frame_number]) \n",
    "    is associated a reward (-1 if it missed the ball, 0 if nothing happens, and 1 if opponent misses the ball), \n",
    "    so we get for instance the following array:\n",
    "    \"\"\"\n",
    "    \n",
    "    # end of an episode - The GYM also invokes DONE automatically. Invoked when one player reaches 21\n",
    "    if done:\n",
    "        \n",
    "        history.append(reward_sum)\n",
    "        print('At the end of episode', episode_nb, 'the total reward was :', reward_sum)\n",
    "        if episode_nb>=3000 and reward_sum >=-12:\n",
    "          break\n",
    "        else:\n",
    "          \n",
    "        \n",
    "          # increment episode number\n",
    "          episode_nb += 1\n",
    "        \n",
    "          # training\n",
    "          model.fit(x=np.vstack(x_train), y=np.vstack(y_train), verbose=1, sample_weight=discount_rewards(rewards, gamma))\n",
    "        \n",
    "          \"\"\"\n",
    "          If an action leads to a positive reward, it tunes the weights of the neural network so \n",
    "          it keeps on predicting this winning action.\n",
    "          Otherwise, it tunes them in the opposite way\n",
    "\n",
    "\n",
    "          The function discount_rewards transforms the list of rewards \n",
    "          so that even actions that remotely lead to positive rewards are encouraged. THIS IS IMPORTANT\n",
    "          OTHERWISE WE WOULD SIMPLY BE TRYING TO REPLICATE RANDOM MOVEMENTS\n",
    "\n",
    "          sample_weights is used to provide a weight for each training sample. \n",
    "          That means that you should pass a 1D array with the same number of elements as your training samples \n",
    "          (indicating the weight for each of those samples. NOTE THIS IS NOT CLASS WEIGHTS\n",
    "          \"\"\"\n",
    "                                                             \n",
    "          # Reinitialization\n",
    "          x_train, y_train, rewards = [],[],[]\n",
    "          observation = env.reset()\n",
    "          reward_sum = 0\n",
    "          prev_input = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cc6dfae3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot results - remember to call keyboard interrupt before this\n",
    "\n",
    "plt.plot(history)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "201e5381",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 2] The system cannot find the file specified",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [43]\u001b[0m, in \u001b[0;36m<cell line: 23>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mIPython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m display \u001b[38;5;28;01mas\u001b[39;00m ipythondisplay\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyvirtualdisplay\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Display\n\u001b[1;32m---> 23\u001b[0m display \u001b[38;5;241m=\u001b[39m \u001b[43mDisplay\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvisible\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m768\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m display\u001b[38;5;241m.\u001b[39mstart()\n",
      "File \u001b[1;32mD:\\Program files\\Anaconda\\envs\\pong\\lib\\site-packages\\pyvirtualdisplay\\display.py:54\u001b[0m, in \u001b[0;36mDisplay.__init__\u001b[1;34m(self, backend, visible, size, color_depth, bgcolor, use_xauth, retries, extra_args, manage_global_env, **kwargs)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcls\u001b[39m:\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munknown backend: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend)\n\u001b[1;32m---> 54\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolor_depth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolor_depth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbgcolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbgcolor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_xauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_xauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# check_startup=check_startup,\u001b[39;49;00m\n\u001b[0;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43mextra_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmanage_global_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanage_global_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Program files\\Anaconda\\envs\\pong\\lib\\site-packages\\pyvirtualdisplay\\xvfb.py:44\u001b[0m, in \u001b[0;36mXvfbDisplay.__init__\u001b[1;34m(self, size, color_depth, bgcolor, use_xauth, fbdir, dpi, retries, extra_args, manage_global_env)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fbdir \u001b[38;5;241m=\u001b[39m fbdir\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dpi \u001b[38;5;241m=\u001b[39m dpi\n\u001b[1;32m---> 44\u001b[0m \u001b[43mAbstractDisplay\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mPROGRAM\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_xauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_xauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mextra_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmanage_global_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanage_global_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Program files\\Anaconda\\envs\\pong\\lib\\site-packages\\pyvirtualdisplay\\abstractdisplay.py:85\u001b[0m, in \u001b[0;36mAbstractDisplay.__init__\u001b[1;34m(self, program, use_xauth, retries, extra_args, manage_global_env)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pipe_wfd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retries_current \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 85\u001b[0m helptext \u001b[38;5;241m=\u001b[39m \u001b[43mget_helptext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprogram\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_displayfd \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-displayfd\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m helptext\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_displayfd:\n",
      "File \u001b[1;32mD:\\Program files\\Anaconda\\envs\\pong\\lib\\site-packages\\pyvirtualdisplay\\util.py:13\u001b[0m, in \u001b[0;36mget_helptext\u001b[1;34m(program)\u001b[0m\n\u001b[0;32m      6\u001b[0m cmd \u001b[38;5;241m=\u001b[39m [program, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-help\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# py3.7+\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# p = subprocess.run(cmd, capture_output=True)\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# stderr = p.stderr\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# py3.6 also\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m p \u001b[38;5;241m=\u001b[39m \u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstdout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPIPE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstderr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPIPE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshell\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m _, stderr \u001b[38;5;241m=\u001b[39m p\u001b[38;5;241m.\u001b[39mcommunicate()\n\u001b[0;32m     21\u001b[0m helptext \u001b[38;5;241m=\u001b[39m stderr\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mD:\\Program files\\Anaconda\\envs\\pong\\lib\\subprocess.py:858\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors, text)\u001b[0m\n\u001b[0;32m    854\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_mode:\n\u001b[0;32m    855\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mTextIOWrapper(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr,\n\u001b[0;32m    856\u001b[0m                     encoding\u001b[38;5;241m=\u001b[39mencoding, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m--> 858\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreexec_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    859\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mpass_fds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    860\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshell\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    861\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mp2cread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp2cwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    862\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mc2pread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2pwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    863\u001b[0m \u001b[43m                        \u001b[49m\u001b[43merrread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    864\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mrestore_signals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_new_session\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    865\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m    866\u001b[0m     \u001b[38;5;66;03m# Cleanup if the child failed starting.\u001b[39;00m\n\u001b[0;32m    867\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstdin, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstdout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr)):\n",
      "File \u001b[1;32mD:\\Program files\\Anaconda\\envs\\pong\\lib\\subprocess.py:1311\u001b[0m, in \u001b[0;36mPopen._execute_child\u001b[1;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_start_new_session)\u001b[0m\n\u001b[0;32m   1309\u001b[0m \u001b[38;5;66;03m# Start the process\u001b[39;00m\n\u001b[0;32m   1310\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1311\u001b[0m     hp, ht, pid, tid \u001b[38;5;241m=\u001b[39m \u001b[43m_winapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCreateProcess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1312\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;66;43;03m# no special security\u001b[39;49;00m\n\u001b[0;32m   1313\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1314\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1315\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1316\u001b[0m \u001b[43m                             \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1317\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1318\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;66;03m# Child is launched. Close the parent's copy of those pipe\u001b[39;00m\n\u001b[0;32m   1321\u001b[0m     \u001b[38;5;66;03m# handles that only the child should have open.  You need\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1324\u001b[0m     \u001b[38;5;66;03m# pipe will not close when the child process exits and the\u001b[39;00m\n\u001b[0;32m   1325\u001b[0m     \u001b[38;5;66;03m# ReadFile will hang.\u001b[39;00m\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_pipe_fds(p2cread, p2cwrite,\n\u001b[0;32m   1327\u001b[0m                          c2pread, c2pwrite,\n\u001b[0;32m   1328\u001b[0m                          errread, errwrite)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] The system cannot find the file specified"
     ]
    }
   ],
   "source": [
    "#To Evaluate model on OpenAI gym, we will record a video via Ipython display\n",
    "\n",
    "import gym\n",
    "from gym import logger as gymlogger\n",
    "# from gym.wrappers import Monitor\n",
    "gymlogger.set_level(40) #error only\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import math\n",
    "import glob\n",
    "import io\n",
    "import base64\n",
    "from IPython.display import HTML\n",
    "\n",
    "from IPython import display as ipythondisplay\n",
    "\n",
    "from pyvirtualdisplay import Display\n",
    "\n",
    "display = Display(visible=0, size=(1024, 768))\n",
    "display.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "57c3c7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Utility functions to enable video recording of gym environment and displaying it\n",
    "To enable video, just do \"env = wrap_env(env)\"\"\n",
    "\"\"\"\n",
    "\n",
    "def show_video():\n",
    "  mp4list = glob.glob('video/*.mp4')\n",
    "  if len(mp4list) > 0:\n",
    "    mp4 = mp4list[0]\n",
    "    video = io.open(mp4, 'r+b').read()\n",
    "    encoded = base64.b64encode(video)\n",
    "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
    "                loop controls style=\"height: 400px;\">\n",
    "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "             </video>'''.format(encoded.decode('ascii'))))\n",
    "  else: \n",
    "    print(\"Could not find video\")\n",
    "    \n",
    "\n",
    "def wrap_env(env):\n",
    "  env = Monitor(env, './video', force=True)\n",
    "  return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "70383b31",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Monitor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [29]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#Evaluate model on openAi GYM\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#To do this consult https://github.com/thinkingparticle/deep_rl_pong_keras/blob/master/reinforcement_learning_pong_keras_policy_gradients.ipynb\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43mwrap_env\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgym\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mALE/Pong-v5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m observation \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m      6\u001b[0m new_observation \u001b[38;5;241m=\u001b[39m observation\n",
      "Input \u001b[1;32mIn [27]\u001b[0m, in \u001b[0;36mwrap_env\u001b[1;34m(env)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrap_env\u001b[39m(env):\n\u001b[1;32m---> 21\u001b[0m   env \u001b[38;5;241m=\u001b[39m \u001b[43mMonitor\u001b[49m(env, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./video\u001b[39m\u001b[38;5;124m'\u001b[39m, force\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     22\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m env\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Monitor' is not defined"
     ]
    }
   ],
   "source": [
    "#Evaluate model on openAi GYM\n",
    "\n",
    "#To do this consult https://github.com/thinkingparticle/deep_rl_pong_keras/blob/master/reinforcement_learning_pong_keras_policy_gradients.ipynb\n",
    "env = wrap_env(gym.make('ALE/Pong-v5'))\n",
    "observation = env.reset()\n",
    "new_observation = observation\n",
    "prev_input = None\n",
    "done = False\n",
    "while True:\n",
    "  if True: \n",
    "    \n",
    "    #set input to network to be difference image\n",
    "  \n",
    "    cur_input = prepro(observation)\n",
    "    x = cur_input - prev_input if prev_input is not None else np.zeros(80 * 80)\n",
    "    prev_input = cur_input\n",
    "  \n",
    "    # Sample an action (policy)\n",
    "    proba = model.predict(np.expand_dims(x, axis=1).T)\n",
    "    action = UP_ACTION if np.random.uniform() < proba else DOWN_ACTION\n",
    "        \n",
    "    env.render()\n",
    "    # Return action to environment and extract\n",
    "    #next observation, reward, and status\n",
    "    observation = new_observation\n",
    "    new_observation, reward, done, info = env.step(action)\n",
    "    if done: \n",
    "      #observation = env.reset()\n",
    "      break\n",
    "      \n",
    "env.close()\n",
    "show_video()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5ae1ce0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyvirtualdisplay.display.Display'>\n"
     ]
    }
   ],
   "source": [
    "from pyvirtualdisplay.display import Display\n",
    "print(Display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8387920b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
