{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce32b9b5",
   "metadata": {},
   "source": [
    "conda create -n pong numpy tensorflow keras jupyter notebook matplotlib\n",
    "\n",
    "pip install --upgrade pip --user\n",
    "\n",
    "pip install gym\n",
    "\n",
    "pip install gym[atari]\n",
    "\n",
    "pip install ale-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7b06c9f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym in d:\\program files\\anaconda\\envs\\pong\\lib\\site-packages (0.23.1)\n",
      "Requirement already satisfied: pyvirtualdisplay in d:\\program files\\anaconda\\envs\\pong\\lib\\site-packages (3.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in d:\\program files\\anaconda\\envs\\pong\\lib\\site-packages (from gym) (1.6.0)\n",
      "Requirement already satisfied: numpy>=1.18.0 in d:\\program files\\anaconda\\envs\\pong\\lib\\site-packages (from gym) (1.21.5)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in d:\\program files\\anaconda\\envs\\pong\\lib\\site-packages (from gym) (0.0.6)\n",
      "Requirement already satisfied: importlib-metadata>=4.10.0 in d:\\program files\\anaconda\\envs\\pong\\lib\\site-packages (from gym) (4.11.3)\n",
      "Requirement already satisfied: zipp>=0.5 in d:\\program files\\anaconda\\envs\\pong\\lib\\site-packages (from importlib-metadata>=4.10.0->gym) (3.7.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'apt-get' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'apt-get' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'apt-get' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: setuptools in d:\\program files\\anaconda\\envs\\pong\\lib\\site-packages (62.0.0)\n",
      "Requirement already satisfied: ez_setup in d:\\program files\\anaconda\\envs\\pong\\lib\\site-packages (0.9)\n",
      "Requirement already satisfied: gym[atari] in d:\\program files\\anaconda\\envs\\pong\\lib\\site-packages (0.23.1)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in d:\\program files\\anaconda\\envs\\pong\\lib\\site-packages (from gym[atari]) (0.0.6)\n",
      "Requirement already satisfied: importlib-metadata>=4.10.0 in d:\\program files\\anaconda\\envs\\pong\\lib\\site-packages (from gym[atari]) (4.11.3)\n",
      "Requirement already satisfied: numpy>=1.18.0 in d:\\program files\\anaconda\\envs\\pong\\lib\\site-packages (from gym[atari]) (1.21.5)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in d:\\program files\\anaconda\\envs\\pong\\lib\\site-packages (from gym[atari]) (1.6.0)\n",
      "Requirement already satisfied: ale-py~=0.7.4 in d:\\program files\\anaconda\\envs\\pong\\lib\\site-packages (from gym[atari]) (0.7.4)\n",
      "Requirement already satisfied: importlib-resources in d:\\program files\\anaconda\\envs\\pong\\lib\\site-packages (from ale-py~=0.7.4->gym[atari]) (5.4.0)\n",
      "Requirement already satisfied: zipp>=0.5 in d:\\program files\\anaconda\\envs\\pong\\lib\\site-packages (from importlib-metadata>=4.10.0->gym[atari]) (3.7.0)\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies first for graphics visualization within Colaboratory\n",
    "\n",
    "#remove \" > /dev/null 2>&1\" to see what is going on under the hood\n",
    "!pip install gym pyvirtualdisplay \n",
    "!apt-get install -y xvfb python-opengl ffmpeg \n",
    "\n",
    "!apt-get update \n",
    "!apt-get install cmake \n",
    "!pip install --upgrade setuptools \n",
    "!pip install ez_setup \n",
    "!pip install gym[atari]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e4ca0a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "\n",
    "# gym initialization\n",
    "\n",
    "env = gym.make(\"ALE/Pong-v5\")\n",
    "observation = env.reset()\n",
    "prev_input = None\n",
    "\n",
    "# Declaring the two actions that can happen in Pong for an agent, move up or move down\n",
    "# Decalring 0 means staying still. Note that this is pre-defined specific to package.\n",
    "UP_ACTION = 2\n",
    "DOWN_ACTION = 3\n",
    "\n",
    "# Hyperparameters. Gamma here allows you to measure the effect of future events\n",
    "gamma = 0.99\n",
    "\n",
    "# initialization of variables used in the main loop\n",
    "x_train, y_train, rewards = [],[],[]\n",
    "reward_sum = 0\n",
    "episode_nb = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3969397d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gym' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43mgym\u001b[49m\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mALE/Pong-v5\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;66;03m# environment info\u001b[39;00m\n\u001b[0;32m      9\u001b[0m observation \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# The ball is released after 20 frames\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'gym' is not defined"
     ]
    }
   ],
   "source": [
    "#Let's take a look at the game in action.\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "env = gym.make(\"ALE/Pong-v5\") # environment info\n",
    "observation = env.reset()\n",
    "# The ball is released after 20 frames\n",
    "for i in range(25):\n",
    "  \n",
    "  if i > 20:\n",
    "    plt.imshow(observation)\n",
    "    plt.show()\n",
    "\n",
    "  observation, _, _, _ = env.step(1)\n",
    "  \n",
    "  \n",
    "  \n",
    "  #Can plot more later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7021c376",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing function\n",
    "\n",
    "\n",
    "def prepro(I):\n",
    "  \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector \"\"\"\n",
    "  I = I[35:195] # crop\n",
    "  I = I[::2,::2,0] # downsample by factor of 2\n",
    "  I[I == 144] = 0 # erase background (background type 1)\n",
    "  I[I == 109] = 0 # erase background (background type 2)\n",
    "  I[I != 0] = 1 # everything else (paddles, ball) just set to 1\n",
    "  return I.astype(float).ravel()\n",
    "  #Can plot more later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "342b9da5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Show preprocessed\n",
    "\n",
    "obs_preprocessed = prepro(observation).reshape(80,80)\n",
    "plt.imshow(obs_preprocessed, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "520b9b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Karpathy Suport class\n",
    "\n",
    "\n",
    "# reward discount used by Karpathy (cf. https://gist.github.com/karpathy/a4166c7fe253700972fcbc77e4ea32c5)\n",
    "def discount_rewards(r, gamma):\n",
    "  \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
    "  r = np.array(r)\n",
    "  discounted_r = np.zeros_like(r)\n",
    "  running_add = 0\n",
    "  # we go from last reward to first one so we don't have to do exponentiations\n",
    "  for t in reversed(range(0, r.size)):\n",
    "    if r[t] != 0: running_add = 0 # if the game ended (in Pong), reset the reward sum\n",
    "    running_add = running_add * gamma + r[t] # the point here is to use Horner's method to compute those rewards efficiently\n",
    "    discounted_r[t] = running_add\n",
    "  discounted_r -= np.mean(discounted_r) #normalizing the result\n",
    "  discounted_r /= np.std(discounted_r) #idem using standar deviation\n",
    "  return discounted_r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c7d76394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 200)               1280200   \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 201       \n",
      "=================================================================\n",
      "Total params: 1,280,401\n",
      "Trainable params: 1,280,401\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# import necessary modules from keras\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import Flatten\n",
    "from keras.models import Sequential\n",
    "import keras\n",
    "from keras.models import InputLayer\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# creates a generic neural network architecture\n",
    "\n",
    "\"\"\"\n",
    "The 80 * 80 input dimension comes from the pre-processing of the raw pixels made by Karpathy (the only important pixels are the balls and the paddle)\n",
    "Input here represents the difference in pixels betewen one frame and another, giving you direction of agents and ball. Encoded in Karpathy's own preprocessing functions\n",
    "\n",
    "TODO, try adding a 400 layer infront of the 200 unit layer\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# hidden layer takes a pre-processed frame as input, and has 200 units. Simple layer architectur of 200 x1, 1x1\n",
    "model.add(Dense(units=200,input_dim=80*80, activation='relu', kernel_initializer='glorot_uniform'))\n",
    "\n",
    "# output layer - we use a Sigmoid here, in order to get a 0, or 1 value to represent ACTION UP\n",
    "model.add(Dense(units=1, activation='sigmoid', kernel_initializer='RandomNormal'))\n",
    "\n",
    "# compile the model using traditional Machine Learning losses and optimizers\n",
    "model.compile(loss='binary_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d6f4879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6400\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[[0.5]]\n"
     ]
    }
   ],
   "source": [
    "history = []\n",
    "observation = env.reset()\n",
    "prev_input = None\n",
    "\n",
    "#sanity checks\n",
    "cur_input = prepro(observation)\n",
    "print(len(cur_input))\n",
    "x = cur_input - prev_input if prev_input is not None else np.zeros(80 * 80)\n",
    "prev_input = cur_input\n",
    "print(x)\n",
    "\n",
    "proba = model.predict(np.expand_dims(x, axis=1).T)\n",
    "print(proba)\n",
    "\n",
    "proba = model.predict(np.expand_dims(x, axis=1).T)\n",
    "action = UP_ACTION if np.random.uniform() < proba else DOWN_ACTION\n",
    "y = 1 if action == 2 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b3706a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At the end of episode 0 the total reward was : -21.0\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 8.4374e-04 - accuracy: 0.5128\n",
      "At the end of episode 1 the total reward was : -19.0\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.0071 - accuracy: 0.5443\n",
      "At the end of episode 2 the total reward was : -21.0\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0030 - accuracy: 0.5974\n",
      "At the end of episode 3 the total reward was : -21.0\n",
      "30/30 [==============================] - 0s 7ms/step - loss: -0.0029 - accuracy: 0.5918\n",
      "At the end of episode 4 the total reward was : -20.0\n",
      "35/35 [==============================] - 0s 7ms/step - loss: 0.0087 - accuracy: 0.5675\n",
      "At the end of episode 5 the total reward was : -21.0\n",
      "38/38 [==============================] - 0s 7ms/step - loss: -0.0048 - accuracy: 0.5592\n",
      "At the end of episode 6 the total reward was : -21.0\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.0088 - accuracy: 0.6045\n",
      "At the end of episode 7 the total reward was : -21.0\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.0102 - accuracy: 0.6163\n",
      "At the end of episode 8 the total reward was : -21.0\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.0085 - accuracy: 0.6046\n",
      "At the end of episode 9 the total reward was : -20.0\n",
      "39/39 [==============================] - 0s 7ms/step - loss: -0.0036 - accuracy: 0.6077\n",
      "At the end of episode 10 the total reward was : -21.0\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.0065 - accuracy: 0.6304TA: 0s - loss: 0.0349 - accuracy: 0\n",
      "At the end of episode 11 the total reward was : -21.0\n",
      "30/30 [==============================] - 0s 7ms/step - loss: -2.7762e-05 - accuracy: 0.6142\n",
      "At the end of episode 12 the total reward was : -21.0\n",
      "34/34 [==============================] - 0s 7ms/step - loss: 0.0038 - accuracy: 0.6463\n",
      "At the end of episode 13 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.0158 - accuracy: 0.6614\n",
      "At the end of episode 14 the total reward was : -21.0\n",
      "34/34 [==============================] - 0s 7ms/step - loss: -0.0033 - accuracy: 0.6082\n",
      "At the end of episode 15 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 6ms/step - loss: -0.0148 - accuracy: 0.6119\n",
      "At the end of episode 16 the total reward was : -21.0\n",
      "36/36 [==============================] - 0s 7ms/step - loss: -0.0042 - accuracy: 0.6233\n",
      "At the end of episode 17 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.0089 - accuracy: 0.6247\n",
      "At the end of episode 18 the total reward was : -20.0\n",
      "27/27 [==============================] - 0s 6ms/step - loss: -0.0050 - accuracy: 0.6209\n",
      "At the end of episode 19 the total reward was : -19.0\n",
      "36/36 [==============================] - 0s 6ms/step - loss: -0.0115 - accuracy: 0.6561\n",
      "At the end of episode 20 the total reward was : -21.0\n",
      "32/32 [==============================] - 0s 7ms/step - loss: -0.0153 - accuracy: 0.6400\n",
      "At the end of episode 21 the total reward was : -21.0\n",
      "26/26 [==============================] - 0s 6ms/step - loss: -0.0095 - accuracy: 0.6707\n",
      "At the end of episode 22 the total reward was : -18.0\n",
      "41/41 [==============================] - 0s 6ms/step - loss: 0.0026 - accuracy: 0.6718\n",
      "At the end of episode 23 the total reward was : -20.0\n",
      "41/41 [==============================] - 0s 7ms/step - loss: -0.0057 - accuracy: 0.6778\n",
      "At the end of episode 24 the total reward was : -21.0\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 0.0146 - accuracy: 0.6881\n",
      "At the end of episode 25 the total reward was : -20.0\n",
      "37/37 [==============================] - 0s 6ms/step - loss: -0.0127 - accuracy: 0.6856\n",
      "At the end of episode 26 the total reward was : -21.0\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.0167 - accuracy: 0.6917\n",
      "At the end of episode 27 the total reward was : -17.0\n",
      "52/52 [==============================] - 0s 7ms/step - loss: -0.0101 - accuracy: 0.6889\n",
      "At the end of episode 28 the total reward was : -21.0\n",
      "36/36 [==============================] - 0s 6ms/step - loss: -0.0339 - accuracy: 0.7050\n",
      "At the end of episode 29 the total reward was : -19.0\n",
      "36/36 [==============================] - 0s 6ms/step - loss: -0.0229 - accuracy: 0.7339\n",
      "At the end of episode 30 the total reward was : -19.0\n",
      "34/34 [==============================] - 0s 7ms/step - loss: -0.0117 - accuracy: 0.7835\n",
      "At the end of episode 31 the total reward was : -18.0\n",
      "43/43 [==============================] - 0s 7ms/step - loss: -0.0221 - accuracy: 0.7526\n",
      "At the end of episode 32 the total reward was : -16.0\n",
      "47/47 [==============================] - 0s 6ms/step - loss: -0.0346 - accuracy: 0.7945\n",
      "At the end of episode 33 the total reward was : -20.0\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 0.0012 - accuracy: 0.8113\n",
      "At the end of episode 34 the total reward was : -18.0\n",
      "41/41 [==============================] - 0s 6ms/step - loss: -0.0132 - accuracy: 0.8194\n",
      "At the end of episode 35 the total reward was : -20.0\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.0014 - accuracy: 0.7998\n",
      "At the end of episode 36 the total reward was : -20.0\n",
      "37/37 [==============================] - 0s 6ms/step - loss: -0.0101 - accuracy: 0.8051\n",
      "At the end of episode 37 the total reward was : -21.0\n",
      "42/42 [==============================] - 0s 6ms/step - loss: -0.0148 - accuracy: 0.8128\n",
      "At the end of episode 38 the total reward was : -21.0\n",
      "38/38 [==============================] - 0s 7ms/step - loss: -0.0141 - accuracy: 0.8164\n",
      "At the end of episode 39 the total reward was : -20.0\n",
      "37/37 [==============================] - 0s 6ms/step - loss: -0.0142 - accuracy: 0.8292\n",
      "At the end of episode 40 the total reward was : -21.0\n",
      "44/44 [==============================] - 0s 7ms/step - loss: -0.0019 - accuracy: 0.8244\n",
      "At the end of episode 41 the total reward was : -21.0\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0097 - accuracy: 0.8292\n",
      "At the end of episode 42 the total reward was : -20.0\n",
      "41/41 [==============================] - 0s 7ms/step - loss: -0.0195 - accuracy: 0.8383\n",
      "At the end of episode 43 the total reward was : -21.0\n",
      "34/34 [==============================] - 0s 6ms/step - loss: -0.0082 - accuracy: 0.8400\n",
      "At the end of episode 44 the total reward was : -20.0\n",
      "31/31 [==============================] - 0s 7ms/step - loss: -9.3902e-04 - accuracy: 0.8310\n",
      "At the end of episode 45 the total reward was : -19.0\n",
      "51/51 [==============================] - 0s 7ms/step - loss: -0.0097 - accuracy: 0.8461\n",
      "At the end of episode 46 the total reward was : -18.0\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.0034 - accuracy: 0.8620\n",
      "At the end of episode 47 the total reward was : -19.0\n",
      "36/36 [==============================] - 0s 6ms/step - loss: -0.0216 - accuracy: 0.8776\n",
      "At the end of episode 48 the total reward was : -19.0\n",
      "40/40 [==============================] - 0s 7ms/step - loss: -0.0117 - accuracy: 0.8475\n",
      "At the end of episode 49 the total reward was : -21.0\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 0.0165 - accuracy: 0.8899\n",
      "At the end of episode 50 the total reward was : -18.0\n",
      "43/43 [==============================] - 0s 6ms/step - loss: -0.0108 - accuracy: 0.8727\n",
      "At the end of episode 51 the total reward was : -20.0\n",
      "41/41 [==============================] - 0s 7ms/step - loss: -0.0063 - accuracy: 0.8679\n",
      "At the end of episode 52 the total reward was : -21.0\n",
      "40/40 [==============================] - 0s 6ms/step - loss: -0.0240 - accuracy: 0.8807\n",
      "At the end of episode 53 the total reward was : -20.0\n",
      "43/43 [==============================] - 0s 6ms/step - loss: -0.0029 - accuracy: 0.8598\n",
      "At the end of episode 54 the total reward was : -20.0\n",
      "39/39 [==============================] - 0s 8ms/step - loss: -0.0058 - accuracy: 0.8693\n",
      "At the end of episode 55 the total reward was : -20.0\n",
      "52/52 [==============================] - 0s 7ms/step - loss: -0.0153 - accuracy: 0.8739\n",
      "At the end of episode 56 the total reward was : -19.0\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 0.0081 - accuracy: 0.8858\n",
      "At the end of episode 57 the total reward was : -20.0\n",
      "46/46 [==============================] - 0s 7ms/step - loss: -0.0080 - accuracy: 0.8765\n",
      "At the end of episode 58 the total reward was : -19.0\n",
      "48/48 [==============================] - 0s 7ms/step - loss: -0.0032 - accuracy: 0.8804\n",
      "At the end of episode 59 the total reward was : -20.0\n",
      "44/44 [==============================] - 0s 6ms/step - loss: -0.0083 - accuracy: 0.8769\n",
      "At the end of episode 60 the total reward was : -18.0\n",
      "41/41 [==============================] - 0s 7ms/step - loss: -0.0131 - accuracy: 0.8754\n",
      "At the end of episode 61 the total reward was : -21.0\n",
      "45/45 [==============================] - 0s 9ms/step - loss: -0.0099 - accuracy: 0.8926\n",
      "At the end of episode 62 the total reward was : -17.0\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.0043 - accuracy: 0.8930\n",
      "At the end of episode 63 the total reward was : -21.0\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.0222 - accuracy: 0.8891\n",
      "At the end of episode 64 the total reward was : -20.0\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 0.0280 - accuracy: 0.8785\n",
      "At the end of episode 65 the total reward was : -20.0\n",
      "44/44 [==============================] - 0s 8ms/step - loss: -0.0136 - accuracy: 0.8793\n",
      "At the end of episode 66 the total reward was : -18.0\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 0.0076 - accuracy: 0.8915\n",
      "At the end of episode 67 the total reward was : -18.0\n",
      "52/52 [==============================] - 0s 8ms/step - loss: -0.0018 - accuracy: 0.8724\n",
      "At the end of episode 68 the total reward was : -20.0\n",
      "46/46 [==============================] - 0s 8ms/step - loss: -0.0025 - accuracy: 0.8944\n",
      "At the end of episode 69 the total reward was : -20.0\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 0.0307 - accuracy: 0.8947\n",
      "At the end of episode 70 the total reward was : -20.0\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.0242 - accuracy: 0.8995\n",
      "At the end of episode 71 the total reward was : -18.0\n",
      "50/50 [==============================] - 0s 8ms/step - loss: -0.0058 - accuracy: 0.8968\n",
      "At the end of episode 72 the total reward was : -20.0\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 0.0320 - accuracy: 0.8967\n",
      "At the end of episode 73 the total reward was : -20.0\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 0.0010 - accuracy: 0.8929\n",
      "At the end of episode 74 the total reward was : -19.0\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 0.0062 - accuracy: 0.8966\n",
      "At the end of episode 75 the total reward was : -20.0\n",
      "46/46 [==============================] - 0s 8ms/step - loss: -0.0096 - accuracy: 0.8927\n",
      "At the end of episode 76 the total reward was : -19.0\n",
      "51/51 [==============================] - 0s 8ms/step - loss: -0.0039 - accuracy: 0.8884\n",
      "At the end of episode 77 the total reward was : -19.0\n",
      "46/46 [==============================] - 0s 8ms/step - loss: -0.0183 - accuracy: 0.8661\n",
      "At the end of episode 78 the total reward was : -21.0\n",
      "45/45 [==============================] - 0s 8ms/step - loss: 0.0147 - accuracy: 0.8646\n",
      "At the end of episode 79 the total reward was : -18.0\n",
      "45/45 [==============================] - 0s 8ms/step - loss: 0.0097 - accuracy: 0.8957\n",
      "At the end of episode 80 the total reward was : -20.0\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 0.0121 - accuracy: 0.8705\n",
      "At the end of episode 81 the total reward was : -19.0\n",
      "61/61 [==============================] - 1s 8ms/step - loss: 0.0142 - accuracy: 0.8859\n",
      "At the end of episode 82 the total reward was : -20.0\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.0073 - accuracy: 0.8720\n",
      "At the end of episode 83 the total reward was : -18.0\n",
      "45/45 [==============================] - 0s 8ms/step - loss: -0.0469 - accuracy: 0.8616\n",
      "At the end of episode 84 the total reward was : -20.0\n",
      "58/58 [==============================] - 0s 8ms/step - loss: -0.0116 - accuracy: 0.8721\n",
      "At the end of episode 85 the total reward was : -18.0\n",
      "47/47 [==============================] - 0s 7ms/step - loss: -0.0270 - accuracy: 0.8814\n",
      "At the end of episode 86 the total reward was : -18.0\n",
      "54/54 [==============================] - 0s 8ms/step - loss: -0.0157 - accuracy: 0.8890\n",
      "At the end of episode 87 the total reward was : -20.0\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 0.0054 - accuracy: 0.8712\n",
      "At the end of episode 88 the total reward was : -19.0\n",
      "42/42 [==============================] - 0s 7ms/step - loss: -0.0087 - accuracy: 0.8909\n",
      "At the end of episode 89 the total reward was : -21.0\n",
      "34/34 [==============================] - 0s 7ms/step - loss: 0.0187 - accuracy: 0.9119\n",
      "At the end of episode 90 the total reward was : -21.0\n",
      "32/32 [==============================] - 0s 7ms/step - loss: -0.0204 - accuracy: 0.8886\n",
      "At the end of episode 91 the total reward was : -17.0\n",
      "52/52 [==============================] - 0s 8ms/step - loss: -0.0098 - accuracy: 0.9039\n",
      "At the end of episode 92 the total reward was : -18.0\n",
      "43/43 [==============================] - 0s 7ms/step - loss: -0.0218 - accuracy: 0.9037\n",
      "At the end of episode 93 the total reward was : -16.0\n",
      "55/55 [==============================] - 0s 7ms/step - loss: -0.0169 - accuracy: 0.9098\n",
      "At the end of episode 94 the total reward was : -20.0\n",
      "37/37 [==============================] - 0s 7ms/step - loss: -0.0221 - accuracy: 0.9200\n",
      "At the end of episode 95 the total reward was : -18.0\n",
      "50/50 [==============================] - 0s 7ms/step - loss: -0.0219 - accuracy: 0.9018\n",
      "At the end of episode 96 the total reward was : -18.0\n",
      "43/43 [==============================] - 0s 7ms/step - loss: -0.0140 - accuracy: 0.9114\n",
      "At the end of episode 97 the total reward was : -15.0\n",
      "59/59 [==============================] - 0s 7ms/step - loss: -0.0095 - accuracy: 0.9102\n",
      "At the end of episode 98 the total reward was : -18.0\n",
      "43/43 [==============================] - 0s 7ms/step - loss: -0.0014 - accuracy: 0.9217\n",
      "At the end of episode 99 the total reward was : -17.0\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0162 - accuracy: 0.9189\n",
      "At the end of episode 100 the total reward was : -20.0\n",
      "37/37 [==============================] - 0s 6ms/step - loss: -0.0053 - accuracy: 0.9193\n",
      "At the end of episode 101 the total reward was : -15.0\n",
      "61/61 [==============================] - 0s 7ms/step - loss: 0.0072 - accuracy: 0.9131\n",
      "At the end of episode 102 the total reward was : -20.0\n",
      "50/50 [==============================] - 0s 7ms/step - loss: -0.0112 - accuracy: 0.9263\n",
      "At the end of episode 103 the total reward was : -19.0\n",
      "47/47 [==============================] - 0s 7ms/step - loss: -0.0019 - accuracy: 0.9288\n",
      "At the end of episode 104 the total reward was : -18.0\n",
      "47/47 [==============================] - 0s 8ms/step - loss: -0.0122 - accuracy: 0.9098\n",
      "At the end of episode 105 the total reward was : -21.0\n",
      "51/51 [==============================] - 0s 6ms/step - loss: 0.0067 - accuracy: 0.9300\n",
      "At the end of episode 106 the total reward was : -20.0\n",
      "48/48 [==============================] - 0s 6ms/step - loss: -0.0180 - accuracy: 0.9161\n",
      "At the end of episode 107 the total reward was : -19.0\n",
      "55/55 [==============================] - 0s 6ms/step - loss: -0.0011 - accuracy: 0.9134\n",
      "At the end of episode 108 the total reward was : -18.0\n",
      "50/50 [==============================] - 0s 7ms/step - loss: -0.0212 - accuracy: 0.9236\n",
      "At the end of episode 109 the total reward was : -19.0\n",
      "44/44 [==============================] - 0s 6ms/step - loss: -0.0041 - accuracy: 0.9172\n",
      "At the end of episode 110 the total reward was : -18.0\n",
      "56/56 [==============================] - 0s 7ms/step - loss: -0.0062 - accuracy: 0.9320\n",
      "At the end of episode 111 the total reward was : -18.0\n",
      "50/50 [==============================] - 0s 7ms/step - loss: -0.0257 - accuracy: 0.9194\n",
      "At the end of episode 112 the total reward was : -20.0\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 0.0230 - accuracy: 0.9254\n",
      "At the end of episode 113 the total reward was : -21.0\n",
      "49/49 [==============================] - 0s 7ms/step - loss: -0.0020 - accuracy: 0.9375\n",
      "At the end of episode 114 the total reward was : -20.0\n",
      "48/48 [==============================] - 0s 7ms/step - loss: -0.0109 - accuracy: 0.9306\n",
      "At the end of episode 115 the total reward was : -19.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49/49 [==============================] - 0s 7ms/step - loss: -0.0210 - accuracy: 0.9321\n",
      "At the end of episode 116 the total reward was : -20.0\n",
      "54/54 [==============================] - 0s 7ms/step - loss: -0.0021 - accuracy: 0.9332\n",
      "At the end of episode 117 the total reward was : -19.0\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0098 - accuracy: 0.9327\n",
      "At the end of episode 118 the total reward was : -19.0\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 0.0022 - accuracy: 0.9333\n",
      "At the end of episode 119 the total reward was : -18.0\n",
      "56/56 [==============================] - 0s 7ms/step - loss: -7.2944e-04 - accuracy: 0.9280\n",
      "At the end of episode 120 the total reward was : -19.0\n",
      "42/42 [==============================] - 0s 7ms/step - loss: -0.0135 - accuracy: 0.9332\n",
      "At the end of episode 121 the total reward was : -18.0\n",
      "54/54 [==============================] - 0s 8ms/step - loss: -9.5008e-04 - accuracy: 0.9261\n",
      "At the end of episode 122 the total reward was : -18.0\n",
      "56/56 [==============================] - 0s 7ms/step - loss: -0.0298 - accuracy: 0.9278\n",
      "At the end of episode 123 the total reward was : -20.0\n",
      "37/37 [==============================] - 0s 6ms/step - loss: -0.0058 - accuracy: 0.9294\n",
      "At the end of episode 124 the total reward was : -17.0\n",
      "51/51 [==============================] - 1s 10ms/step - loss: 0.0025 - accuracy: 0.9344\n",
      "At the end of episode 125 the total reward was : -19.0\n",
      "59/59 [==============================] - 0s 7ms/step - loss: 0.0090 - accuracy: 0.9447\n",
      "At the end of episode 126 the total reward was : -19.0\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.0071 - accuracy: 0.9393\n",
      "At the end of episode 127 the total reward was : -21.0\n",
      "32/32 [==============================] - 0s 6ms/step - loss: -0.0071 - accuracy: 0.9194\n",
      "At the end of episode 128 the total reward was : -19.0\n",
      "45/45 [==============================] - 0s 7ms/step - loss: -0.0044 - accuracy: 0.9214\n",
      "At the end of episode 129 the total reward was : -20.0\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 0.0094 - accuracy: 0.9373\n",
      "At the end of episode 130 the total reward was : -20.0\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 0.0067 - accuracy: 0.9310\n",
      "At the end of episode 131 the total reward was : -18.0\n",
      "56/56 [==============================] - 0s 7ms/step - loss: -2.3010e-04 - accuracy: 0.9375\n",
      "At the end of episode 132 the total reward was : -18.0\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 1.3859e-04 - accuracy: 0.9194\n",
      "At the end of episode 133 the total reward was : -20.0\n",
      "42/42 [==============================] - 0s 7ms/step - loss: -0.0197 - accuracy: 0.9226\n",
      "At the end of episode 134 the total reward was : -20.0\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.0098 - accuracy: 0.9339\n",
      "At the end of episode 135 the total reward was : -19.0\n",
      "49/49 [==============================] - 0s 7ms/step - loss: -0.0148 - accuracy: 0.9296\n",
      "At the end of episode 136 the total reward was : -19.0\n",
      "55/55 [==============================] - 0s 7ms/step - loss: -0.0018 - accuracy: 0.9375\n",
      "At the end of episode 137 the total reward was : -17.0\n",
      "74/74 [==============================] - 1s 7ms/step - loss: -0.0084 - accuracy: 0.9163\n",
      "At the end of episode 138 the total reward was : -19.0\n",
      "34/34 [==============================] - 0s 7ms/step - loss: -0.0172 - accuracy: 0.9136\n",
      "At the end of episode 139 the total reward was : -21.0\n",
      "49/49 [==============================] - 0s 7ms/step - loss: 0.0238 - accuracy: 0.9294\n",
      "At the end of episode 140 the total reward was : -20.0\n",
      "52/52 [==============================] - 0s 7ms/step - loss: -0.0118 - accuracy: 0.9327\n",
      "At the end of episode 141 the total reward was : -20.0\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.0179 - accuracy: 0.9369\n",
      "At the end of episode 142 the total reward was : -21.0\n",
      "32/32 [==============================] - 0s 7ms/step - loss: -0.0262 - accuracy: 0.9376\n",
      "At the end of episode 143 the total reward was : -21.0\n",
      "47/47 [==============================] - 0s 7ms/step - loss: -0.0110 - accuracy: 0.9289\n",
      "At the end of episode 144 the total reward was : -20.0\n",
      "41/41 [==============================] - 0s 6ms/step - loss: -0.0087 - accuracy: 0.9306\n",
      "At the end of episode 145 the total reward was : -20.0\n",
      "48/48 [==============================] - 0s 7ms/step - loss: -0.0010 - accuracy: 0.9382\n",
      "At the end of episode 146 the total reward was : -17.0\n",
      "61/61 [==============================] - 0s 7ms/step - loss: -0.0165 - accuracy: 0.9343\n",
      "At the end of episode 147 the total reward was : -18.0\n",
      "62/62 [==============================] - 0s 7ms/step - loss: -0.0169 - accuracy: 0.9322\n",
      "At the end of episode 148 the total reward was : -19.0\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 2.8499e-04 - accuracy: 0.9371\n",
      "At the end of episode 149 the total reward was : -19.0\n",
      "47/47 [==============================] - 0s 7ms/step - loss: -0.0016 - accuracy: 0.9481\n",
      "At the end of episode 150 the total reward was : -19.0\n",
      "59/59 [==============================] - 0s 7ms/step - loss: -0.0112 - accuracy: 0.9523\n",
      "At the end of episode 151 the total reward was : -19.0\n",
      "61/61 [==============================] - 0s 7ms/step - loss: -0.0153 - accuracy: 0.9574\n",
      "At the end of episode 152 the total reward was : -20.0\n",
      "50/50 [==============================] - 0s 7ms/step - loss: -0.0042 - accuracy: 0.9486\n",
      "At the end of episode 153 the total reward was : -18.0\n",
      "47/47 [==============================] - 0s 7ms/step - loss: -0.0140 - accuracy: 0.9472\n",
      "At the end of episode 154 the total reward was : -20.0\n",
      "54/54 [==============================] - 0s 7ms/step - loss: -0.0042 - accuracy: 0.9287\n",
      "At the end of episode 155 the total reward was : -17.0\n",
      "40/40 [==============================] - 0s 6ms/step - loss: -0.0065 - accuracy: 0.9315\n",
      "At the end of episode 156 the total reward was : -20.0\n",
      "41/41 [==============================] - 0s 6ms/step - loss: -0.0085 - accuracy: 0.9463\n",
      "At the end of episode 157 the total reward was : -19.0\n",
      "51/51 [==============================] - 0s 7ms/step - loss: -0.0052 - accuracy: 0.9428\n",
      "At the end of episode 158 the total reward was : -17.0\n",
      "52/52 [==============================] - 0s 6ms/step - loss: -0.0169 - accuracy: 0.9474\n",
      "At the end of episode 159 the total reward was : -19.0\n",
      "45/45 [==============================] - 0s 7ms/step - loss: -0.0038 - accuracy: 0.9388\n",
      "At the end of episode 160 the total reward was : -20.0\n",
      "39/39 [==============================] - 0s 7ms/step - loss: -0.0014 - accuracy: 0.9550\n",
      "At the end of episode 161 the total reward was : -17.0\n",
      "66/66 [==============================] - 0s 7ms/step - loss: 0.0030 - accuracy: 0.9489\n",
      "At the end of episode 162 the total reward was : -19.0\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.0044 - accuracy: 0.9553\n",
      "At the end of episode 163 the total reward was : -17.0\n",
      "55/55 [==============================] - 0s 9ms/step - loss: -0.0096 - accuracy: 0.9460\n",
      "At the end of episode 164 the total reward was : -20.0\n",
      "52/52 [==============================] - 0s 7ms/step - loss: -0.0147 - accuracy: 0.9492\n",
      "At the end of episode 165 the total reward was : -21.0\n",
      "57/57 [==============================] - 0s 7ms/step - loss: -0.0296 - accuracy: 0.9499\n",
      "At the end of episode 166 the total reward was : -14.0\n",
      "64/64 [==============================] - 0s 7ms/step - loss: -0.0074 - accuracy: 0.9515\n",
      "At the end of episode 167 the total reward was : -21.0\n",
      "58/58 [==============================] - 0s 7ms/step - loss: -0.0077 - accuracy: 0.9547\n",
      "At the end of episode 168 the total reward was : -19.0\n",
      "44/44 [==============================] - 0s 7ms/step - loss: -0.0042 - accuracy: 0.9458\n",
      "At the end of episode 169 the total reward was : -19.0\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0084 - accuracy: 0.9586\n",
      "At the end of episode 170 the total reward was : -20.0\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.0033 - accuracy: 0.9456\n",
      "At the end of episode 171 the total reward was : -19.0\n",
      "70/70 [==============================] - 1s 7ms/step - loss: -0.0088 - accuracy: 0.9480\n",
      "At the end of episode 172 the total reward was : -18.0\n",
      "71/71 [==============================] - 0s 7ms/step - loss: 0.0019 - accuracy: 0.9396\n",
      "At the end of episode 173 the total reward was : -20.0\n",
      "48/48 [==============================] - ETA: 0s - loss: -0.0295 - accuracy: 0.939 - 0s 7ms/step - loss: -0.0327 - accuracy: 0.9348\n",
      "At the end of episode 174 the total reward was : -17.0\n",
      "61/61 [==============================] - 0s 7ms/step - loss: -0.0062 - accuracy: 0.9551\n",
      "At the end of episode 175 the total reward was : -20.0\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.0041 - accuracy: 0.9476\n",
      "At the end of episode 176 the total reward was : -19.0\n",
      "55/55 [==============================] - 0s 7ms/step - loss: -0.0113 - accuracy: 0.9553\n",
      "At the end of episode 177 the total reward was : -21.0\n",
      "64/64 [==============================] - 0s 7ms/step - loss: 0.0095 - accuracy: 0.9428\n",
      "At the end of episode 178 the total reward was : -19.0\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 0.0185 - accuracy: 0.9616\n",
      "At the end of episode 179 the total reward was : -20.0\n",
      "43/43 [==============================] - 0s 7ms/step - loss: -0.0017 - accuracy: 0.9376\n",
      "At the end of episode 180 the total reward was : -16.0\n",
      "49/49 [==============================] - 0s 7ms/step - loss: -3.1442e-04 - accuracy: 0.9502\n",
      "At the end of episode 181 the total reward was : -19.0\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0019 - accuracy: 0.9509\n",
      "At the end of episode 182 the total reward was : -19.0\n",
      "57/57 [==============================] - 0s 7ms/step - loss: -0.0063 - accuracy: 0.9454\n",
      "At the end of episode 183 the total reward was : -18.0\n",
      "60/60 [==============================] - 0s 7ms/step - loss: -0.0112 - accuracy: 0.9479\n",
      "At the end of episode 184 the total reward was : -21.0\n",
      "53/53 [==============================] - 0s 7ms/step - loss: -0.0082 - accuracy: 0.9446\n",
      "At the end of episode 185 the total reward was : -20.0\n",
      "60/60 [==============================] - 0s 7ms/step - loss: -0.0232 - accuracy: 0.9488\n",
      "At the end of episode 186 the total reward was : -19.0\n",
      "49/49 [==============================] - 0s 7ms/step - loss: -0.0060 - accuracy: 0.9495\n",
      "At the end of episode 187 the total reward was : -19.0\n",
      "59/59 [==============================] - 0s 7ms/step - loss: -0.0157 - accuracy: 0.9572\n",
      "At the end of episode 188 the total reward was : -20.0\n",
      "58/58 [==============================] - 0s 7ms/step - loss: 0.0063 - accuracy: 0.9571\n",
      "At the end of episode 189 the total reward was : -20.0\n",
      "75/75 [==============================] - 1s 7ms/step - loss: 6.1307e-04 - accuracy: 0.9610\n",
      "At the end of episode 190 the total reward was : -18.0\n",
      "51/51 [==============================] - 0s 7ms/step - loss: -0.0015 - accuracy: 0.9650\n",
      "At the end of episode 191 the total reward was : -19.0\n",
      "65/65 [==============================] - 0s 7ms/step - loss: 0.0067 - accuracy: 0.9582\n",
      "At the end of episode 192 the total reward was : -20.0\n",
      "66/66 [==============================] - 0s 7ms/step - loss: 0.0076 - accuracy: 0.9592\n",
      "At the end of episode 193 the total reward was : -14.0\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.0016 - accuracy: 0.9551\n",
      "At the end of episode 194 the total reward was : -17.0\n",
      "69/69 [==============================] - 1s 7ms/step - loss: -0.0083 - accuracy: 0.9575\n",
      "At the end of episode 195 the total reward was : -20.0\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.0164 - accuracy: 0.9511\n",
      "At the end of episode 196 the total reward was : -14.0\n",
      "76/76 [==============================] - 1s 7ms/step - loss: 7.5376e-07 - accuracy: 0.9543\n",
      "At the end of episode 197 the total reward was : -19.0\n",
      "61/61 [==============================] - 0s 7ms/step - loss: 0.0019 - accuracy: 0.9410\n",
      "At the end of episode 198 the total reward was : -19.0\n",
      "61/61 [==============================] - 0s 7ms/step - loss: 0.0108 - accuracy: 0.9496\n",
      "At the end of episode 199 the total reward was : -20.0\n",
      "56/56 [==============================] - 0s 7ms/step - loss: 0.0178 - accuracy: 0.9595\n",
      "At the end of episode 200 the total reward was : -21.0\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 0.0117 - accuracy: 0.9585\n",
      "At the end of episode 201 the total reward was : -21.0\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.0168 - accuracy: 0.9573\n",
      "At the end of episode 202 the total reward was : -21.0\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0055 - accuracy: 0.9446\n",
      "At the end of episode 203 the total reward was : -15.0\n",
      "76/76 [==============================] - 1s 8ms/step - loss: 0.0059 - accuracy: 0.9539\n",
      "At the end of episode 204 the total reward was : -20.0\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.0087 - accuracy: 0.9558\n",
      "At the end of episode 205 the total reward was : -21.0\n",
      "61/61 [==============================] - 0s 7ms/step - loss: 0.0024 - accuracy: 0.9466\n",
      "At the end of episode 206 the total reward was : -18.0\n",
      "64/64 [==============================] - 0s 7ms/step - loss: -0.0034 - accuracy: 0.9536\n",
      "At the end of episode 207 the total reward was : -19.0\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 0.0107 - accuracy: 0.9445\n",
      "At the end of episode 208 the total reward was : -20.0\n",
      "58/58 [==============================] - 0s 7ms/step - loss: 0.0036 - accuracy: 0.9506\n",
      "At the end of episode 209 the total reward was : -21.0\n",
      "59/59 [==============================] - 0s 7ms/step - loss: 0.0139 - accuracy: 0.9453\n",
      "At the end of episode 210 the total reward was : -19.0\n",
      "59/59 [==============================] - 0s 7ms/step - loss: -0.0076 - accuracy: 0.9572\n",
      "At the end of episode 211 the total reward was : -17.0\n",
      "65/65 [==============================] - 0s 7ms/step - loss: 0.0026 - accuracy: 0.9567\n",
      "At the end of episode 212 the total reward was : -19.0\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0159 - accuracy: 0.9496\n",
      "At the end of episode 213 the total reward was : -19.0\n",
      "78/78 [==============================] - 1s 7ms/step - loss: 0.0092 - accuracy: 0.9579\n",
      "At the end of episode 214 the total reward was : -17.0\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.0085 - accuracy: 0.9570\n",
      "At the end of episode 215 the total reward was : -19.0\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 0.0097 - accuracy: 0.9535\n",
      "At the end of episode 216 the total reward was : -19.0\n",
      "59/59 [==============================] - 1s 9ms/step - loss: 9.0153e-04 - accuracy: 0.9528\n",
      "At the end of episode 217 the total reward was : -20.0\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 0.0303 - accuracy: 0.9510\n",
      "At the end of episode 218 the total reward was : -20.0\n",
      "65/65 [==============================] - 1s 8ms/step - loss: -0.0076 - accuracy: 0.9486\n",
      "At the end of episode 219 the total reward was : -18.0\n",
      "52/52 [==============================] - 0s 8ms/step - loss: -4.8390e-04 - accuracy: 0.9578\n",
      "At the end of episode 220 the total reward was : -16.0\n",
      "70/70 [==============================] - 1s 8ms/step - loss: -0.0048 - accuracy: 0.9476\n",
      "At the end of episode 221 the total reward was : -19.0\n",
      "64/64 [==============================] - 1s 8ms/step - loss: 0.0028 - accuracy: 0.9541\n",
      "At the end of episode 222 the total reward was : -18.0\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 0.0044 - accuracy: 0.9559\n",
      "At the end of episode 223 the total reward was : -21.0\n",
      "57/57 [==============================] - 0s 8ms/step - loss: -9.6144e-05 - accuracy: 0.9605\n",
      "At the end of episode 224 the total reward was : -19.0\n",
      "76/76 [==============================] - 1s 9ms/step - loss: -0.0112 - accuracy: 0.9490\n",
      "At the end of episode 225 the total reward was : -16.0\n",
      "68/68 [==============================] - 1s 8ms/step - loss: -2.8415e-04 - accuracy: 0.9481\n",
      "At the end of episode 226 the total reward was : -19.0\n",
      "59/59 [==============================] - 0s 8ms/step - loss: -0.0081 - accuracy: 0.9594\n",
      "At the end of episode 227 the total reward was : -17.0\n",
      "57/57 [==============================] - 0s 8ms/step - loss: -0.0020 - accuracy: 0.9566\n",
      "At the end of episode 228 the total reward was : -20.0\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 1.5082e-04 - accuracy: 0.9511\n",
      "At the end of episode 229 the total reward was : -21.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47/47 [==============================] - 0s 7ms/step - loss: 0.0079 - accuracy: 0.9566\n",
      "At the end of episode 230 the total reward was : -17.0\n",
      "63/63 [==============================] - 0s 8ms/step - loss: -0.0096 - accuracy: 0.9611\n",
      "At the end of episode 231 the total reward was : -18.0\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 7.9393e-04 - accuracy: 0.9584\n",
      "At the end of episode 232 the total reward was : -19.0\n",
      "61/61 [==============================] - 0s 7ms/step - loss: 0.0133 - accuracy: 0.9622\n",
      "At the end of episode 233 the total reward was : -19.0\n",
      "49/49 [==============================] - 0s 7ms/step - loss: 0.0040 - accuracy: 0.9475\n",
      "At the end of episode 234 the total reward was : -20.0\n",
      "64/64 [==============================] - 1s 8ms/step - loss: -0.0038 - accuracy: 0.9529\n",
      "At the end of episode 235 the total reward was : -19.0\n",
      "67/67 [==============================] - 1s 8ms/step - loss: -0.0089 - accuracy: 0.9551\n",
      "At the end of episode 236 the total reward was : -18.0\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 2.0009e-04 - accuracy: 0.9584\n",
      "At the end of episode 237 the total reward was : -16.0\n",
      "68/68 [==============================] - 1s 9ms/step - loss: 0.0038 - accuracy: 0.9615\n",
      "At the end of episode 238 the total reward was : -21.0\n",
      "59/59 [==============================] - 0s 8ms/step - loss: 0.0083 - accuracy: 0.9640\n",
      "At the end of episode 239 the total reward was : -21.0\n",
      "57/57 [==============================] - 0s 9ms/step - loss: -0.0285 - accuracy: 0.9591\n",
      "At the end of episode 240 the total reward was : -17.0\n",
      "61/61 [==============================] - 1s 9ms/step - loss: -0.0222 - accuracy: 0.9474\n",
      "At the end of episode 241 the total reward was : -16.0\n",
      "58/58 [==============================] - 0s 9ms/step - loss: -3.4586e-04 - accuracy: 0.9649\n",
      "At the end of episode 242 the total reward was : -18.0\n",
      "62/62 [==============================] - 1s 9ms/step - loss: -0.0054 - accuracy: 0.9497\n",
      "At the end of episode 243 the total reward was : -17.0\n",
      "71/71 [==============================] - 1s 9ms/step - loss: -0.0102 - accuracy: 0.9546\n",
      "At the end of episode 244 the total reward was : -17.0\n",
      "78/78 [==============================] - 1s 9ms/step - loss: 0.0163 - accuracy: 0.9541\n",
      "At the end of episode 245 the total reward was : -19.0\n",
      "59/59 [==============================] - 1s 9ms/step - loss: 0.0023 - accuracy: 0.9555\n",
      "At the end of episode 246 the total reward was : -18.0\n",
      "69/69 [==============================] - 1s 9ms/step - loss: -0.0028 - accuracy: 0.9628\n",
      "At the end of episode 247 the total reward was : -20.0\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.0123 - accuracy: 0.9636\n",
      "At the end of episode 248 the total reward was : -19.0\n",
      "66/66 [==============================] - 1s 9ms/step - loss: -0.0035 - accuracy: 0.9554\n",
      "At the end of episode 249 the total reward was : -20.0\n",
      "52/52 [==============================] - 0s 9ms/step - loss: -0.0099 - accuracy: 0.9679\n",
      "At the end of episode 250 the total reward was : -18.0\n",
      "47/47 [==============================] - 0s 8ms/step - loss: -0.0085 - accuracy: 0.9561\n",
      "At the end of episode 251 the total reward was : -19.0\n",
      "67/67 [==============================] - 1s 9ms/step - loss: -1.7257e-04 - accuracy: 0.9659\n",
      "At the end of episode 252 the total reward was : -18.0\n",
      "62/62 [==============================] - 1s 8ms/step - loss: -0.0031 - accuracy: 0.9594\n",
      "At the end of episode 253 the total reward was : -19.0\n",
      "63/63 [==============================] - 1s 8ms/step - loss: -0.0094 - accuracy: 0.9583\n",
      "At the end of episode 254 the total reward was : -18.0\n",
      "58/58 [==============================] - 1s 10ms/step - loss: -0.0057 - accuracy: 0.9675\n",
      "At the end of episode 255 the total reward was : -19.0\n",
      "53/53 [==============================] - 1s 10ms/step - loss: 0.0024 - accuracy: 0.9620\n",
      "At the end of episode 256 the total reward was : -18.0\n",
      "50/50 [==============================] - 0s 9ms/step - loss: -0.0041 - accuracy: 0.9737\n",
      "At the end of episode 257 the total reward was : -17.0\n",
      "67/67 [==============================] - 1s 10ms/step - loss: -0.0024 - accuracy: 0.9651\n",
      "At the end of episode 258 the total reward was : -16.0\n",
      "79/79 [==============================] - 1s 8ms/step - loss: -0.0070 - accuracy: 0.9524\n",
      "At the end of episode 259 the total reward was : -19.0\n",
      "46/46 [==============================] - 0s 9ms/step - loss: -0.0038 - accuracy: 0.9633\n",
      "At the end of episode 260 the total reward was : -19.0\n",
      "53/53 [==============================] - 0s 8ms/step - loss: -0.0073 - accuracy: 0.9727\n",
      "At the end of episode 261 the total reward was : -18.0\n",
      "54/54 [==============================] - 0s 8ms/step - loss: -0.0062 - accuracy: 0.9686\n",
      "At the end of episode 262 the total reward was : -21.0\n",
      "42/42 [==============================] - 0s 8ms/step - loss: 0.0052 - accuracy: 0.9651\n",
      "At the end of episode 263 the total reward was : -21.0\n",
      "57/57 [==============================] - 0s 8ms/step - loss: 0.0027 - accuracy: 0.9633\n",
      "At the end of episode 264 the total reward was : -17.0\n",
      "69/69 [==============================] - 1s 7ms/step - loss: 0.0021 - accuracy: 0.9629 \n",
      "At the end of episode 265 the total reward was : -19.0\n",
      "46/46 [==============================] - 0s 7ms/step - loss: -0.0065 - accuracy: 0.9604\n",
      "At the end of episode 266 the total reward was : -18.0\n",
      "56/56 [==============================] - 0s 8ms/step - loss: -0.0116 - accuracy: 0.9674\n",
      "At the end of episode 267 the total reward was : -20.0\n",
      "63/63 [==============================] - 1s 8ms/step - loss: 0.0096 - accuracy: 0.9653\n",
      "At the end of episode 268 the total reward was : -16.0\n",
      "55/55 [==============================] - 0s 8ms/step - loss: 0.0109 - accuracy: 0.9655\n",
      "At the end of episode 269 the total reward was : -19.0\n",
      "57/57 [==============================] - 0s 8ms/step - loss: -1.0158e-04 - accuracy: 0.9629\n",
      "At the end of episode 270 the total reward was : -17.0\n",
      "72/72 [==============================] - 1s 7ms/step - loss: -0.0012 - accuracy: 0.9617\n",
      "At the end of episode 271 the total reward was : -18.0\n",
      "56/56 [==============================] - 0s 7ms/step - loss: -0.0050 - accuracy: 0.9719\n",
      "At the end of episode 272 the total reward was : -20.0\n",
      "54/54 [==============================] - 0s 7ms/step - loss: -0.0092 - accuracy: 0.9708\n",
      "At the end of episode 273 the total reward was : -21.0\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0150 - accuracy: 0.9731\n",
      "At the end of episode 274 the total reward was : -19.0\n",
      "55/55 [==============================] - 0s 7ms/step - loss: -0.0088 - accuracy: 0.9519\n",
      "At the end of episode 275 the total reward was : -19.0\n",
      "47/47 [==============================] - 0s 8ms/step - loss: -0.0061 - accuracy: 0.9688\n",
      "At the end of episode 276 the total reward was : -18.0\n",
      "64/64 [==============================] - 0s 7ms/step - loss: -0.0015 - accuracy: 0.9719\n",
      "At the end of episode 277 the total reward was : -19.0\n",
      "65/65 [==============================] - 0s 7ms/step - loss: -0.0049 - accuracy: 0.9707\n",
      "At the end of episode 278 the total reward was : -20.0\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 0.0233 - accuracy: 0.9612\n",
      "At the end of episode 279 the total reward was : -17.0\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.0149 - accuracy: 0.9727\n",
      "At the end of episode 280 the total reward was : -19.0\n",
      "49/49 [==============================] - 0s 7ms/step - loss: 0.0048 - accuracy: 0.9668\n",
      "At the end of episode 281 the total reward was : -21.0\n",
      "55/55 [==============================] - 0s 7ms/step - loss: -0.0088 - accuracy: 0.9650\n",
      "At the end of episode 282 the total reward was : -16.0\n",
      "62/62 [==============================] - 1s 9ms/step - loss: -0.0177 - accuracy: 0.9675\n",
      "At the end of episode 283 the total reward was : -20.0\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.0023 - accuracy: 0.9642\n",
      "At the end of episode 284 the total reward was : -20.0\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.0039 - accuracy: 0.9663\n",
      "At the end of episode 285 the total reward was : -17.0\n",
      "59/59 [==============================] - 0s 7ms/step - loss: 0.0060 - accuracy: 0.9670\n",
      "At the end of episode 286 the total reward was : -18.0\n",
      "58/58 [==============================] - 0s 7ms/step - loss: -0.0029 - accuracy: 0.9674\n",
      "At the end of episode 287 the total reward was : -18.0\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0049 - accuracy: 0.9687\n",
      "At the end of episode 288 the total reward was : -18.0\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.0013 - accuracy: 0.9725\n",
      "At the end of episode 289 the total reward was : -16.0\n",
      "70/70 [==============================] - 0s 7ms/step - loss: -0.0040 - accuracy: 0.9622\n",
      "At the end of episode 290 the total reward was : -15.0\n",
      "82/82 [==============================] - 1s 7ms/step - loss: 0.0056 - accuracy: 0.9549\n",
      "At the end of episode 291 the total reward was : -20.0\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 0.0041 - accuracy: 0.9712\n",
      "At the end of episode 292 the total reward was : -21.0\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 9.8392e-04 - accuracy: 0.9709\n",
      "At the end of episode 293 the total reward was : -20.0\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 0.0148 - accuracy: 0.9619\n",
      "At the end of episode 294 the total reward was : -18.0\n",
      "68/68 [==============================] - 1s 7ms/step - loss: 0.0154 - accuracy: 0.9520\n",
      "At the end of episode 295 the total reward was : -21.0\n",
      "59/59 [==============================] - 0s 7ms/step - loss: -0.0107 - accuracy: 0.9605\n",
      "At the end of episode 296 the total reward was : -18.0\n",
      "60/60 [==============================] - 0s 7ms/step - loss: -5.2127e-05 - accuracy: 0.9675\n",
      "At the end of episode 297 the total reward was : -20.0\n",
      "52/52 [==============================] - 0s 7ms/step - loss: -0.0040 - accuracy: 0.9492\n",
      "At the end of episode 298 the total reward was : -18.0\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.0027 - accuracy: 0.9592\n",
      "At the end of episode 299 the total reward was : -18.0\n",
      "53/53 [==============================] - 0s 7ms/step - loss: -0.0017 - accuracy: 0.9629\n",
      "At the end of episode 300 the total reward was : -18.0\n",
      "45/45 [==============================] - 0s 6ms/step - loss: -0.0127 - accuracy: 0.9620\n",
      "At the end of episode 301 the total reward was : -15.0\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0099 - accuracy: 0.9625\n",
      "At the end of episode 302 the total reward was : -20.0\n",
      "45/45 [==============================] - 0s 7ms/step - loss: -0.0051 - accuracy: 0.9540\n",
      "At the end of episode 303 the total reward was : -18.0\n",
      "77/77 [==============================] - 1s 7ms/step - loss: 0.0024 - accuracy: 0.9571\n",
      "At the end of episode 304 the total reward was : -18.0\n",
      "54/54 [==============================] - 0s 7ms/step - loss: -0.0176 - accuracy: 0.9576\n",
      "At the end of episode 305 the total reward was : -18.0\n",
      "64/64 [==============================] - 0s 7ms/step - loss: -0.0037 - accuracy: 0.9636\n",
      "At the end of episode 306 the total reward was : -18.0\n",
      "49/49 [==============================] - 0s 7ms/step - loss: -0.0092 - accuracy: 0.9694\n",
      "At the end of episode 307 the total reward was : -19.0\n",
      "55/55 [==============================] - 0s 7ms/step - loss: -0.0104 - accuracy: 0.9663\n",
      "At the end of episode 308 the total reward was : -18.0\n",
      "58/58 [==============================] - 0s 8ms/step - loss: -0.0091 - accuracy: 0.9627\n",
      "At the end of episode 309 the total reward was : -16.0\n",
      "66/66 [==============================] - 0s 7ms/step - loss: 0.0129 - accuracy: 0.9585\n",
      "At the end of episode 310 the total reward was : -18.0\n",
      "71/71 [==============================] - 1s 7ms/step - loss: -0.0116 - accuracy: 0.9559\n",
      "At the end of episode 311 the total reward was : -20.0\n",
      "56/56 [==============================] - 0s 7ms/step - loss: 0.0055 - accuracy: 0.9640\n",
      "At the end of episode 312 the total reward was : -19.0\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 0.0057 - accuracy: 0.9600\n",
      "At the end of episode 313 the total reward was : -19.0\n",
      "57/57 [==============================] - 0s 7ms/step - loss: -0.0114 - accuracy: 0.9669\n",
      "At the end of episode 314 the total reward was : -19.0\n",
      "53/53 [==============================] - 0s 7ms/step - loss: -0.0106 - accuracy: 0.9622\n",
      "At the end of episode 315 the total reward was : -19.0\n",
      "57/57 [==============================] - 0s 7ms/step - loss: -0.0073 - accuracy: 0.9713\n",
      "At the end of episode 316 the total reward was : -19.0\n",
      "59/59 [==============================] - 0s 7ms/step - loss: 0.0045 - accuracy: 0.9615\n",
      "At the end of episode 317 the total reward was : -17.0\n",
      "73/73 [==============================] - 1s 7ms/step - loss: 0.0034 - accuracy: 0.9662\n",
      "At the end of episode 318 the total reward was : -18.0\n",
      "56/56 [==============================] - 0s 7ms/step - loss: -0.0281 - accuracy: 0.9595\n",
      "At the end of episode 319 the total reward was : -19.0\n",
      "72/72 [==============================] - 1s 7ms/step - loss: -0.0080 - accuracy: 0.9642\n",
      "At the end of episode 320 the total reward was : -18.0\n",
      "47/47 [==============================] - 0s 7ms/step - loss: -0.0024 - accuracy: 0.9688\n",
      "At the end of episode 321 the total reward was : -19.0\n",
      "55/55 [==============================] - 0s 8ms/step - loss: 0.0102 - accuracy: 0.9714\n",
      "At the end of episode 322 the total reward was : -19.0\n",
      "66/66 [==============================] - 1s 8ms/step - loss: -0.0030 - accuracy: 0.9516\n",
      "At the end of episode 323 the total reward was : -19.0\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 0.0024 - accuracy: 0.9718\n",
      "At the end of episode 324 the total reward was : -19.0\n",
      "49/49 [==============================] - 0s 7ms/step - loss: 2.6220e-04 - accuracy: 0.9660\n",
      "At the end of episode 325 the total reward was : -16.0\n",
      "53/53 [==============================] - 0s 8ms/step - loss: -0.0078 - accuracy: 0.9581\n",
      "At the end of episode 326 the total reward was : -20.0\n",
      "41/41 [==============================] - 0s 7ms/step - loss: -0.0077 - accuracy: 0.9674\n",
      "At the end of episode 327 the total reward was : -19.0\n",
      "46/46 [==============================] - 0s 7ms/step - loss: -0.0074 - accuracy: 0.9605\n",
      "At the end of episode 328 the total reward was : -19.0\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.0161 - accuracy: 0.9658\n",
      "At the end of episode 329 the total reward was : -19.0\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.0016 - accuracy: 0.9669\n",
      "At the end of episode 330 the total reward was : -20.0\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0081 - accuracy: 0.9792\n",
      "At the end of episode 331 the total reward was : -20.0\n",
      "43/43 [==============================] - 0s 7ms/step - loss: -0.0048 - accuracy: 0.9695\n",
      "At the end of episode 332 the total reward was : -20.0\n",
      "44/44 [==============================] - 0s 7ms/step - loss: -0.0099 - accuracy: 0.9772\n",
      "At the end of episode 333 the total reward was : -18.0\n",
      "49/49 [==============================] - 0s 7ms/step - loss: -0.0086 - accuracy: 0.9714\n",
      "At the end of episode 334 the total reward was : -20.0\n",
      "56/56 [==============================] - 0s 7ms/step - loss: -0.0024 - accuracy: 0.9667\n",
      "At the end of episode 335 the total reward was : -21.0\n",
      "42/42 [==============================] - 0s 7ms/step - loss: 0.0012 - accuracy: 0.9711\n",
      "At the end of episode 336 the total reward was : -17.0\n",
      "61/61 [==============================] - 0s 7ms/step - loss: -0.0057 - accuracy: 0.9654\n",
      "At the end of episode 337 the total reward was : -20.0\n",
      "41/41 [==============================] - 0s 7ms/step - loss: 0.0029 - accuracy: 0.9540\n",
      "At the end of episode 338 the total reward was : -20.0\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.0179 - accuracy: 0.9627\n",
      "At the end of episode 339 the total reward was : -20.0\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 0.0015 - accuracy: 0.9721\n",
      "At the end of episode 340 the total reward was : -18.0\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 0.0027 - accuracy: 0.9722\n",
      "At the end of episode 341 the total reward was : -18.0\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.0020 - accuracy: 0.9656\n",
      "At the end of episode 342 the total reward was : -18.0\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.0013 - accuracy: 0.9680\n",
      "At the end of episode 343 the total reward was : -20.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35/35 [==============================] - 0s 6ms/step - loss: 0.0096 - accuracy: 0.9710\n",
      "At the end of episode 344 the total reward was : -20.0\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 0.0015 - accuracy: 0.9702\n",
      "At the end of episode 345 the total reward was : -19.0\n",
      "49/49 [==============================] - 0s 7ms/step - loss: 7.1419e-05 - accuracy: 0.9706\n",
      "At the end of episode 346 the total reward was : -17.0\n",
      "51/51 [==============================] - 0s 7ms/step - loss: -0.0040 - accuracy: 0.9724\n",
      "At the end of episode 347 the total reward was : -20.0\n",
      "48/48 [==============================] - 0s 7ms/step - loss: -0.0032 - accuracy: 0.9725\n",
      "At the end of episode 348 the total reward was : -16.0\n",
      "55/55 [==============================] - 0s 7ms/step - loss: -0.0157 - accuracy: 0.9769\n",
      "At the end of episode 349 the total reward was : -18.0\n",
      "48/48 [==============================] - 0s 7ms/step - loss: -8.2131e-04 - accuracy: 0.9674\n",
      "At the end of episode 350 the total reward was : -16.0\n",
      "60/60 [==============================] - 0s 7ms/step - loss: -4.0532e-04 - accuracy: 0.9712\n",
      "At the end of episode 351 the total reward was : -19.0\n",
      "63/63 [==============================] - 0s 7ms/step - loss: -0.0077 - accuracy: 0.9744\n",
      "At the end of episode 352 the total reward was : -19.0\n",
      "51/51 [==============================] - 0s 8ms/step - loss: -0.0151 - accuracy: 0.9649\n",
      "At the end of episode 353 the total reward was : -20.0\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.0034 - accuracy: 0.9666\n",
      "At the end of episode 354 the total reward was : -19.0\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 0.0046 - accuracy: 0.9614\n",
      "At the end of episode 355 the total reward was : -17.0\n",
      "71/71 [==============================] - 0s 7ms/step - loss: -0.0121 - accuracy: 0.9612\n",
      "At the end of episode 356 the total reward was : -18.0\n",
      "52/52 [==============================] - 0s 7ms/step - loss: -0.0093 - accuracy: 0.9783\n",
      "At the end of episode 357 the total reward was : -17.0\n",
      "46/46 [==============================] - 0s 7ms/step - loss: -0.0073 - accuracy: 0.9643\n",
      "At the end of episode 358 the total reward was : -20.0\n",
      "54/54 [==============================] - 0s 7ms/step - loss: -0.0156 - accuracy: 0.9655\n",
      "At the end of episode 359 the total reward was : -21.0\n",
      "42/42 [==============================] - 0s 7ms/step - loss: -0.0098 - accuracy: 0.9696\n",
      "At the end of episode 360 the total reward was : -15.0\n",
      "84/84 [==============================] - 1s 7ms/step - loss: -9.4404e-05 - accuracy: 0.9734\n",
      "At the end of episode 361 the total reward was : -17.0\n",
      "78/78 [==============================] - 1s 7ms/step - loss: -0.0013 - accuracy: 0.9638\n",
      "At the end of episode 362 the total reward was : -19.0\n",
      "59/59 [==============================] - 0s 7ms/step - loss: -0.0060 - accuracy: 0.9722\n",
      "At the end of episode 363 the total reward was : -19.0\n",
      "57/57 [==============================] - 0s 7ms/step - loss: 0.0029 - accuracy: 0.9818\n",
      "At the end of episode 364 the total reward was : -19.0\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.0040 - accuracy: 0.9721\n",
      "At the end of episode 365 the total reward was : -18.0\n",
      "85/85 [==============================] - 1s 16ms/step - loss: -0.0024 - accuracy: 0.9770\n",
      "At the end of episode 366 the total reward was : -15.0\n",
      "69/69 [==============================] - 1s 8ms/step - loss: 0.0131 - accuracy: 0.9767\n",
      "At the end of episode 367 the total reward was : -18.0\n",
      "66/66 [==============================] - 0s 7ms/step - loss: -0.0089 - accuracy: 0.9688\n",
      "At the end of episode 368 the total reward was : -19.0\n",
      "57/57 [==============================] - 0s 7ms/step - loss: 9.5052e-04 - accuracy: 0.9673\n",
      "At the end of episode 369 the total reward was : -20.0\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 8.8685e-04 - accuracy: 0.9618\n",
      "At the end of episode 370 the total reward was : -20.0\n",
      "56/56 [==============================] - 0s 8ms/step - loss: 7.4685e-04 - accuracy: 0.9667\n",
      "At the end of episode 371 the total reward was : -18.0\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.0178 - accuracy: 0.9717\n",
      "At the end of episode 372 the total reward was : -15.0\n",
      "75/75 [==============================] - 1s 7ms/step - loss: -0.0043 - accuracy: 0.9663\n",
      "At the end of episode 373 the total reward was : -21.0\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 0.0090 - accuracy: 0.9637\n",
      "At the end of episode 374 the total reward was : -21.0\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 0.0016 - accuracy: 0.9720 \n",
      "At the end of episode 375 the total reward was : -19.0\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 8.1203e-04 - accuracy: 0.9742\n",
      "At the end of episode 376 the total reward was : -18.0\n",
      "65/65 [==============================] - 0s 7ms/step - loss: -0.0022 - accuracy: 0.9731\n",
      "At the end of episode 377 the total reward was : -16.0\n",
      "75/75 [==============================] - 1s 8ms/step - loss: 0.0226 - accuracy: 0.9641\n",
      "At the end of episode 378 the total reward was : -20.0\n",
      "63/63 [==============================] - 1s 8ms/step - loss: -0.0014 - accuracy: 0.9638\n",
      "At the end of episode 379 the total reward was : -21.0\n",
      "61/61 [==============================] - 0s 8ms/step - loss: -0.0104 - accuracy: 0.9678\n",
      "At the end of episode 380 the total reward was : -15.0\n",
      "61/61 [==============================] - 0s 7ms/step - loss: 0.0170 - accuracy: 0.9692\n",
      "At the end of episode 381 the total reward was : -21.0\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.0168 - accuracy: 0.9611\n",
      "At the end of episode 382 the total reward was : -18.0\n",
      "52/52 [==============================] - 0s 7ms/step - loss: -0.0080 - accuracy: 0.9710\n",
      "At the end of episode 383 the total reward was : -18.0\n",
      "64/64 [==============================] - 0s 7ms/step - loss: -0.0050 - accuracy: 0.9798\n",
      "At the end of episode 384 the total reward was : -16.0\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 0.0059 - accuracy: 0.9658\n",
      "At the end of episode 385 the total reward was : -18.0\n",
      "62/62 [==============================] - 0s 8ms/step - loss: 0.0023 - accuracy: 0.9714\n",
      "At the end of episode 386 the total reward was : -19.0\n",
      "59/59 [==============================] - 0s 7ms/step - loss: -0.0047 - accuracy: 0.9716\n",
      "At the end of episode 387 the total reward was : -20.0\n",
      "60/60 [==============================] - 0s 7ms/step - loss: -0.0036 - accuracy: 0.9667\n",
      "At the end of episode 388 the total reward was : -16.0\n",
      "64/64 [==============================] - 0s 8ms/step - loss: -0.0023 - accuracy: 0.9671\n",
      "At the end of episode 389 the total reward was : -15.0\n",
      "69/69 [==============================] - 1s 8ms/step - loss: -0.0066 - accuracy: 0.9674\n",
      "At the end of episode 390 the total reward was : -19.0\n",
      "72/72 [==============================] - 1s 8ms/step - loss: -0.0111 - accuracy: 0.9582\n",
      "At the end of episode 391 the total reward was : -17.0\n",
      "63/63 [==============================] - 0s 8ms/step - loss: -0.0068 - accuracy: 0.9744\n",
      "At the end of episode 392 the total reward was : -18.0\n",
      "56/56 [==============================] - 0s 7ms/step - loss: 8.3733e-04 - accuracy: 0.9736\n",
      "At the end of episode 393 the total reward was : -19.0\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 0.0030 - accuracy: 0.9714\n",
      "At the end of episode 394 the total reward was : -18.0\n",
      "66/66 [==============================] - 1s 8ms/step - loss: 0.0036 - accuracy: 0.9650\n",
      "At the end of episode 395 the total reward was : -20.0\n",
      "60/60 [==============================] - 0s 7ms/step - loss: 0.0074 - accuracy: 0.9735\n",
      "At the end of episode 396 the total reward was : -21.0\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.0084 - accuracy: 0.9756\n",
      "At the end of episode 397 the total reward was : -17.0\n",
      "61/61 [==============================] - 0s 7ms/step - loss: -0.0014 - accuracy: 0.9721\n",
      "At the end of episode 398 the total reward was : -15.0\n",
      "59/59 [==============================] - 0s 8ms/step - loss: 0.0066 - accuracy: 0.9767\n",
      "At the end of episode 399 the total reward was : -19.0\n",
      "51/51 [==============================] - 0s 8ms/step - loss: -0.0139 - accuracy: 0.9679\n",
      "At the end of episode 400 the total reward was : -21.0\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.0020 - accuracy: 0.9671\n",
      "At the end of episode 401 the total reward was : -11.0\n",
      "73/73 [==============================] - 1s 8ms/step - loss: -0.0025 - accuracy: 0.9635\n",
      "At the end of episode 402 the total reward was : -17.0\n",
      "55/55 [==============================] - 0s 8ms/step - loss: 9.9143e-05 - accuracy: 0.9703\n",
      "At the end of episode 403 the total reward was : -17.0\n",
      "82/82 [==============================] - 1s 8ms/step - loss: 0.0051 - accuracy: 0.9682\n",
      "At the end of episode 404 the total reward was : -15.0\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 0.0092 - accuracy: 0.9706\n",
      "At the end of episode 405 the total reward was : -19.0\n",
      "64/64 [==============================] - 0s 7ms/step - loss: -0.0012 - accuracy: 0.9697\n",
      "At the end of episode 406 the total reward was : -18.0\n",
      "56/56 [==============================] - 0s 7ms/step - loss: 0.0115 - accuracy: 0.9758\n",
      "At the end of episode 407 the total reward was : -19.0\n",
      "59/59 [==============================] - 0s 7ms/step - loss: 0.0017 - accuracy: 0.9593\n",
      "At the end of episode 408 the total reward was : -20.0\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 0.0040 - accuracy: 0.9713\n",
      "At the end of episode 409 the total reward was : -17.0\n",
      "57/57 [==============================] - 0s 7ms/step - loss: -3.7477e-04 - accuracy: 0.9741\n",
      "At the end of episode 410 the total reward was : -18.0\n",
      "51/51 [==============================] - 0s 7ms/step - loss: -0.0049 - accuracy: 0.9738\n",
      "At the end of episode 411 the total reward was : -13.0\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0037 - accuracy: 0.9724\n",
      "At the end of episode 412 the total reward was : -14.0\n",
      "74/74 [==============================] - 1s 7ms/step - loss: -0.0012 - accuracy: 0.9705\n",
      "At the end of episode 413 the total reward was : -20.0\n",
      "37/37 [==============================] - 0s 7ms/step - loss: -0.0048 - accuracy: 0.9708\n",
      "At the end of episode 414 the total reward was : -17.0\n",
      "63/63 [==============================] - 0s 7ms/step - loss: -0.0087 - accuracy: 0.9709\n",
      "At the end of episode 415 the total reward was : -18.0\n",
      "64/64 [==============================] - 0s 7ms/step - loss: -0.0015 - accuracy: 0.9690\n",
      "At the end of episode 416 the total reward was : -15.0\n",
      "86/86 [==============================] - 1s 8ms/step - loss: -0.0030 - accuracy: 0.9685\n",
      "At the end of episode 417 the total reward was : -15.0\n",
      "75/75 [==============================] - 1s 7ms/step - loss: 0.0042 - accuracy: 0.9727\n",
      "At the end of episode 418 the total reward was : -20.0\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 4.5992e-04 - accuracy: 0.9749\n",
      "At the end of episode 419 the total reward was : -17.0\n",
      "63/63 [==============================] - 1s 8ms/step - loss: 0.0024 - accuracy: 0.9695\n",
      "At the end of episode 420 the total reward was : -21.0\n",
      "49/49 [==============================] - 0s 8ms/step - loss: 0.0028 - accuracy: 0.9680\n",
      "At the end of episode 421 the total reward was : -18.0\n",
      "60/60 [==============================] - 0s 7ms/step - loss: 0.0237 - accuracy: 0.9590\n",
      "At the end of episode 422 the total reward was : -15.0\n",
      "65/65 [==============================] - 0s 7ms/step - loss: -0.0037 - accuracy: 0.9719\n",
      "At the end of episode 423 the total reward was : -16.0\n",
      "70/70 [==============================] - 0s 7ms/step - loss: -0.0076 - accuracy: 0.9676\n",
      "At the end of episode 424 the total reward was : -17.0\n",
      "52/52 [==============================] - 0s 7ms/step - loss: -0.0032 - accuracy: 0.9731\n",
      "At the end of episode 425 the total reward was : -18.0\n",
      "71/71 [==============================] - 1s 7ms/step - loss: 5.0426e-04 - accuracy: 0.9700\n",
      "At the end of episode 426 the total reward was : -21.0\n",
      "44/44 [==============================] - 0s 8ms/step - loss: -0.0042 - accuracy: 0.9674\n",
      "At the end of episode 427 the total reward was : -16.0\n",
      "75/75 [==============================] - 1s 8ms/step - loss: -0.0195 - accuracy: 0.9645\n",
      "At the end of episode 428 the total reward was : -17.0\n",
      "53/53 [==============================] - 0s 7ms/step - loss: -0.0200 - accuracy: 0.9699\n",
      "At the end of episode 429 the total reward was : -19.0\n",
      "69/69 [==============================] - 0s 7ms/step - loss: 0.0035 - accuracy: 0.9698\n",
      "At the end of episode 430 the total reward was : -19.0\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 0.0102 - accuracy: 0.9566\n",
      "At the end of episode 431 the total reward was : -19.0\n",
      "53/53 [==============================] - 0s 7ms/step - loss: -0.0040 - accuracy: 0.9562\n",
      "At the end of episode 432 the total reward was : -18.0\n",
      "64/64 [==============================] - 0s 7ms/step - loss: -0.0057 - accuracy: 0.9763\n",
      "At the end of episode 433 the total reward was : -16.0\n",
      "74/74 [==============================] - 1s 7ms/step - loss: -0.0059 - accuracy: 0.9595\n",
      "At the end of episode 434 the total reward was : -17.0\n",
      "48/48 [==============================] - 0s 7ms/step - loss: -0.0092 - accuracy: 0.9702\n",
      "At the end of episode 435 the total reward was : -9.0\n",
      "94/94 [==============================] - 1s 7ms/step - loss: -0.0082 - accuracy: 0.9727\n",
      "At the end of episode 436 the total reward was : -16.0\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 0.0013 - accuracy: 0.9622\n",
      "At the end of episode 437 the total reward was : -21.0\n",
      "57/57 [==============================] - 0s 7ms/step - loss: 0.0094 - accuracy: 0.9772\n",
      "At the end of episode 438 the total reward was : -11.0\n",
      "84/84 [==============================] - 1s 7ms/step - loss: -0.0076 - accuracy: 0.9721\n",
      "At the end of episode 439 the total reward was : -14.0\n",
      "93/93 [==============================] - 1s 7ms/step - loss: -0.0133 - accuracy: 0.9742\n",
      "At the end of episode 440 the total reward was : -19.0\n",
      "48/48 [==============================] - 0s 8ms/step - loss: -0.0037 - accuracy: 0.9827\n",
      "At the end of episode 441 the total reward was : -20.0\n",
      "43/43 [==============================] - 0s 7ms/step - loss: -2.5282e-05 - accuracy: 0.9771\n",
      "At the end of episode 442 the total reward was : -17.0\n",
      "71/71 [==============================] - 0s 7ms/step - loss: -0.0106 - accuracy: 0.9608\n",
      "At the end of episode 443 the total reward was : -20.0\n",
      "48/48 [==============================] - 0s 7ms/step - loss: -0.0066 - accuracy: 0.9635\n",
      "At the end of episode 444 the total reward was : -17.0\n",
      "57/57 [==============================] - 0s 7ms/step - loss: -0.0084 - accuracy: 0.9753\n",
      "At the end of episode 445 the total reward was : -9.0\n",
      "81/81 [==============================] - 1s 7ms/step - loss: -3.0554e-04 - accuracy: 0.9643\n",
      "At the end of episode 446 the total reward was : -17.0\n",
      "56/56 [==============================] - 0s 7ms/step - loss: -0.0118 - accuracy: 0.9733\n",
      "At the end of episode 447 the total reward was : -20.0\n",
      "64/64 [==============================] - 0s 7ms/step - loss: 0.0011 - accuracy: 0.9680\n",
      "At the end of episode 448 the total reward was : -20.0\n",
      "68/68 [==============================] - 1s 7ms/step - loss: 0.0216 - accuracy: 0.9637\n",
      "At the end of episode 449 the total reward was : -20.0\n",
      "64/64 [==============================] - 1s 8ms/step - loss: 0.0031 - accuracy: 0.9643\n",
      "At the end of episode 450 the total reward was : -21.0\n",
      "49/49 [==============================] - 0s 7ms/step - loss: -0.0074 - accuracy: 0.9659\n",
      "At the end of episode 451 the total reward was : -20.0\n",
      "50/50 [==============================] - 0s 7ms/step - loss: -0.0090 - accuracy: 0.9636\n",
      "At the end of episode 452 the total reward was : -17.0\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.0011 - accuracy: 0.9813\n",
      "At the end of episode 453 the total reward was : -16.0\n",
      "64/64 [==============================] - 0s 8ms/step - loss: -0.0036 - accuracy: 0.9729\n",
      "At the end of episode 454 the total reward was : -16.0\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 0.0074 - accuracy: 0.9736\n",
      "At the end of episode 455 the total reward was : -18.0\n",
      "54/54 [==============================] - 0s 7ms/step - loss: -0.0116 - accuracy: 0.9691\n",
      "At the end of episode 456 the total reward was : -19.0\n",
      "57/57 [==============================] - 0s 7ms/step - loss: -0.0028 - accuracy: 0.9696\n",
      "At the end of episode 457 the total reward was : -19.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57/57 [==============================] - 0s 7ms/step - loss: -0.0042 - accuracy: 0.9790\n",
      "At the end of episode 458 the total reward was : -16.0\n",
      "66/66 [==============================] - 0s 7ms/step - loss: -0.0024 - accuracy: 0.9512\n",
      "At the end of episode 459 the total reward was : -19.0\n",
      "59/59 [==============================] - 0s 7ms/step - loss: -0.0094 - accuracy: 0.9712\n",
      "At the end of episode 460 the total reward was : -14.0\n",
      "72/72 [==============================] - 1s 7ms/step - loss: -0.0110 - accuracy: 0.9785\n",
      "At the end of episode 461 the total reward was : -10.0\n",
      "91/91 [==============================] - 1s 8ms/step - loss: -7.4595e-04 - accuracy: 0.9715\n",
      "At the end of episode 462 the total reward was : -21.0\n",
      "55/55 [==============================] - 0s 7ms/step - loss: -0.0097 - accuracy: 0.9724\n",
      "At the end of episode 463 the total reward was : -19.0\n",
      "59/59 [==============================] - 0s 7ms/step - loss: 0.0072 - accuracy: 0.9721\n",
      "At the end of episode 464 the total reward was : -15.0\n",
      "65/65 [==============================] - 0s 7ms/step - loss: -0.0173 - accuracy: 0.9701\n",
      "At the end of episode 465 the total reward was : -17.0\n",
      "87/87 [==============================] - 1s 7ms/step - loss: -0.0166 - accuracy: 0.9702\n",
      "At the end of episode 466 the total reward was : -21.0\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0061 - accuracy: 0.9716\n",
      "At the end of episode 467 the total reward was : -18.0\n",
      "70/70 [==============================] - 1s 7ms/step - loss: -0.0033 - accuracy: 0.9692\n",
      "At the end of episode 468 the total reward was : -18.0\n",
      "68/68 [==============================] - 0s 7ms/step - loss: -0.0090 - accuracy: 0.9666\n",
      "At the end of episode 469 the total reward was : -17.0\n",
      "69/69 [==============================] - 1s 8ms/step - loss: 0.0050 - accuracy: 0.9739\n",
      "At the end of episode 470 the total reward was : -13.0\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 0.0027 - accuracy: 0.9699\n",
      "At the end of episode 471 the total reward was : -14.0\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 0.0021 - accuracy: 0.9751\n",
      "At the end of episode 472 the total reward was : -19.0\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0115 - accuracy: 0.9745\n",
      "At the end of episode 473 the total reward was : -20.0\n",
      "66/66 [==============================] - 1s 8ms/step - loss: -0.0054 - accuracy: 0.9732\n",
      "At the end of episode 474 the total reward was : -20.0\n",
      "60/60 [==============================] - 0s 7ms/step - loss: -9.7014e-05 - accuracy: 0.9684\n",
      "At the end of episode 475 the total reward was : -16.0\n",
      "51/51 [==============================] - 0s 7ms/step - loss: -0.0074 - accuracy: 0.9709A: 0s - loss: -0.0167 - accuracy: 0.\n",
      "At the end of episode 476 the total reward was : -20.0\n",
      "60/60 [==============================] - 0s 7ms/step - loss: 0.0072 - accuracy: 0.9791\n",
      "At the end of episode 477 the total reward was : -17.0\n",
      "67/67 [==============================] - 0s 7ms/step - loss: -0.0018 - accuracy: 0.9793\n",
      "At the end of episode 478 the total reward was : -20.0\n",
      "50/50 [==============================] - 0s 7ms/step - loss: -0.0023 - accuracy: 0.9743\n",
      "At the end of episode 479 the total reward was : -16.0\n",
      "70/70 [==============================] - 1s 8ms/step - loss: -0.0049 - accuracy: 0.9810\n",
      "At the end of episode 480 the total reward was : -15.0\n",
      "75/75 [==============================] - 1s 8ms/step - loss: -9.3965e-04 - accuracy: 0.9776\n",
      "At the end of episode 481 the total reward was : -15.0\n",
      "59/59 [==============================] - 0s 8ms/step - loss: -0.0093 - accuracy: 0.9735\n",
      "At the end of episode 482 the total reward was : -12.0\n",
      "72/72 [==============================] - 1s 7ms/step - loss: -0.0068 - accuracy: 0.9782\n",
      "At the end of episode 483 the total reward was : -20.0\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 0.0038 - accuracy: 0.9761\n",
      "At the end of episode 484 the total reward was : -20.0\n",
      "60/60 [==============================] - 0s 7ms/step - loss: 0.0015 - accuracy: 0.9722\n",
      "At the end of episode 485 the total reward was : -17.0\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0016 - accuracy: 0.9752\n",
      "At the end of episode 486 the total reward was : -21.0\n",
      "49/49 [==============================] - 0s 7ms/step - loss: -0.0010 - accuracy: 0.9757\n",
      "At the end of episode 487 the total reward was : -21.0\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 0.0037 - accuracy: 0.9770\n",
      "At the end of episode 488 the total reward was : -18.0\n",
      "62/62 [==============================] - 0s 8ms/step - loss: 0.0028 - accuracy: 0.9791\n",
      "At the end of episode 489 the total reward was : -20.0\n",
      "58/58 [==============================] - 0s 7ms/step - loss: -0.0157 - accuracy: 0.9711\n",
      "At the end of episode 490 the total reward was : -18.0\n",
      "66/66 [==============================] - 0s 7ms/step - loss: -1.4621e-04 - accuracy: 0.9646\n",
      "At the end of episode 491 the total reward was : -17.0\n",
      "59/59 [==============================] - 0s 7ms/step - loss: -0.0019 - accuracy: 0.9717\n",
      "At the end of episode 492 the total reward was : -17.0\n",
      "61/61 [==============================] - 0s 7ms/step - loss: -0.0028 - accuracy: 0.9778\n",
      "At the end of episode 493 the total reward was : -13.0\n",
      "67/67 [==============================] - 0s 7ms/step - loss: -0.0023 - accuracy: 0.9803\n",
      "At the end of episode 494 the total reward was : -19.0\n",
      "34/34 [==============================] - 0s 7ms/step - loss: -0.0081 - accuracy: 0.9814\n",
      "At the end of episode 495 the total reward was : -16.0\n",
      "56/56 [==============================] - 0s 7ms/step - loss: -0.0015 - accuracy: 0.9748\n",
      "At the end of episode 496 the total reward was : -17.0\n",
      "69/69 [==============================] - 0s 7ms/step - loss: -0.0108 - accuracy: 0.9703\n",
      "At the end of episode 497 the total reward was : -18.0\n",
      "50/50 [==============================] - 0s 7ms/step - loss: -0.0126 - accuracy: 0.9724\n",
      "At the end of episode 498 the total reward was : -18.0\n",
      "52/52 [==============================] - 0s 7ms/step - loss: -0.0106 - accuracy: 0.9723\n",
      "At the end of episode 499 the total reward was : -19.0\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0074 - accuracy: 0.9756\n",
      "At the end of episode 500 the total reward was : -15.0\n",
      "61/61 [==============================] - 0s 7ms/step - loss: -0.0079 - accuracy: 0.9774\n",
      "At the end of episode 501 the total reward was : -18.0\n",
      "75/75 [==============================] - 1s 8ms/step - loss: 6.5268e-04 - accuracy: 0.9753\n",
      "At the end of episode 502 the total reward was : -20.0\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 9.5708e-04 - accuracy: 0.9802\n",
      "At the end of episode 503 the total reward was : -17.0\n",
      "72/72 [==============================] - 0s 7ms/step - loss: -0.0077 - accuracy: 0.9800\n",
      "At the end of episode 504 the total reward was : -14.0\n",
      "60/60 [==============================] - 0s 7ms/step - loss: 0.0028 - accuracy: 0.9707\n",
      "At the end of episode 505 the total reward was : -20.0\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.0069 - accuracy: 0.9776\n",
      "At the end of episode 506 the total reward was : -19.0\n",
      "51/51 [==============================] - 0s 8ms/step - loss: 0.0112 - accuracy: 0.9741\n",
      "At the end of episode 507 the total reward was : -21.0\n",
      "55/55 [==============================] - 0s 8ms/step - loss: -0.0081 - accuracy: 0.9689\n",
      "At the end of episode 508 the total reward was : -16.0\n",
      "68/68 [==============================] - 0s 7ms/step - loss: -0.0060 - accuracy: 0.9745\n",
      "At the end of episode 509 the total reward was : -17.0\n",
      "46/46 [==============================] - 0s 7ms/step - loss: -0.0096 - accuracy: 0.9772\n",
      "At the end of episode 510 the total reward was : -15.0\n",
      "67/67 [==============================] - 0s 7ms/step - loss: -0.0112 - accuracy: 0.9741\n",
      "At the end of episode 511 the total reward was : -13.0\n",
      "63/63 [==============================] - 0s 8ms/step - loss: -0.0117 - accuracy: 0.9767\n",
      "At the end of episode 512 the total reward was : -14.0\n",
      "62/62 [==============================] - 0s 7ms/step - loss: -0.0049 - accuracy: 0.9803\n",
      "At the end of episode 513 the total reward was : -18.0\n",
      "47/47 [==============================] - 0s 8ms/step - loss: 0.0039 - accuracy: 0.9710\n",
      "At the end of episode 514 the total reward was : -21.0\n",
      "64/64 [==============================] - 1s 8ms/step - loss: 0.0196 - accuracy: 0.9702\n",
      "At the end of episode 515 the total reward was : -6.0\n",
      "94/94 [==============================] - 1s 7ms/step - loss: 6.2112e-05 - accuracy: 0.9822\n",
      "At the end of episode 516 the total reward was : -16.0\n",
      "70/70 [==============================] - 1s 7ms/step - loss: -0.0119 - accuracy: 0.9761\n",
      "At the end of episode 517 the total reward was : -18.0\n",
      "43/43 [==============================] - 0s 7ms/step - loss: -0.0011 - accuracy: 0.9830\n",
      "At the end of episode 518 the total reward was : -16.0\n",
      "72/72 [==============================] - 1s 7ms/step - loss: -0.0078 - accuracy: 0.9702\n",
      "At the end of episode 519 the total reward was : -14.0\n",
      "64/64 [==============================] - 0s 7ms/step - loss: -0.0081 - accuracy: 0.9848\n",
      "At the end of episode 520 the total reward was : -11.0\n",
      "71/71 [==============================] - 0s 7ms/step - loss: 0.0047 - accuracy: 0.9823\n",
      "At the end of episode 521 the total reward was : -11.0\n",
      "75/75 [==============================] - 1s 8ms/step - loss: -0.0068 - accuracy: 0.9731\n",
      "At the end of episode 522 the total reward was : -18.0\n",
      "43/43 [==============================] - 0s 7ms/step - loss: -0.0199 - accuracy: 0.9735\n",
      "At the end of episode 523 the total reward was : -15.0\n",
      "65/65 [==============================] - 0s 7ms/step - loss: -0.0097 - accuracy: 0.9787\n",
      "At the end of episode 524 the total reward was : -14.0\n",
      "66/66 [==============================] - 0s 7ms/step - loss: -0.0039 - accuracy: 0.9791\n",
      "At the end of episode 525 the total reward was : -17.0\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 0.0013 - accuracy: 0.9819\n",
      "At the end of episode 526 the total reward was : -20.0\n",
      "56/56 [==============================] - 0s 8ms/step - loss: -0.0022 - accuracy: 0.9752\n",
      "At the end of episode 527 the total reward was : -17.0\n",
      "61/61 [==============================] - 0s 7ms/step - loss: -0.0096 - accuracy: 0.9746\n",
      "At the end of episode 528 the total reward was : -8.0\n",
      "94/94 [==============================] - 1s 7ms/step - loss: -0.0052 - accuracy: 0.9718\n",
      "At the end of episode 529 the total reward was : -16.0\n",
      "60/60 [==============================] - 0s 7ms/step - loss: -0.0014 - accuracy: 0.9765\n",
      "At the end of episode 530 the total reward was : -19.0\n",
      "57/57 [==============================] - 0s 7ms/step - loss: 0.0031 - accuracy: 0.9713\n",
      "At the end of episode 531 the total reward was : -14.0\n",
      "76/76 [==============================] - 1s 8ms/step - loss: -0.0039 - accuracy: 0.9775\n",
      "At the end of episode 532 the total reward was : -16.0\n",
      "68/68 [==============================] - 0s 7ms/step - loss: -0.0013 - accuracy: 0.9800\n",
      "At the end of episode 533 the total reward was : -16.0\n",
      "49/49 [==============================] - 0s 8ms/step - loss: -0.0028 - accuracy: 0.9813\n",
      "At the end of episode 534 the total reward was : -14.0\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 0.0078 - accuracy: 0.9731\n",
      "At the end of episode 535 the total reward was : -13.0\n",
      "73/73 [==============================] - 1s 8ms/step - loss: 0.0123 - accuracy: 0.9750\n",
      "At the end of episode 536 the total reward was : -10.0\n",
      "72/72 [==============================] - 1s 7ms/step - loss: -0.0054 - accuracy: 0.9695\n",
      "At the end of episode 537 the total reward was : -11.0\n",
      "81/81 [==============================] - 1s 7ms/step - loss: -0.0173 - accuracy: 0.9716\n",
      "At the end of episode 538 the total reward was : -15.0\n",
      "79/79 [==============================] - 1s 7ms/step - loss: -0.0119 - accuracy: 0.9748\n",
      "At the end of episode 539 the total reward was : -18.0\n",
      "73/73 [==============================] - 1s 7ms/step - loss: -0.0035 - accuracy: 0.9755\n",
      "At the end of episode 540 the total reward was : -19.0\n",
      " 1/65 [..............................] - ETA: 0s - loss: 0.0017 - accuracy: 1.0000WARNING:tensorflow:Callbacks method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0046s vs `on_train_batch_begin` time: 0.0085s). Check your callbacks.\n",
      "65/65 [==============================] - 0s 7ms/step - loss: -0.0070 - accuracy: 0.9693\n",
      "At the end of episode 541 the total reward was : -15.0\n",
      "75/75 [==============================] - 1s 8ms/step - loss: -0.0083 - accuracy: 0.9680\n",
      "At the end of episode 542 the total reward was : -18.0\n",
      "58/58 [==============================] - 0s 7ms/step - loss: -0.0016 - accuracy: 0.9745\n",
      "At the end of episode 543 the total reward was : -16.0\n",
      "51/51 [==============================] - 0s 7ms/step - loss: 0.0112 - accuracy: 0.9801\n",
      "At the end of episode 544 the total reward was : -17.0\n",
      "67/67 [==============================] - 1s 7ms/step - loss: -0.0119 - accuracy: 0.9803\n",
      "At the end of episode 545 the total reward was : -20.0\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 0.0059 - accuracy: 0.9738 \n",
      "At the end of episode 546 the total reward was : -17.0\n",
      "67/67 [==============================] - 1s 7ms/step - loss: -0.0063 - accuracy: 0.9835\n",
      "At the end of episode 547 the total reward was : -15.0\n",
      "69/69 [==============================] - 1s 7ms/step - loss: -0.0015 - accuracy: 0.9768\n",
      "At the end of episode 548 the total reward was : -9.0\n",
      "89/89 [==============================] - 1s 7ms/step - loss: -0.0020 - accuracy: 0.9773\n",
      "At the end of episode 549 the total reward was : -17.0\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.0078 - accuracy: 0.9670\n",
      "At the end of episode 550 the total reward was : -16.0\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 0.0069 - accuracy: 0.9829\n",
      "At the end of episode 551 the total reward was : -20.0\n",
      "62/62 [==============================] - 0s 8ms/step - loss: -0.0107 - accuracy: 0.9754\n",
      "At the end of episode 552 the total reward was : -19.0\n",
      "74/74 [==============================] - 1s 8ms/step - loss: -0.0014 - accuracy: 0.9694\n",
      "At the end of episode 553 the total reward was : -20.0\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.0076 - accuracy: 0.9830\n",
      "At the end of episode 554 the total reward was : -13.0\n",
      "71/71 [==============================] - 1s 8ms/step - loss: -0.0122 - accuracy: 0.9738\n",
      "At the end of episode 555 the total reward was : -18.0\n",
      "56/56 [==============================] - 1s 15ms/step - loss: -0.0079 - accuracy: 0.9713\n",
      "At the end of episode 556 the total reward was : -16.0\n",
      "57/57 [==============================] - 0s 8ms/step - loss: -0.0157 - accuracy: 0.9727\n",
      "At the end of episode 557 the total reward was : -15.0\n",
      "73/73 [==============================] - 1s 8ms/step - loss: -0.0199 - accuracy: 0.9668\n",
      "At the end of episode 558 the total reward was : -12.0\n",
      "82/82 [==============================] - 1s 9ms/step - loss: 0.0078 - accuracy: 0.9746\n",
      "At the end of episode 559 the total reward was : -12.0\n",
      "76/76 [==============================] - 1s 8ms/step - loss: -0.0029 - accuracy: 0.9776\n",
      "At the end of episode 560 the total reward was : -16.0\n",
      "85/85 [==============================] - 1s 9ms/step - loss: -0.0022 - accuracy: 0.9682\n",
      "At the end of episode 561 the total reward was : -18.0\n",
      "69/69 [==============================] - 1s 9ms/step - loss: -0.0035 - accuracy: 0.9719\n",
      "At the end of episode 562 the total reward was : -21.0\n",
      "51/51 [==============================] - 0s 8ms/step - loss: -0.0020 - accuracy: 0.9660\n",
      "At the end of episode 563 the total reward was : -17.0\n",
      "63/63 [==============================] - 1s 8ms/step - loss: 0.0044 - accuracy: 0.9740\n",
      "At the end of episode 564 the total reward was : -20.0\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 3.9543e-04 - accuracy: 0.9820\n",
      "At the end of episode 565 the total reward was : -19.0\n",
      "59/59 [==============================] - 0s 8ms/step - loss: -0.0014 - accuracy: 0.9749\n",
      "At the end of episode 566 the total reward was : -16.0\n",
      "56/56 [==============================] - 1s 10ms/step - loss: 0.0030 - accuracy: 0.9731\n",
      "At the end of episode 567 the total reward was : -18.0\n",
      "58/58 [==============================] - 0s 8ms/step - loss: -0.0013 - accuracy: 0.9826\n",
      "At the end of episode 568 the total reward was : -17.0\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 7.4213e-04 - accuracy: 0.9788\n",
      "At the end of episode 569 the total reward was : -19.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59/59 [==============================] - 0s 8ms/step - loss: -0.0048 - accuracy: 0.9738\n",
      "At the end of episode 570 the total reward was : -15.0\n",
      "75/75 [==============================] - 1s 8ms/step - loss: -0.0035 - accuracy: 0.9739\n",
      "At the end of episode 571 the total reward was : -13.0\n",
      "81/81 [==============================] - 1s 8ms/step - loss: 0.0033 - accuracy: 0.9762\n",
      "At the end of episode 572 the total reward was : -16.0\n",
      "58/58 [==============================] - 0s 8ms/step - loss: 0.0062 - accuracy: 0.9794\n",
      "At the end of episode 573 the total reward was : -12.0\n",
      "104/104 [==============================] - 1s 8ms/step - loss: -0.0191 - accuracy: 0.9708\n",
      "At the end of episode 574 the total reward was : -16.0\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 7.0399e-04 - accuracy: 0.9833\n",
      "At the end of episode 575 the total reward was : -18.0\n",
      "50/50 [==============================] - 0s 7ms/step - loss: -0.0118 - accuracy: 0.9762\n",
      "At the end of episode 576 the total reward was : -14.0\n",
      "62/62 [==============================] - 0s 7ms/step - loss: -0.0158 - accuracy: 0.9717\n",
      "At the end of episode 577 the total reward was : -18.0\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 0.0026 - accuracy: 0.9804\n",
      "At the end of episode 578 the total reward was : -16.0\n",
      "85/85 [==============================] - 1s 7ms/step - loss: -6.7127e-04 - accuracy: 0.9770\n",
      "At the end of episode 579 the total reward was : -18.0\n",
      "75/75 [==============================] - 1s 8ms/step - loss: 0.0028 - accuracy: 0.9720\n",
      "At the end of episode 580 the total reward was : -18.0\n",
      "51/51 [==============================] - 0s 7ms/step - loss: -0.0211 - accuracy: 0.9757\n",
      "At the end of episode 581 the total reward was : -19.0\n",
      "42/42 [==============================] - 0s 7ms/step - loss: -0.0014 - accuracy: 0.9690\n",
      "At the end of episode 582 the total reward was : -19.0\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0070 - accuracy: 0.9733\n",
      "At the end of episode 583 the total reward was : -18.0\n",
      "45/45 [==============================] - 0s 7ms/step - loss: -4.4649e-05 - accuracy: 0.9788\n",
      "At the end of episode 584 the total reward was : -15.0\n",
      "82/82 [==============================] - 1s 7ms/step - loss: -6.2062e-04 - accuracy: 0.9748\n",
      "At the end of episode 585 the total reward was : -16.0\n",
      "77/77 [==============================] - 1s 7ms/step - loss: 0.0016 - accuracy: 0.9784\n",
      "At the end of episode 586 the total reward was : -18.0\n",
      " 1/45 [..............................] - ETA: 0s - loss: -0.0224 - accuracy: 0.9688WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0016s vs `on_train_batch_end` time: 0.0070s). Check your callbacks.\n",
      "45/45 [==============================] - 0s 7ms/step - loss: -0.0081 - accuracy: 0.9703\n",
      "At the end of episode 587 the total reward was : -11.0\n",
      "90/90 [==============================] - 1s 7ms/step - loss: -0.0179 - accuracy: 0.9669\n",
      "At the end of episode 588 the total reward was : -21.0\n",
      "55/55 [==============================] - 0s 7ms/step - loss: -0.0082 - accuracy: 0.9782A: 0s - loss: -0.0103 - accuracy: 0.\n",
      "At the end of episode 589 the total reward was : -16.0\n",
      "60/60 [==============================] - 0s 8ms/step - loss: -0.0070 - accuracy: 0.9795\n",
      "At the end of episode 590 the total reward was : -18.0\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.0110 - accuracy: 0.9837\n",
      "At the end of episode 591 the total reward was : -20.0\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.0051 - accuracy: 0.9785\n",
      "At the end of episode 592 the total reward was : -19.0\n",
      "62/62 [==============================] - 0s 7ms/step - loss: -0.0044 - accuracy: 0.9713\n",
      "At the end of episode 593 the total reward was : -16.0\n",
      "68/68 [==============================] - 0s 7ms/step - loss: 2.7148e-04 - accuracy: 0.9754\n",
      "At the end of episode 594 the total reward was : -17.0\n",
      "61/61 [==============================] - 0s 7ms/step - loss: -0.0096 - accuracy: 0.9798\n",
      "At the end of episode 595 the total reward was : -20.0\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.0045 - accuracy: 0.9794\n",
      "At the end of episode 596 the total reward was : -18.0\n",
      "62/62 [==============================] - 0s 7ms/step - loss: -0.0067 - accuracy: 0.9806 0s - loss: 0.0086 - accuracy:\n",
      "At the end of episode 597 the total reward was : -19.0\n",
      "74/74 [==============================] - 1s 8ms/step - loss: -0.0060 - accuracy: 0.9886\n",
      "At the end of episode 598 the total reward was : -20.0\n",
      "56/56 [==============================] - 0s 7ms/step - loss: -7.1349e-04 - accuracy: 0.9797\n",
      "At the end of episode 599 the total reward was : -18.0\n",
      "73/73 [==============================] - 1s 7ms/step - loss: -0.0067 - accuracy: 0.9806\n",
      "At the end of episode 600 the total reward was : -18.0\n",
      "58/58 [==============================] - 0s 7ms/step - loss: -0.0074 - accuracy: 0.9734\n",
      "At the end of episode 601 the total reward was : -4.0\n",
      "107/107 [==============================] - 1s 8ms/step - loss: -0.0039 - accuracy: 0.9763\n",
      "At the end of episode 602 the total reward was : -20.0\n",
      "58/58 [==============================] - 0s 7ms/step - loss: 0.0026 - accuracy: 0.9799\n",
      "At the end of episode 603 the total reward was : -17.0\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 0.0074 - accuracy: 0.9742\n",
      "At the end of episode 604 the total reward was : -18.0\n",
      "68/68 [==============================] - 0s 7ms/step - loss: -4.8116e-05 - accuracy: 0.9824\n",
      "At the end of episode 605 the total reward was : -18.0\n",
      "62/62 [==============================] - 0s 7ms/step - loss: -0.0015 - accuracy: 0.9781\n",
      "At the end of episode 606 the total reward was : -18.0\n",
      "77/77 [==============================] - 1s 7ms/step - loss: -0.0116 - accuracy: 0.9792\n",
      "At the end of episode 607 the total reward was : -18.0\n",
      "60/60 [==============================] - 0s 7ms/step - loss: -0.0053 - accuracy: 0.9869\n",
      "At the end of episode 608 the total reward was : -20.0\n",
      "88/88 [==============================] - 1s 8ms/step - loss: -0.0062 - accuracy: 0.9854\n",
      "At the end of episode 609 the total reward was : -19.0\n",
      "49/49 [==============================] - 0s 8ms/step - loss: 0.0166 - accuracy: 0.9840\n",
      "At the end of episode 610 the total reward was : -13.0\n",
      "71/71 [==============================] - 1s 7ms/step - loss: 0.0104 - accuracy: 0.9757\n",
      "At the end of episode 611 the total reward was : -19.0\n",
      "68/68 [==============================] - 0s 7ms/step - loss: -0.0068 - accuracy: 0.9797\n",
      "At the end of episode 612 the total reward was : -14.0\n",
      "76/76 [==============================] - 1s 7ms/step - loss: 0.0065 - accuracy: 0.9796\n",
      "At the end of episode 613 the total reward was : -11.0\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 4.2751e-04 - accuracy: 0.9767\n",
      "At the end of episode 614 the total reward was : -18.0\n",
      "73/73 [==============================] - 1s 8ms/step - loss: 0.0042 - accuracy: 0.9836\n",
      "At the end of episode 615 the total reward was : -18.0\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 0.0029 - accuracy: 0.9797\n",
      "At the end of episode 616 the total reward was : -16.0\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 5.8378e-04 - accuracy: 0.9791\n",
      "At the end of episode 617 the total reward was : -20.0\n",
      "48/48 [==============================] - 0s 7ms/step - loss: -0.0012 - accuracy: 0.9784\n",
      "At the end of episode 618 the total reward was : -18.0\n",
      "60/60 [==============================] - 0s 7ms/step - loss: -0.0017 - accuracy: 0.9853\n",
      "At the end of episode 619 the total reward was : -15.0\n",
      "82/82 [==============================] - 1s 7ms/step - loss: -0.0071 - accuracy: 0.9836\n",
      "At the end of episode 620 the total reward was : -21.0\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 0.0022 - accuracy: 0.9768\n",
      "At the end of episode 621 the total reward was : -20.0\n",
      "56/56 [==============================] - 0s 7ms/step - loss: 0.0029 - accuracy: 0.9808\n",
      "At the end of episode 622 the total reward was : -18.0\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 0.0057 - accuracy: 0.9838\n",
      "At the end of episode 623 the total reward was : -17.0\n",
      "72/72 [==============================] - 0s 7ms/step - loss: 8.2113e-04 - accuracy: 0.9752\n",
      "At the end of episode 624 the total reward was : -19.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 0s 7ms/step - loss: 0.0069 - accuracy: 0.9810\n",
      "At the end of episode 625 the total reward was : -20.0\n",
      "48/48 [==============================] - 0s 7ms/step - loss: -6.0249e-04 - accuracy: 0.9732\n",
      "At the end of episode 626 the total reward was : -20.0\n",
      "52/52 [==============================] - 0s 8ms/step - loss: -0.0023 - accuracy: 0.9831\n",
      "At the end of episode 627 the total reward was : -16.0\n",
      "76/76 [==============================] - 1s 7ms/step - loss: 0.0033 - accuracy: 0.9784\n",
      "At the end of episode 628 the total reward was : -18.0\n",
      "64/64 [==============================] - 0s 7ms/step - loss: -0.0062 - accuracy: 0.9773\n",
      "At the end of episode 629 the total reward was : -12.0\n",
      "70/70 [==============================] - 1s 9ms/step - loss: -0.0054 - accuracy: 0.9888\n",
      "At the end of episode 630 the total reward was : -21.0\n",
      "55/55 [==============================] - 0s 8ms/step - loss: -0.0056 - accuracy: 0.9781\n",
      "At the end of episode 631 the total reward was : -20.0\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 0.0016 - accuracy: 0.9836\n",
      "At the end of episode 632 the total reward was : -19.0\n",
      "61/61 [==============================] - 0s 7ms/step - loss: -0.0034 - accuracy: 0.9798\n",
      "At the end of episode 633 the total reward was : -19.0\n",
      "67/67 [==============================] - 0s 7ms/step - loss: -0.0045 - accuracy: 0.9796\n",
      "At the end of episode 634 the total reward was : -16.0\n",
      "68/68 [==============================] - 1s 8ms/step - loss: -0.0150 - accuracy: 0.9746\n",
      "At the end of episode 635 the total reward was : -18.0\n",
      "69/69 [==============================] - 1s 7ms/step - loss: 0.0126 - accuracy: 0.9837\n",
      "At the end of episode 636 the total reward was : -19.0\n",
      "57/57 [==============================] - 0s 8ms/step - loss: -0.0011 - accuracy: 0.9884\n",
      "At the end of episode 637 the total reward was : -13.0\n",
      "83/83 [==============================] - 1s 9ms/step - loss: 0.0028 - accuracy: 0.9863\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mD:\\Program files\\Anaconda\\envs\\pong\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:3873\u001b[0m, in \u001b[0;36mGraph._get_op_def\u001b[1;34m(self, type)\u001b[0m\n\u001b[0;32m   3872\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3873\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_op_def_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m   3874\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Const'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [37]\u001b[0m, in \u001b[0;36m<cell line: 14>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# forward the policy network and sample action according to the probability distribution\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;124;03mRunning model.predict to know what the current model thinks about the probability of doing the UP_ACTION, \u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;124;03mgiven the current frame setting.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;124;03mKeras requires a third?? dimension perhaps hre\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m proba \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand_dims\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# proba=model.predict(np.expand_dims(x.reshape(80,80), axis=0)) 2D model stuff DELETE\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \n\u001b[0;32m     33\u001b[0m \n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m#Intorucing another probability distirubtion here, not sure\u001b[39;00m\n\u001b[0;32m     35\u001b[0m action \u001b[38;5;241m=\u001b[39m UP_ACTION \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39muniform() \u001b[38;5;241m<\u001b[39m proba \u001b[38;5;28;01melse\u001b[39;00m DOWN_ACTION\n",
      "File \u001b[1;32mD:\\Program files\\Anaconda\\envs\\pong\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:130\u001b[0m, in \u001b[0;36mdisable_multi_worker.<locals>._method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_multi_worker_mode():  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    128\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m is not supported in multi-worker mode.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    129\u001b[0m       method\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m))\n\u001b[1;32m--> 130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Program files\\Anaconda\\envs\\pong\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1569\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1566\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1567\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_strategy\u001b[38;5;241m.\u001b[39mscope():\n\u001b[0;32m   1568\u001b[0m   \u001b[38;5;66;03m# Creates a `tf.data.Dataset` and handles batch and epoch iteration.\u001b[39;00m\n\u001b[1;32m-> 1569\u001b[0m   data_handler \u001b[38;5;241m=\u001b[39m \u001b[43mdata_adapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataHandler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1570\u001b[0m \u001b[43m      \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1571\u001b[0m \u001b[43m      \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1572\u001b[0m \u001b[43m      \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1573\u001b[0m \u001b[43m      \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1574\u001b[0m \u001b[43m      \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1575\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1576\u001b[0m \u001b[43m      \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1577\u001b[0m \u001b[43m      \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1578\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1579\u001b[0m \u001b[43m      \u001b[49m\u001b[43msteps_per_execution\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_steps_per_execution\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1581\u001b[0m   \u001b[38;5;66;03m# Container that configures and calls `tf.keras.Callback`s.\u001b[39;00m\n\u001b[0;32m   1582\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(callbacks, callbacks_module\u001b[38;5;241m.\u001b[39mCallbackList):\n",
      "File \u001b[1;32mD:\\Program files\\Anaconda\\envs\\pong\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py:1105\u001b[0m, in \u001b[0;36mDataHandler.__init__\u001b[1;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution)\u001b[0m\n\u001b[0;32m   1102\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_steps_per_execution_value \u001b[38;5;241m=\u001b[39m steps_per_execution\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m   1104\u001b[0m adapter_cls \u001b[38;5;241m=\u001b[39m select_data_adapter(x, y)\n\u001b[1;32m-> 1105\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_adapter \u001b[38;5;241m=\u001b[39m \u001b[43madapter_cls\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1106\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1107\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1108\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1109\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1110\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1111\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1112\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1113\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1114\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1115\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1116\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdistribution_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mds_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_strategy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1117\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1119\u001b[0m strategy \u001b[38;5;241m=\u001b[39m ds_context\u001b[38;5;241m.\u001b[39mget_strategy()\n\u001b[0;32m   1120\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_adapter\u001b[38;5;241m.\u001b[39mget_dataset()\n",
      "File \u001b[1;32mD:\\Program files\\Anaconda\\envs\\pong\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py:328\u001b[0m, in \u001b[0;36mTensorLikeDataAdapter.__init__\u001b[1;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[0;32m    322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m indices\n\u001b[0;32m    324\u001b[0m \u001b[38;5;66;03m# We prefetch a single element. Computing large permutations can take quite\u001b[39;00m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;66;03m# a while so we don't want to wait for prefetching over an epoch boundary to\u001b[39;00m\n\u001b[0;32m    326\u001b[0m \u001b[38;5;66;03m# trigger the next permutation. On the other hand, too many simultaneous\u001b[39;00m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;66;03m# shuffles can contend on a hardware level and degrade all performance.\u001b[39;00m\n\u001b[1;32m--> 328\u001b[0m indices_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mindices_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpermutation\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mprefetch(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    330\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mslice_batch_indices\u001b[39m(indices):\n\u001b[0;32m    331\u001b[0m   \u001b[38;5;124;03m\"\"\"Convert a Tensor of indices into a dataset of batched indices.\u001b[39;00m\n\u001b[0;32m    332\u001b[0m \n\u001b[0;32m    333\u001b[0m \u001b[38;5;124;03m  This step can be accomplished in several ways. The most natural is to\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[38;5;124;03m    A Dataset of batched indices.\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Program files\\Anaconda\\envs\\pong\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:1695\u001b[0m, in \u001b[0;36mDatasetV2.map\u001b[1;34m(self, map_func, num_parallel_calls, deterministic)\u001b[0m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;124;03m\"\"\"Maps `map_func` across the elements of this dataset.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \n\u001b[0;32m   1560\u001b[0m \u001b[38;5;124;03mThis transformation applies `map_func` to each element of this dataset, and\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1692\u001b[0m \u001b[38;5;124;03m  Dataset: A `Dataset`.\u001b[39;00m\n\u001b[0;32m   1693\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1694\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_parallel_calls \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1695\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mMapDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreserve_cardinality\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   1696\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1697\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m ParallelMapDataset(\n\u001b[0;32m   1698\u001b[0m       \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1699\u001b[0m       map_func,\n\u001b[0;32m   1700\u001b[0m       num_parallel_calls,\n\u001b[0;32m   1701\u001b[0m       deterministic,\n\u001b[0;32m   1702\u001b[0m       preserve_cardinality\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mD:\\Program files\\Anaconda\\envs\\pong\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:4041\u001b[0m, in \u001b[0;36mMapDataset.__init__\u001b[1;34m(self, input_dataset, map_func, use_inter_op_parallelism, preserve_cardinality, use_legacy_function)\u001b[0m\n\u001b[0;32m   4039\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_inter_op_parallelism \u001b[38;5;241m=\u001b[39m use_inter_op_parallelism\n\u001b[0;32m   4040\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_preserve_cardinality \u001b[38;5;241m=\u001b[39m preserve_cardinality\n\u001b[1;32m-> 4041\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_map_func \u001b[38;5;241m=\u001b[39m \u001b[43mStructuredFunctionWrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4042\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmap_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4043\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transformation_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4044\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4045\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_legacy_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_legacy_function\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4046\u001b[0m variant_tensor \u001b[38;5;241m=\u001b[39m gen_dataset_ops\u001b[38;5;241m.\u001b[39mmap_dataset(\n\u001b[0;32m   4047\u001b[0m     input_dataset\u001b[38;5;241m.\u001b[39m_variant_tensor,  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   4048\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_map_func\u001b[38;5;241m.\u001b[39mfunction\u001b[38;5;241m.\u001b[39mcaptured_inputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4051\u001b[0m     preserve_cardinality\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_preserve_cardinality,\n\u001b[0;32m   4052\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_structure)\n\u001b[0;32m   4053\u001b[0m \u001b[38;5;28msuper\u001b[39m(MapDataset, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(input_dataset, variant_tensor)\n",
      "File \u001b[1;32mD:\\Program files\\Anaconda\\envs\\pong\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:3371\u001b[0m, in \u001b[0;36mStructuredFunctionWrapper.__init__\u001b[1;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[0;32m   3368\u001b[0m resource_tracker \u001b[38;5;241m=\u001b[39m tracking\u001b[38;5;241m.\u001b[39mResourceTracker()\n\u001b[0;32m   3369\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tracking\u001b[38;5;241m.\u001b[39mresource_tracker_scope(resource_tracker):\n\u001b[0;32m   3370\u001b[0m   \u001b[38;5;66;03m# TODO(b/141462134): Switch to using garbage collection.\u001b[39;00m\n\u001b[1;32m-> 3371\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function \u001b[38;5;241m=\u001b[39m \u001b[43mwrapper_fn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_concrete_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3372\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m add_to_graph:\n\u001b[0;32m   3373\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function\u001b[38;5;241m.\u001b[39madd_to_graph(ops\u001b[38;5;241m.\u001b[39mget_default_graph())\n",
      "File \u001b[1;32mD:\\Program files\\Anaconda\\envs\\pong\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2938\u001b[0m, in \u001b[0;36mFunction.get_concrete_function\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2931\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_concrete_function\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   2932\u001b[0m   \u001b[38;5;124;03m\"\"\"Returns a `ConcreteFunction` specialized to inputs and execution context.\u001b[39;00m\n\u001b[0;32m   2933\u001b[0m \n\u001b[0;32m   2934\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[0;32m   2935\u001b[0m \u001b[38;5;124;03m    *args: inputs to specialize on.\u001b[39;00m\n\u001b[0;32m   2936\u001b[0m \u001b[38;5;124;03m    **kwargs: inputs to specialize on.\u001b[39;00m\n\u001b[0;32m   2937\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2938\u001b[0m   graph_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_concrete_function_garbage_collected\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2939\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2940\u001b[0m   graph_function\u001b[38;5;241m.\u001b[39m_garbage_collector\u001b[38;5;241m.\u001b[39mrelease()  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   2941\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m graph_function\n",
      "File \u001b[1;32mD:\\Program files\\Anaconda\\envs\\pong\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2906\u001b[0m, in \u001b[0;36mFunction._get_concrete_function_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2904\u001b[0m   args, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   2905\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m-> 2906\u001b[0m   graph_function, args, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_define_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2907\u001b[0m   seen_names \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[0;32m   2908\u001b[0m   captured \u001b[38;5;241m=\u001b[39m object_identity\u001b[38;5;241m.\u001b[39mObjectIdentitySet(\n\u001b[0;32m   2909\u001b[0m       graph_function\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39minternal_captures)\n",
      "File \u001b[1;32mD:\\Program files\\Anaconda\\envs\\pong\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:3213\u001b[0m, in \u001b[0;36mFunction._maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3210\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_define_function_with_shape_relaxation(args, kwargs)\n\u001b[0;32m   3212\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function_cache\u001b[38;5;241m.\u001b[39mmissed\u001b[38;5;241m.\u001b[39madd(call_context_key)\n\u001b[1;32m-> 3213\u001b[0m graph_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_graph_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3214\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function_cache\u001b[38;5;241m.\u001b[39mprimary[cache_key] \u001b[38;5;241m=\u001b[39m graph_function\n\u001b[0;32m   3215\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m graph_function, args, kwargs\n",
      "File \u001b[1;32mD:\\Program files\\Anaconda\\envs\\pong\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:3065\u001b[0m, in \u001b[0;36mFunction._create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3060\u001b[0m missing_arg_names \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m   3061\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (arg, i) \u001b[38;5;28;01mfor\u001b[39;00m i, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(missing_arg_names)\n\u001b[0;32m   3062\u001b[0m ]\n\u001b[0;32m   3063\u001b[0m arg_names \u001b[38;5;241m=\u001b[39m base_arg_names \u001b[38;5;241m+\u001b[39m missing_arg_names\n\u001b[0;32m   3064\u001b[0m graph_function \u001b[38;5;241m=\u001b[39m ConcreteFunction(\n\u001b[1;32m-> 3065\u001b[0m     \u001b[43mfunc_graph_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc_graph_from_py_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3066\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3067\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_python_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3068\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3069\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3070\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_signature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3071\u001b[0m \u001b[43m        \u001b[49m\u001b[43mautograph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_autograph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3072\u001b[0m \u001b[43m        \u001b[49m\u001b[43mautograph_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_autograph_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3073\u001b[0m \u001b[43m        \u001b[49m\u001b[43marg_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43marg_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3074\u001b[0m \u001b[43m        \u001b[49m\u001b[43moverride_flat_arg_shapes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverride_flat_arg_shapes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3075\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapture_by_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_capture_by_value\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m   3076\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function_attributes,\n\u001b[0;32m   3077\u001b[0m     function_spec\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_spec,\n\u001b[0;32m   3078\u001b[0m     \u001b[38;5;66;03m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[39;00m\n\u001b[0;32m   3079\u001b[0m     \u001b[38;5;66;03m# scope. This is not the default behavior since it gets used in some\u001b[39;00m\n\u001b[0;32m   3080\u001b[0m     \u001b[38;5;66;03m# places (like Keras) where the FuncGraph lives longer than the\u001b[39;00m\n\u001b[0;32m   3081\u001b[0m     \u001b[38;5;66;03m# ConcreteFunction.\u001b[39;00m\n\u001b[0;32m   3082\u001b[0m     shared_func_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   3083\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m graph_function\n",
      "File \u001b[1;32mD:\\Program files\\Anaconda\\envs\\pong\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:986\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    983\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    984\u001b[0m   _, original_func \u001b[38;5;241m=\u001b[39m tf_decorator\u001b[38;5;241m.\u001b[39munwrap(python_func)\n\u001b[1;32m--> 986\u001b[0m func_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mpython_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfunc_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfunc_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    988\u001b[0m \u001b[38;5;66;03m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[39;00m\n\u001b[0;32m    989\u001b[0m \u001b[38;5;66;03m# TensorArrays and `None`s.\u001b[39;00m\n\u001b[0;32m    990\u001b[0m func_outputs \u001b[38;5;241m=\u001b[39m nest\u001b[38;5;241m.\u001b[39mmap_structure(convert, func_outputs,\n\u001b[0;32m    991\u001b[0m                                   expand_composites\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mD:\\Program files\\Anaconda\\envs\\pong\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:3364\u001b[0m, in \u001b[0;36mStructuredFunctionWrapper.__init__.<locals>.wrapper_fn\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m   3358\u001b[0m \u001b[38;5;129m@eager_function\u001b[39m\u001b[38;5;241m.\u001b[39mdefun_with_attributes(\n\u001b[0;32m   3359\u001b[0m     input_signature\u001b[38;5;241m=\u001b[39mstructure\u001b[38;5;241m.\u001b[39mget_flat_tensor_specs(\n\u001b[0;32m   3360\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_structure),\n\u001b[0;32m   3361\u001b[0m     autograph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   3362\u001b[0m     attributes\u001b[38;5;241m=\u001b[39mdefun_kwargs)\n\u001b[0;32m   3363\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper_fn\u001b[39m(\u001b[38;5;241m*\u001b[39margs):  \u001b[38;5;66;03m# pylint: disable=missing-docstring\u001b[39;00m\n\u001b[1;32m-> 3364\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43m_wrapper_helper\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3365\u001b[0m   ret \u001b[38;5;241m=\u001b[39m structure\u001b[38;5;241m.\u001b[39mto_tensor_list(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_structure, ret)\n\u001b[0;32m   3366\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m [ops\u001b[38;5;241m.\u001b[39mconvert_to_tensor(t) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m ret]\n",
      "File \u001b[1;32mD:\\Program files\\Anaconda\\envs\\pong\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:3299\u001b[0m, in \u001b[0;36mStructuredFunctionWrapper.__init__.<locals>._wrapper_helper\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m   3296\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _should_unpack_args(nested_args):\n\u001b[0;32m   3297\u001b[0m   nested_args \u001b[38;5;241m=\u001b[39m (nested_args,)\n\u001b[1;32m-> 3299\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[43mautograph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtf_convert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag_ctx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mnested_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3300\u001b[0m \u001b[38;5;66;03m# If `func` returns a list of tensors, `nest.flatten()` and\u001b[39;00m\n\u001b[0;32m   3301\u001b[0m \u001b[38;5;66;03m# `ops.convert_to_tensor()` would conspire to attempt to stack\u001b[39;00m\n\u001b[0;32m   3302\u001b[0m \u001b[38;5;66;03m# those tensors into a single tensor, because the customized\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3308\u001b[0m \u001b[38;5;66;03m# the return value into a single tensor can use an explicit\u001b[39;00m\n\u001b[0;32m   3309\u001b[0m \u001b[38;5;66;03m# `tf.stack()` before returning.\u001b[39;00m\n\u001b[0;32m   3310\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, \u001b[38;5;28mlist\u001b[39m):\n",
      "File \u001b[1;32mD:\\Program files\\Anaconda\\envs\\pong\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:255\u001b[0m, in \u001b[0;36mconvert.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    253\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    254\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m conversion_ctx:\n\u001b[1;32m--> 255\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[0;32m    257\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mag_error_metadata\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "File \u001b[1;32mD:\\Program files\\Anaconda\\envs\\pong\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:532\u001b[0m, in \u001b[0;36mconverted_call\u001b[1;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[0;32m    529\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _call_unconverted(f, args, kwargs, options)\n\u001b[0;32m    531\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m options\u001b[38;5;241m.\u001b[39muser_requested \u001b[38;5;129;01mand\u001b[39;00m conversion\u001b[38;5;241m.\u001b[39mis_whitelisted(f):\n\u001b[1;32m--> 532\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_call_unconverted\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    534\u001b[0m \u001b[38;5;66;03m# internal_convert_user_code is for example turned off when issuing a dynamic\u001b[39;00m\n\u001b[0;32m    535\u001b[0m \u001b[38;5;66;03m# call conversion from generated code while in nonrecursive mode. In that\u001b[39;00m\n\u001b[0;32m    536\u001b[0m \u001b[38;5;66;03m# case we evidently don't want to recurse, but we still have to convert\u001b[39;00m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;66;03m# things like builtins.\u001b[39;00m\n\u001b[0;32m    538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m options\u001b[38;5;241m.\u001b[39minternal_convert_user_code:\n",
      "File \u001b[1;32mD:\\Program files\\Anaconda\\envs\\pong\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:339\u001b[0m, in \u001b[0;36m_call_unconverted\u001b[1;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[0;32m    336\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m f\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__self__\u001b[39m\u001b[38;5;241m.\u001b[39mcall(args, kwargs)\n\u001b[0;32m    338\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 339\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    340\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs)\n",
      "File \u001b[1;32mD:\\Program files\\Anaconda\\envs\\pong\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py:319\u001b[0m, in \u001b[0;36mTensorLikeDataAdapter.__init__.<locals>.permutation\u001b[1;34m(_)\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpermutation\u001b[39m(_):\n\u001b[0;32m    316\u001b[0m   \u001b[38;5;66;03m# It turns out to be more performant to make a new set of indices rather\u001b[39;00m\n\u001b[0;32m    317\u001b[0m   \u001b[38;5;66;03m# than reusing the same range Tensor. (presumably because of buffer\u001b[39;00m\n\u001b[0;32m    318\u001b[0m   \u001b[38;5;66;03m# forwarding.)\u001b[39;00m\n\u001b[1;32m--> 319\u001b[0m   indices \u001b[38;5;241m=\u001b[39m \u001b[43mmath_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrange\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint64\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    320\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m shuffle \u001b[38;5;129;01mand\u001b[39;00m shuffle \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    321\u001b[0m     indices \u001b[38;5;241m=\u001b[39m random_ops\u001b[38;5;241m.\u001b[39mrandom_shuffle(indices)\n",
      "File \u001b[1;32mD:\\Program files\\Anaconda\\envs\\pong\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:201\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;124;03m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[39;00m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 201\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[0;32m    203\u001b[0m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[0;32m    204\u001b[0m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[0;32m    205\u001b[0m   result \u001b[38;5;241m=\u001b[39m dispatch(wrapper, args, kwargs)\n",
      "File \u001b[1;32mD:\\Program files\\Anaconda\\envs\\pong\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:1789\u001b[0m, in \u001b[0;36mrange\u001b[1;34m(start, limit, delta, dtype, name)\u001b[0m\n\u001b[0;32m   1787\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mname_scope(name, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRange\u001b[39m\u001b[38;5;124m\"\u001b[39m, [start, limit, delta]) \u001b[38;5;28;01mas\u001b[39;00m name:\n\u001b[0;32m   1788\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(start, ops\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m-> 1789\u001b[0m     start \u001b[38;5;241m=\u001b[39m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstart\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1790\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(limit, ops\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m   1791\u001b[0m     limit \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mconvert_to_tensor(limit, dtype\u001b[38;5;241m=\u001b[39mdtype, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlimit\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mD:\\Program files\\Anaconda\\envs\\pong\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1499\u001b[0m, in \u001b[0;36mconvert_to_tensor\u001b[1;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[0;32m   1494\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconvert_to_tensor did not convert to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1495\u001b[0m                       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe preferred dtype: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m vs \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1496\u001b[0m                       (ret\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mbase_dtype, preferred_dtype\u001b[38;5;241m.\u001b[39mbase_dtype))\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1499\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mconversion_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_ref\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mas_ref\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1501\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[0;32m   1502\u001b[0m   \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Program files\\Anaconda\\envs\\pong\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_conversion_registry.py:52\u001b[0m, in \u001b[0;36m_default_conversion_function\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_default_conversion_function\u001b[39m(value, dtype, name, as_ref):\n\u001b[0;32m     51\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m as_ref  \u001b[38;5;66;03m# Unused.\u001b[39;00m\n\u001b[1;32m---> 52\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconstant_op\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconstant\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Program files\\Anaconda\\envs\\pong\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:263\u001b[0m, in \u001b[0;36mconstant\u001b[1;34m(value, dtype, shape, name)\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconstant\u001b[39m\u001b[38;5;124m\"\u001b[39m, v1\u001b[38;5;241m=\u001b[39m[])\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconstant\u001b[39m(value, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, shape\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConst\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    168\u001b[0m   \u001b[38;5;124;03m\"\"\"Creates a constant tensor from a tensor-like object.\u001b[39;00m\n\u001b[0;32m    169\u001b[0m \n\u001b[0;32m    170\u001b[0m \u001b[38;5;124;03m  Note: All eager `tf.Tensor` values are immutable (in contrast to\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    261\u001b[0m \u001b[38;5;124;03m    ValueError: if called on a symbolic tensor.\u001b[39;00m\n\u001b[0;32m    262\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 263\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_constant_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    264\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mallow_broadcast\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Program files\\Anaconda\\envs\\pong\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:285\u001b[0m, in \u001b[0;36m_constant_impl\u001b[1;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[0;32m    283\u001b[0m dtype_value \u001b[38;5;241m=\u001b[39m attr_value_pb2\u001b[38;5;241m.\u001b[39mAttrValue(\u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39mtensor_value\u001b[38;5;241m.\u001b[39mtensor\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m    284\u001b[0m attrs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m: tensor_value, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: dtype_value}\n\u001b[1;32m--> 285\u001b[0m const_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_op_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    286\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mConst\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mdtype_value\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtype\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39moutputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m op_callbacks\u001b[38;5;241m.\u001b[39mshould_invoke_op_callbacks():\n\u001b[0;32m    289\u001b[0m   \u001b[38;5;66;03m# TODO(b/147670703): Once the special-op creation code paths\u001b[39;00m\n\u001b[0;32m    290\u001b[0m   \u001b[38;5;66;03m# are unified. Remove this `if` block.\u001b[39;00m\n\u001b[0;32m    291\u001b[0m   callback_outputs \u001b[38;5;241m=\u001b[39m op_callbacks\u001b[38;5;241m.\u001b[39minvoke_op_callbacks(\n\u001b[0;32m    292\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConst\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mtuple\u001b[39m(), attrs, (const_tensor,), op_name\u001b[38;5;241m=\u001b[39mname, graph\u001b[38;5;241m=\u001b[39mg)\n",
      "File \u001b[1;32mD:\\Program files\\Anaconda\\envs\\pong\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:591\u001b[0m, in \u001b[0;36mFuncGraph._create_op_internal\u001b[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[0;32m    589\u001b[0m   inp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcapture(inp)\n\u001b[0;32m    590\u001b[0m   inputs[i] \u001b[38;5;241m=\u001b[39m inp\n\u001b[1;32m--> 591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mFuncGraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_op_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    592\u001b[0m \u001b[43m    \u001b[49m\u001b[43mop_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_types\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_def\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    593\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute_device\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Program files\\Anaconda\\envs\\pong\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:3477\u001b[0m, in \u001b[0;36mGraph._create_op_internal\u001b[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[0;32m   3474\u001b[0m \u001b[38;5;66;03m# _create_op_helper mutates the new Operation. `_mutation_lock` ensures a\u001b[39;00m\n\u001b[0;32m   3475\u001b[0m \u001b[38;5;66;03m# Session.run call cannot occur between creating and mutating the op.\u001b[39;00m\n\u001b[0;32m   3476\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mutation_lock():\n\u001b[1;32m-> 3477\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mOperation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3478\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnode_def\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3479\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3480\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3481\u001b[0m \u001b[43m      \u001b[49m\u001b[43moutput_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3482\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcontrol_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontrol_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3483\u001b[0m \u001b[43m      \u001b[49m\u001b[43minput_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3484\u001b[0m \u001b[43m      \u001b[49m\u001b[43moriginal_op\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_default_original_op\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3485\u001b[0m \u001b[43m      \u001b[49m\u001b[43mop_def\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mop_def\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3486\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_op_helper(ret, compute_device\u001b[38;5;241m=\u001b[39mcompute_device)\n\u001b[0;32m   3487\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "File \u001b[1;32mD:\\Program files\\Anaconda\\envs\\pong\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1973\u001b[0m, in \u001b[0;36mOperation.__init__\u001b[1;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[0;32m   1971\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1972\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m op_def \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1973\u001b[0m     op_def \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_graph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_op_def\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode_def\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1974\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_c_op \u001b[38;5;241m=\u001b[39m _create_c_op(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph, node_def, inputs,\n\u001b[0;32m   1975\u001b[0m                             control_input_ops, op_def)\n\u001b[0;32m   1976\u001b[0m   name \u001b[38;5;241m=\u001b[39m compat\u001b[38;5;241m.\u001b[39mas_str(node_def\u001b[38;5;241m.\u001b[39mname)\n",
      "File \u001b[1;32mD:\\Program files\\Anaconda\\envs\\pong\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:3877\u001b[0m, in \u001b[0;36mGraph._get_op_def\u001b[1;34m(self, type)\u001b[0m\n\u001b[0;32m   3874\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[0;32m   3875\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m c_api_util\u001b[38;5;241m.\u001b[39mtf_buffer() \u001b[38;5;28;01mas\u001b[39;00m buf:\n\u001b[0;32m   3876\u001b[0m     \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m-> 3877\u001b[0m     \u001b[43mpywrap_tf_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTF_GraphGetOpDef\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_c_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3878\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mbuf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3879\u001b[0m     \u001b[38;5;66;03m# pylint: enable=protected-access\u001b[39;00m\n\u001b[0;32m   3880\u001b[0m     data \u001b[38;5;241m=\u001b[39m pywrap_tf_session\u001b[38;5;241m.\u001b[39mTF_GetBuffer(buf)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = []\n",
    "observation = env.reset()\n",
    "prev_input = None\n",
    "# main training loop\n",
    "while (True):\n",
    "\n",
    "    \"\"\"\n",
    "    Start by preprocessing the observation frame, and then doing the difference with the previous frame. \n",
    "    Naturally if frame 1 then we subtract by zeros\n",
    "    \n",
    "    X here is the frame-frame difference, Y is the next action (kind of like an RNN). \n",
    "    We are trying to predict the next action given two observations.\n",
    "    \"\"\"\n",
    "    cur_input = prepro(observation)\n",
    "    #print(len(cur_input)) - Sanity Check reasons only\n",
    "    \n",
    "    x = cur_input - prev_input if prev_input is not None else np.zeros(80 * 80)\n",
    "    prev_input = cur_input\n",
    "    \n",
    "    # forward the policy network and sample action according to the probability distribution\n",
    "    \"\"\"\n",
    "    Running model.predict to know what the current model thinks about the probability of doing the UP_ACTION, \n",
    "    given the current frame setting.\n",
    "    \n",
    "    Double check size and shape of the array here im pretty sure its for bias term?\n",
    "    Keras requires a third?? dimension perhaps hre\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    proba = model.predict(np.expand_dims(x, axis=1).T)\n",
    "    # proba=model.predict(np.expand_dims(x.reshape(80,80), axis=0)) 2D model stuff DELETE\n",
    "    \n",
    "    \n",
    "    #Intorucing another probability distirubtion here, not sure\n",
    "    action = UP_ACTION if np.random.uniform() < proba else DOWN_ACTION\n",
    "    y = 1 if action == 2 else 0 # 0 and 1 are our labels\n",
    "\n",
    "    # log the input and label to train later\n",
    "    x_train.append(x)\n",
    "    y_train.append(y)\n",
    "\n",
    "    # do one step in our environment - This is returned by our environment in OpenAI gym.  \n",
    "    observation, reward, done, info = env.step(action)\n",
    "    #Note how AT EACH STEP A REWARD IS CALCULATED. THIS IS NOT EACH GAME BUT EACH FRAME-FRAME DIFFERENCE. \n",
    "    #MOST OF THE TIME THIS IS 0\n",
    "    #THESE REWARDS ARE USED TO ENCOURAGE OR DISCOURAGE MOVEMENTS\n",
    "    rewards.append(reward)\n",
    "    reward_sum += reward\n",
    "    \n",
    "    \"\"\"\n",
    "    rewards : to each frame (x_train[frame_number]) and action (y_train[frame_number]) \n",
    "    is associated a reward (-1 if it missed the ball, 0 if nothing happens, and 1 if opponent misses the ball), \n",
    "    so we get for instance the following array:\n",
    "    \"\"\"\n",
    "    \n",
    "    # end of an episode - The GYM also invokes DONE automatically. Invoked when one player reaches 21\n",
    "    if done:\n",
    "        \n",
    "        history.append(reward_sum)\n",
    "        print('At the end of episode', episode_nb, 'the total reward was :', reward_sum)\n",
    "        if episode_nb>=3000 and reward_sum >=-12:\n",
    "          break\n",
    "        else:\n",
    "          \n",
    "        \n",
    "          # increment episode number\n",
    "          episode_nb += 1\n",
    "        \n",
    "          # training\n",
    "          model.fit(x=np.vstack(x_train), y=np.vstack(y_train), verbose=1, sample_weight=discount_rewards(rewards, gamma))\n",
    "        \n",
    "          \"\"\"\n",
    "          If an action leads to a positive reward, it tunes the weights of the neural network so \n",
    "          it keeps on predicting this winning action.\n",
    "          Otherwise, it tunes them in the opposite way\n",
    "\n",
    "\n",
    "          The function discount_rewards transforms the list of rewards \n",
    "          so that even actions that remotely lead to positive rewards are encouraged. THIS IS IMPORTANT\n",
    "          OTHERWISE WE WOULD SIMPLY BE TRYING TO REPLICATE RANDOM MOVEMENTS\n",
    "\n",
    "          sample_weights is used to provide a weight for each training sample. \n",
    "          That means that you should pass a 1D array with the same number of elements as your training samples \n",
    "          (indicating the weight for each of those samples. NOTE THIS IS NOT CLASS WEIGHTS\n",
    "          \"\"\"\n",
    "                                                             \n",
    "          # Reinitialization\n",
    "          x_train, y_train, rewards = [],[],[]\n",
    "          observation = env.reset()\n",
    "          reward_sum = 0\n",
    "          prev_input = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc6dfae3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#Plot results - remember to call keyboard interrupt before this\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[43mhistory\u001b[49m)\n\u001b[0;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "#Plot results - remember to call keyboard interrupt before this\n",
    "\n",
    "plt.plot(history)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "201e5381",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 2] The system cannot find the file specified",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [43]\u001b[0m, in \u001b[0;36m<cell line: 23>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mIPython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m display \u001b[38;5;28;01mas\u001b[39;00m ipythondisplay\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyvirtualdisplay\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Display\n\u001b[1;32m---> 23\u001b[0m display \u001b[38;5;241m=\u001b[39m \u001b[43mDisplay\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvisible\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m768\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m display\u001b[38;5;241m.\u001b[39mstart()\n",
      "File \u001b[1;32mD:\\Program files\\Anaconda\\envs\\pong\\lib\\site-packages\\pyvirtualdisplay\\display.py:54\u001b[0m, in \u001b[0;36mDisplay.__init__\u001b[1;34m(self, backend, visible, size, color_depth, bgcolor, use_xauth, retries, extra_args, manage_global_env, **kwargs)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcls\u001b[39m:\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munknown backend: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend)\n\u001b[1;32m---> 54\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolor_depth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolor_depth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbgcolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbgcolor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_xauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_xauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# check_startup=check_startup,\u001b[39;49;00m\n\u001b[0;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43mextra_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmanage_global_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanage_global_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Program files\\Anaconda\\envs\\pong\\lib\\site-packages\\pyvirtualdisplay\\xvfb.py:44\u001b[0m, in \u001b[0;36mXvfbDisplay.__init__\u001b[1;34m(self, size, color_depth, bgcolor, use_xauth, fbdir, dpi, retries, extra_args, manage_global_env)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fbdir \u001b[38;5;241m=\u001b[39m fbdir\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dpi \u001b[38;5;241m=\u001b[39m dpi\n\u001b[1;32m---> 44\u001b[0m \u001b[43mAbstractDisplay\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mPROGRAM\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_xauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_xauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mextra_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmanage_global_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanage_global_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Program files\\Anaconda\\envs\\pong\\lib\\site-packages\\pyvirtualdisplay\\abstractdisplay.py:85\u001b[0m, in \u001b[0;36mAbstractDisplay.__init__\u001b[1;34m(self, program, use_xauth, retries, extra_args, manage_global_env)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pipe_wfd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retries_current \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 85\u001b[0m helptext \u001b[38;5;241m=\u001b[39m \u001b[43mget_helptext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprogram\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_displayfd \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-displayfd\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m helptext\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_displayfd:\n",
      "File \u001b[1;32mD:\\Program files\\Anaconda\\envs\\pong\\lib\\site-packages\\pyvirtualdisplay\\util.py:13\u001b[0m, in \u001b[0;36mget_helptext\u001b[1;34m(program)\u001b[0m\n\u001b[0;32m      6\u001b[0m cmd \u001b[38;5;241m=\u001b[39m [program, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-help\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# py3.7+\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# p = subprocess.run(cmd, capture_output=True)\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# stderr = p.stderr\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# py3.6 also\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m p \u001b[38;5;241m=\u001b[39m \u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstdout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPIPE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstderr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPIPE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshell\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m _, stderr \u001b[38;5;241m=\u001b[39m p\u001b[38;5;241m.\u001b[39mcommunicate()\n\u001b[0;32m     21\u001b[0m helptext \u001b[38;5;241m=\u001b[39m stderr\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mD:\\Program files\\Anaconda\\envs\\pong\\lib\\subprocess.py:858\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors, text)\u001b[0m\n\u001b[0;32m    854\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_mode:\n\u001b[0;32m    855\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mTextIOWrapper(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr,\n\u001b[0;32m    856\u001b[0m                     encoding\u001b[38;5;241m=\u001b[39mencoding, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m--> 858\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreexec_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    859\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mpass_fds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    860\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshell\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    861\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mp2cread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp2cwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    862\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mc2pread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2pwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    863\u001b[0m \u001b[43m                        \u001b[49m\u001b[43merrread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    864\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mrestore_signals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_new_session\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    865\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m    866\u001b[0m     \u001b[38;5;66;03m# Cleanup if the child failed starting.\u001b[39;00m\n\u001b[0;32m    867\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstdin, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstdout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr)):\n",
      "File \u001b[1;32mD:\\Program files\\Anaconda\\envs\\pong\\lib\\subprocess.py:1311\u001b[0m, in \u001b[0;36mPopen._execute_child\u001b[1;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_start_new_session)\u001b[0m\n\u001b[0;32m   1309\u001b[0m \u001b[38;5;66;03m# Start the process\u001b[39;00m\n\u001b[0;32m   1310\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1311\u001b[0m     hp, ht, pid, tid \u001b[38;5;241m=\u001b[39m \u001b[43m_winapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCreateProcess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1312\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;66;43;03m# no special security\u001b[39;49;00m\n\u001b[0;32m   1313\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1314\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1315\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1316\u001b[0m \u001b[43m                             \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1317\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1318\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;66;03m# Child is launched. Close the parent's copy of those pipe\u001b[39;00m\n\u001b[0;32m   1321\u001b[0m     \u001b[38;5;66;03m# handles that only the child should have open.  You need\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1324\u001b[0m     \u001b[38;5;66;03m# pipe will not close when the child process exits and the\u001b[39;00m\n\u001b[0;32m   1325\u001b[0m     \u001b[38;5;66;03m# ReadFile will hang.\u001b[39;00m\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_pipe_fds(p2cread, p2cwrite,\n\u001b[0;32m   1327\u001b[0m                          c2pread, c2pwrite,\n\u001b[0;32m   1328\u001b[0m                          errread, errwrite)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] The system cannot find the file specified"
     ]
    }
   ],
   "source": [
    "#To Evaluate model on OpenAI gym, we will record a video via Ipython display\n",
    "\n",
    "import gym\n",
    "from gym import logger as gymlogger\n",
    "# from gym.wrappers import Monitor\n",
    "gymlogger.set_level(40) #error only\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import math\n",
    "import glob\n",
    "import io\n",
    "import base64\n",
    "from IPython.display import HTML\n",
    "\n",
    "from IPython import display as ipythondisplay\n",
    "\n",
    "from pyvirtualdisplay import Display\n",
    "\n",
    "display = Display(visible=0, size=(1024, 768))\n",
    "display.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "57c3c7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Utility functions to enable video recording of gym environment and displaying it\n",
    "To enable video, just do \"env = wrap_env(env)\"\"\n",
    "\"\"\"\n",
    "\n",
    "def show_video():\n",
    "  mp4list = glob.glob('video/*.mp4')\n",
    "  if len(mp4list) > 0:\n",
    "    mp4 = mp4list[0]\n",
    "    video = io.open(mp4, 'r+b').read()\n",
    "    encoded = base64.b64encode(video)\n",
    "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
    "                loop controls style=\"height: 400px;\">\n",
    "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "             </video>'''.format(encoded.decode('ascii'))))\n",
    "  else: \n",
    "    print(\"Could not find video\")\n",
    "    \n",
    "\n",
    "def wrap_env(env):\n",
    "  env = Monitor(env, './video', force=True)\n",
    "  return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "70383b31",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Monitor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [29]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#Evaluate model on openAi GYM\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#To do this consult https://github.com/thinkingparticle/deep_rl_pong_keras/blob/master/reinforcement_learning_pong_keras_policy_gradients.ipynb\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43mwrap_env\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgym\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mALE/Pong-v5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m observation \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m      6\u001b[0m new_observation \u001b[38;5;241m=\u001b[39m observation\n",
      "Input \u001b[1;32mIn [27]\u001b[0m, in \u001b[0;36mwrap_env\u001b[1;34m(env)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrap_env\u001b[39m(env):\n\u001b[1;32m---> 21\u001b[0m   env \u001b[38;5;241m=\u001b[39m \u001b[43mMonitor\u001b[49m(env, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./video\u001b[39m\u001b[38;5;124m'\u001b[39m, force\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     22\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m env\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Monitor' is not defined"
     ]
    }
   ],
   "source": [
    "#Evaluate model on openAi GYM\n",
    "\n",
    "#To do this consult https://github.com/thinkingparticle/deep_rl_pong_keras/blob/master/reinforcement_learning_pong_keras_policy_gradients.ipynb\n",
    "env = wrap_env(gym.make('ALE/Pong-v5'))\n",
    "observation = env.reset()\n",
    "new_observation = observation\n",
    "prev_input = None\n",
    "done = False\n",
    "while True:\n",
    "  if True: \n",
    "    \n",
    "    #set input to network to be difference image\n",
    "  \n",
    "    cur_input = prepro(observation)\n",
    "    x = cur_input - prev_input if prev_input is not None else np.zeros(80 * 80)\n",
    "    prev_input = cur_input\n",
    "  \n",
    "    # Sample an action (policy)\n",
    "    proba = model.predict(np.expand_dims(x, axis=1).T)\n",
    "    action = UP_ACTION if np.random.uniform() < proba else DOWN_ACTION\n",
    "        \n",
    "    env.render()\n",
    "    # Return action to environment and extract\n",
    "    #next observation, reward, and status\n",
    "    observation = new_observation\n",
    "    new_observation, reward, done, info = env.step(action)\n",
    "    if done: \n",
    "      #observation = env.reset()\n",
    "      break\n",
    "      \n",
    "env.close()\n",
    "show_video()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5ae1ce0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyvirtualdisplay.display.Display'>\n"
     ]
    }
   ],
   "source": [
    "from pyvirtualdisplay.display import Display\n",
    "print(Display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8387920b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
